,Paper ID,Presentation,Title,Authors,Title (arxiv),Abstract,URL
0,52,Oral,Embodied Question Answering,"Abhishek Das, Georgia Tech; Samyak Datta, Georgia Tech; Georgia Gkioxari, Facebook; Devi Parikh, Georgia Tech; Dhruv Batra, Georgia Tech; Stefan Lee, Georgia Tech",Embodied Question Answering,"We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (""What color is the car?""). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question (""orange"").   This challenging task requires a range of AI skills -- active perception, language understanding, goal-driven navigation, commonsense reasoning, and grounding of language into actions. In this work, we develop the environments, end-to-end-trained reinforcement learning agents, and evaluation protocols for EmbodiedQA.",http://arxiv.org/pdf/1711.11543v2
1,120,Oral,Learning by Asking Questions,"Ishan Misra, CMU; Ross Girshick, ; Rob Fergus, New York University; Martial Hebert, Carnegie Mellon University; Abhinav Gupta, ; Laurens van der Maaten, Facebook",Adviser Networks: Learning What Question to Ask for Human-In-The-Loop Viewpoint Estimation,"Humans have an unparalleled visual intelligence and can overcome visual ambiguities that machines currently cannot. Recent works have shown that incorporating guidance from humans during inference for monocular viewpoint-estimation can help overcome difficult cases in which the computer-alone would have otherwise failed. These hybrid intelligence approaches are hence gaining traction. However, deciding what question to ask the human at inference time remains an unknown for these problems.   We address this question by formulating it as an Adviser Problem: can we learn a mapping from the input to a specific question to ask the human to maximize the expected positive impact to the overall task? We formulate a solution to the adviser problem for viewpoint estimation using a deep network where the question asks for the location of a keypoint in the input image. We show that by using the Adviser Network's recommendations, the model and the human outperforms the previous hybrid-intelligence state-of-the-art by 3.7%, and the computer-only state-of-the-art by 5.28% absolute.",http://arxiv.org/pdf/1802.01666v2
2,565,Oral,Finding Tiny Faces in the Wild with Generative Adversarial Network,"Yancheng Bai, Kaust/Iscas; Yongqiang Zhang, Harbin institute of Technology/KAUST; Mingli Ding, ; Bernard Ghanem,",,,
3,NaN,Oral,Learning Face Age Progression: A Pyramid Architecture of GANs,"Hongyu Yang, BEIHANG UNIVERSITY; Di Huang, ; Yunhong Wang, ; Anil Jain, MSU",Learning Face Age Progression: A Pyramid Architecture of GANs,"The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well handled in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable for diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.",http://arxiv.org/pdf/1711.10352v1
4,3623,Oral,MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks,"Huiwen Chang, ; Jingwan Lu, Adobe Research; Fisher Yu, UC Berkeley; Adam Finkelstein, Princeton University",,,
5,736,Spotlight,GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB,"Franziska Mueller, MPI Informatics; Florian Bernard, MPI Informatics; Oleksandr Sotnychenko, MPI Informatics; Dushyant Mehta, MPI For Informatics; Srinath Sridhar, ; Dan Casas, MPI; Christian Theobalt, MPI Informatics",GANerated Hands for Real-time 3D Hand Tracking from Monocular RGB,"We address the highly challenging problem of real-time 3D hand tracking based on a monocular RGB-only sequence. Our tracking method combines a convolutional neural network with a kinematic 3D hand model, such that it generalizes well to unseen data, is robust to occlusions and varying camera viewpoints, and leads to anatomically plausible as well as temporally smooth hand motions. For training our CNN we propose a novel approach for the synthetic generation of training data that is based on a geometrically consistent image-to-image translation network. To be more specific, we use a neural network that translates synthetic images to ""real"" images, such that the so-generated images follow the same statistical distribution as real-world hand images. For training this translation network we combine an adversarial loss and a cycle-consistency loss with a geometric consistency loss in order to preserve geometric properties (such as hand pose) during translation. We demonstrate that our hand tracking system outperforms the current state-of-the-art on challenging RGB-only footage.",http://arxiv.org/pdf/1712.01057v1
6,1772,Spotlight,Learning Pose Specific Representations by Predicting different Views,"Georg Poier, Graz University of Technology; David Schinagl, ; Horst Bischof,",,,
7,2130,Spotlight,Weakly Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer,"Hao-Shu Fang, Shanghai Jiao Tong University; Guansong Lu, Shanghai Jiao Tong University; Xiaolin Fang, Zhejiang University; Yu-Wing Tai, Tencent YouTu; Cewu Lu, Shanghai Jiao Tong University",,,
8,2672,Spotlight,Person Transfer GAN to Bridge Domain Gap for Person Re-Identification,"Longhui Wei, Peking University; Shiliang Zhang, Peking University; Wen Gao, ; Qi Tian,",Person Transfer GAN to Bridge Domain Gap for Person Re-Identification,"Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.",http://arxiv.org/pdf/1711.08565v1
9,3284,Spotlight,Cross-modal Deep Variational Hand Pose Estimation,"Adrian Spurr, ETH Zurich; Jie Song, ETHZ; Seonwook Park, ETH Zurich; Otmar HIlliges, ETH Zurich",Cross-modal Deep Variational Hand Pose Estimation,"The human hand moves in complex and high-dimensional ways, making estimation of 3D hand pose configurations from images alone a challenging task. In this work we propose a method to learn a statistical hand model represented by a cross-modal trained latent space via a generative deep neural network. We derive an objective function from the variational lower bound of the VAE framework and jointly optimize the resulting cross-modal KL-divergence and the posterior reconstruction objective, naturally admitting a training regime that leads to a coherent latent space across multiple modalities such as RGB images, 2D keypoint detections or 3D hand configurations. Additionally, it grants a straightforward way of using semi-supervision. This latent space can be directly used to estimate 3D hand poses from RGB images, outperforming the state-of-the art in different settings. Furthermore, we show that our proposed method can be used without changes on depth images and performs comparably to specialized methods. Finally, the model is fully generative and can synthesize consistent pairs of hand configurations across modalities. We evaluate our method on both RGB and depth datasets and analyze the latent space qualitatively.",http://arxiv.org/pdf/1803.11404v1
10,1801,Spotlight,Disentangled Person Image Generation,"Liqian Ma, KU Leuven; Qianru Sun, MPI for Informatics; Stamatios Georgoulis, KU Leuven; Mario Fritz, MPI, Saarbrucken, Germany; Bernt Schiele, MPI Informatics Germany; Luc Van Gool, KU Leuven",Disentangled Person Image Generation,"Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.",http://arxiv.org/pdf/1712.02621v2
11,568,Spotlight,Integrated facial landmark localization and super-resolution of real-world very low resolution faces in arbitrary poses with GANs,"Adrian Bulat, ; Georgios Tzimiropoulos,",Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs,"This paper addresses 2 challenging tasks: improving the quality of low resolution facial images and accurately locating the facial landmarks on such poor resolution images. To this end, we make the following 5 contributions: (a) we propose Super-FAN: the very first end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the facial landmarks. The novelty or Super-FAN lies in incorporating structural information in a GAN-based super-resolution algorithm via integrating a sub-network for face alignment through heatmap regression and optimizing a novel heatmap loss. (b) We illustrate the benefit of training the two networks jointly by reporting good results not only on frontal images (as in prior work) but on the whole spectrum of facial poses, and not only on synthetic low resolution images (as in prior work) but also on real-world images. (c) We improve upon the state-of-the-art in face super-resolution by proposing a new residual-based architecture. (d) Quantitatively, we show large improvement over the state-of-the-art for both face super-resolution and alignment. (e) Qualitatively, we show for the first time good results on real-world low resolution images.",http://arxiv.org/pdf/1712.02765v2
12,1621,Spotlight,Multistage Adversarial Losses for Pose-Based Human Image Synthesis,"Chenyang Si, Institute of Automation, Chine; Wei Wang, ; Liang Wang, unknown; Tieniu Tan, NLPR China",,,
13,984,Oral,Rotation Averaging and Strong Duality,"Anders Eriksson, ; Fredrik Kahl, Chalmers; Carl Olsson, Lund University; Tat-Jun Chin,",Rotation Averaging and Strong Duality,"In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of computer vision applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time.   We also propose an efficient, scalable algorithm that out-performs general purpose numerical solvers and is able to handle the large problem instances commonly occurring in structure from motion settings. The potential of this proposed method is demonstrated on a number of different problems, consisting of both synthetic and real-world data.",http://arxiv.org/pdf/1705.01362v2
14,2462,Oral,Hybrid Camera Pose Estimation,"Federico Camposeco, ETH; Andrea Cohen, ETH Zurich; Marc Pollefeys, ETH; Torsten Sattler, ETH Zurich",Laser map aided visual inertial localization in changing environment,"Long-term visual localization in outdoor environment is a challenging problem, especially faced with the cross-seasonal, bi-directional tasks and changing environment. In this paper we propose a novel visual inertial localization framework that localizes against the LiDAR-built map. Based on the geometry information of the laser map, a hybrid bundle adjustment framework is proposed, which estimates the poses of the cameras with respect to the prior laser map as well as optimizes the state variables of the online visual inertial odometry system simultaneously. For more accurate cross-modal data association, the laser map is optimized using multi-session laser and visual data to extract the salient and stable subset for localization. To validate the efficiency of the proposed method, we collect data in south part of our campus in different seasons, along the same and opposite-direction route. In all sessions of localization data, our proposed method gives satisfactory results, and shows the superiority of the hybrid bundle adjustment and map optimization.",http://arxiv.org/pdf/1803.01104v1
15,3968,Oral,A Globally Optimal Solution to the Non-Minimal Relative Pose Problem,"Jesus Briales, University of Malaga; Laurent Kneip, ; Javier Gonzalez-Jimenez,",A Certifiably Correct Algorithm for Synchronization over the Special Euclidean Group,"Many geometric estimation problems take the form of synchronization over the special Euclidean group: estimate the values of a set of poses given noisy measurements of a subset of their pairwise relative transforms. This problem is typically formulated as a maximum-likelihood estimation that requires solving a nonconvex nonlinear program, which is computationally intractable in general. Nevertheless, in this paper we present an algorithm that is able to efficiently recover certifiably globally optimal solutions of this estimation problem in a non-adversarial noise regime. The crux of our approach is the development of a semidefinite relaxation of the maximum-likelihood estimation whose minimizer provides the exact MLE so long as the magnitude of the noise corrupting the available measurements falls below a certain critical threshold; furthermore, whenever exactness obtains, it is possible to verify this fact a posteriori, thereby certifying the optimality of the recovered estimate. We develop a specialized optimization scheme for solving large-scale instances of this semidefinite relaxation by exploiting its low-rank, geometric, and graph-theoretic structure to reduce it to an equivalent optimization problem on a low-dimensional Riemannian manifold, and then design a Riemannian truncated-Newton trust-region method to solve this reduction efficiently. We combine this fast optimization approach with a simple rounding procedure to produce our algorithm, SE-Sync. Experimental evaluation on a variety of simulated and real-world pose-graph SLAM datasets shows that SE-Sync is capable of recovering globally optimal solutions when the available measurements are corrupted by noise up to an order of magnitude greater than that typically encountered in robotics applications, and does so at a computational cost that scales comparably with that of direct Newton-type local search techniques.",http://arxiv.org/pdf/1611.00128v3
16,2956,Spotlight,Single View Stereo Matching,"Yue Luo, SenseTime; Jimmy Ren, SenseTime Group Limited; Mude Lin, Sun Yat-Sen University; Jiahao Pang, SenseTime Group Limited; Wenxiu Sun, SenseTime Group Limited; Hongsheng Li, ; Liang Lin,",Prioritized Multi-View Stereo Depth Map Generation Using Confidence Prediction,"In this work, we propose a novel approach to prioritize the depth map computation of multi-view stereo (MVS) to obtain compact 3D point clouds of high quality and completeness at low computational cost. Our prioritization approach operates before the MVS algorithm is executed and consists of two steps. In the first step, we aim to find a good set of matching partners for each view. In the second step, we rank the resulting view clusters (i.e. key views with matching partners) according to their impact on the fulfillment of desired quality parameters such as completeness, ground resolution and accuracy. Additional to geometric analysis, we use a novel machine learning technique for training a confidence predictor. The purpose of this confidence predictor is to estimate the chances of a successful depth reconstruction for each pixel in each image for one specific MVS algorithm based on the RGB images and the image constellation. The underlying machine learning technique does not require any ground truth or manually labeled data for training, but instead adapts ideas from depth map fusion for providing a supervision signal. The trained confidence predictor allows us to evaluate the quality of image constellations and their potential impact to the resulting 3D reconstruction and thus builds a solid foundation for our prioritization approach. In our experiments, we are thus able to reach more than 70% of the maximal reachable quality fulfillment using only 5% of the available images as key views. For evaluating our approach within and across different domains, we use two completely different scenarios, i.e. cultural heritage preservation and reconstruction of single family houses.",http://arxiv.org/pdf/1803.08323v1
17,2980,Spotlight,Fight ill-posedness with ill-posedness: Single-shot variational depth super-resolution from shading,"Bjoern Haefner, TU Munich; Yvain Queau, Technical University Munich; Thomas Möllenhoff, Technical University of Munich; Daniel Cremers,",,,
18,3324,Spotlight,Deep Depth Completion of a Single RGB-D Image,"Yinda Zhang, Princeton; Thomas Funkhouser, Princeton",,,
19,142,Spotlight,Multi-view Harmonized Bilinear Network for 3D Object Recognition,"Tan Yu, Nanyang Technological Univ; Jingjing Meng, ; Junsong Yuan, Nanyang Technological University",,,
20,1025,Spotlight,PPFNet: Global Context Aware Local Features for Robust 3D Point Matching,"Haowen Deng, Technical University of Munich; Tolga Birdal, Technical University of Munich; Slobodan Ilic, Siemens AG",PPFNet: Global Context Aware Local Features for Robust 3D Point Matching,"We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel $\textit{N-tuple}$ loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.",http://arxiv.org/pdf/1802.02669v2
21,1129,Spotlight,FoldingNet: Interpretable Unsupervised Learning on 3D Point Clouds,"Yaoqing Yang, Carnegie Mellon University; Chen Feng, MERL; Yiru Shen, Clemson University; Dong Tian, Mitsubishi Electric Research Laboratories",,,
22,1775,Spotlight,A Papier-Mâché Approach to Learning 3D Surface Generation,"Thibault GROUEIX, École des ponts ParisTech; Bryan Russell, Adobe; Mathew Fisher, Adobe Systems; Mathieu Aubry, ; Vladimir Kim, Adobe Research",,,
23,2629,Spotlight,LEGO: Learning Edge with Geometry all at Once by Watching Videos,"Zhenheng Yang, ; Peng Wang, Baidu; Yang Wang, Baidu USA; Wei Xu, ; Ram Nevatia,",LEGO: Learning Edge with Geometry all at Once by Watching Videos,"Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network is attracting significant attention. In this paper, we introduce a ""3D as-smooth-as-possible (3D-ASAP)"" prior inside the pipeline, which enables joint estimation of edges and 3D scene, yielding results with significant improvement in accuracy for fine detailed structures. Specifically, we define the 3D-ASAP prior by requiring that any two points recovered in 3D from an image should lie on an existing planar surface if no other cues provided. We design an unsupervised framework that Learns Edges and Geometry (depth, normal) all at Once (LEGO). The predicted edges are embedded into depth and surface normal smoothness terms, where pixels without edges in-between are constrained to satisfy the prior. In our framework, the predicted depths, normals and edges are forced to be consistent all the time. We conduct experiments on KITTI to evaluate our estimated geometry and CityScapes to perform edge evaluation. We show that in all of the tasks, i.e.depth, normal and edge, our algorithm vastly outperforms other state-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach.",http://arxiv.org/pdf/1803.05648v2
24,3633,Poster,Learning Face Age Progression: A Pyramid Architecture of GANs,"Hongyu Yang, BEIHANG UNIVERSITY; Di Huang, ; Yunhong Wang, ; Anil Jain, MSU",Learning Face Age Progression: A Pyramid Architecture of GANs,"The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well handled in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable for diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.",http://arxiv.org/pdf/1711.10352v1
25,363,Poster,Five-point Fundamental Matrix Estimation for Uncalibrated Cameras,"Daniel Barath, MTA SZTAKI",Five-point Fundamental Matrix Estimation for Uncalibrated Cameras,"We aim at estimating the fundamental matrix in two views from five correspondences of rotation invariant features obtained by e.g.\ the SIFT detector. The proposed minimal solver first estimates a homography from three correspondences assuming that they are co-planar and exploiting their rotational components. Then the fundamental matrix is obtained from the homography and two additional point pairs in general position. The proposed approach, combined with robust estimators like Graph-Cut RANSAC, is superior to other state-of-the-art algorithms both in terms of accuracy and number of iterations required. This is validated on synthesized data and $561$ real image pairs. Moreover, the tests show that requiring three points on a plane is not too restrictive in urban environment and locally optimized robust estimators lead to accurate estimates even if the points are not entirely co-planar. As a potential application, we show that using the proposed method makes two-view multi-motion estimation more accurate.",http://arxiv.org/pdf/1803.00260v1
26,766,Poster,PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation,"Danfei Xu, Stanford Univesity; dragomir Anguelov, Zoox Inc.; Ashesh Jain, Zoox Inc.",PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation,"We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.",http://arxiv.org/pdf/1711.10871v1
27,1350,Poster,Scalable Dense Non-rigid Structure-from-Motion: A Grassmannian Perspective,"Suryansh Kumar, Australian National University; Anoop Cherian, ; Yuchao Dai, Australian National University; Hongdong Li, Australian National University",Scalable Dense Non-rigid Structure-from-Motion: A Grassmannian Perspective,"This paper addresses the task of dense non-rigid structure-from-motion (NRSfM) using multiple images. State-of-the-art methods to this problem are often hurdled by scalability, expensive computations, and noisy measurements. Further, recent methods to NRSfM usually either assume a small number of sparse feature points or ignore local non-linearities of shape deformations, and thus cannot reliably model complex non-rigid deformations. To address these issues, in this paper, we propose a new approach for dense NRSfM by modeling the problem on a Grassmann manifold. Specifically, we assume the complex non-rigid deformations lie on a union of local linear subspaces both spatially and temporally. This naturally allows for a compact representation of the complex non-rigid deformation over frames. We provide experimental results on several synthetic and real benchmark datasets. The procured results clearly demonstrate that our method, apart from being scalable and more accurate than state-of-the-art methods, is also more robust to noise and generalizes to highly non-linear deformations.",http://arxiv.org/pdf/1803.00233v2
28,1389,Poster,GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition,"Yifan Feng, Xidian university; Zizhao Zhang, ; xibin Zhao, ; Rongrong Ji, ; Yue Gao, Tsinghua University",,,
29,2455,Poster,Depth and Transient Imaging with Compressive SPAD Array Cameras,"Qilin Sun, KAUST; Xiong Dun, KAUST; Yifan (Evan) Peng, UBC; Wolfgang Heidrich,",,,
30,2837,Poster,GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation,"Xiaojuan Qi, CUHK; Renjie Liao, ; Zhengzhe Liu, CUHK; Raquel Urtasun, University of Toronto; Jiaya Jia, Chinese University of Hong Kong",,,
31,3117,Poster,Real-Time Seamless Single Shot 6D Object Pose Prediction,"Bugra Tekin, ; Sudipta Sinha, Microsoft Research; Pascal Fua,",Real-Time Seamless Single Shot 6D Object Pose Prediction,"We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task (Kehl et al., ICCV'17) that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by the YOLO network design that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm.   For single object and multiple object pose estimation on the LINEMOD and OCCLUSION datasets, our approach substantially outperforms other recent CNN-based approaches when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of the existing methods, but at 10 fps or less, they are much slower than our method.",http://arxiv.org/pdf/1711.08848v2
32,757,Poster,"Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene","Shubham Tulsiani, UC Berkeley; David Fouhey, UC Berkeley; Saurabh Gupta, ; Alexei Efros, UC Berkeley; Jitendra Malik,","Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene","The goal of this paper is to take a single 2D image of a scene and recover the 3D structure in terms of a small set of factors: a layout representing the enclosing surfaces as well as a set of objects represented in terms of shape and pose. We propose a convolutional neural network-based approach to predict this representation and benchmark it on a large dataset of indoor scenes. Our experiments evaluate a number of practical design questions, demonstrate that we can infer this representation, and quantitatively and qualitatively demonstrate its merits compared to alternate representations.",http://arxiv.org/pdf/1712.01812v1
33,944,Poster,Monocular Relative Depth Perception with Web Stereo Data Supervision,"Ke Xian, Huazhong University of Science and Technology; Chunhua Shen, University of Adelaide; Zhiguo Cao, Huazhong University of Science and Technology; Hao Lu, Huazhong University of Science and Technology; yang xiao, Huazhong University of Science and Technology; Ruibo Li, Huazhong University of Science and Technology; Zhenbo Luo, Samsung Research Beijing",,,
34,2138,Poster,Spline Error Weighting for Robust Visual-Inertial Fusion,"Hannes Ovrén, Linköping University; Per-Erik Forssen, Linkoping University",,,
35,2873,Poster,Single-Image Depth Estimation Based on Fourier Domain Analysis,"Jaehan Lee, Korea University; Minhyeok Heo, Korea Unversity; Kyung-Rae Kim, Korea University; Chang-Su Kim,",,,
36,4186,Poster,Unsupervised Learning of Single View Depth Estimation and Visual Odometry with Deep Feature Reconstruction,"Huangying Zhan, The University of Adelaide; Ravi Garg, The University of Adelaide; Chamara Weerasekera, The University of Adelaide; Kejie Li, The University of Adelaide; Harsh Agarwal, Indian Institute of Technology (BHU); Ian Reid,",Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction,"Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat",http://arxiv.org/pdf/1803.03893v3
37,122,Poster,Detect-and-Track: Efficient Pose Estimation in Videos,"Rohit Girdhar, CMU; Georgia Gkioxari, Facebook; Lorenzo Torresani, Darthmout College, USA; Manohar Paluri, ; Du Tran, Dartmouth College",Detect-and-Track: Efficient Pose Estimation in Videos,"This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge.",http://arxiv.org/pdf/1712.09184v1
38,191,Poster,Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors,"Xuanyi Dong, UTS; Shoou-I Yu, Oculus; Xinshuo Weng, Carnegie Mellon University; Shih-En Wei, Oculus Research; Yi Yang, ; Yaser Sheikh,",,,
39,312,Poster,Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification,"Shuang Li, The Chinese University of HK; Slawomir Bak, Disney Research; Peter Carr, Disney Research",Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification,"Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts. This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments. The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part. Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics.",http://arxiv.org/pdf/1803.09882v1
40,436,Poster,Style Aggregated Network for Facial Landmark Detection,"Xuanyi Dong, UTS; Yan Yan, UTS; Wanli Ouyang, The University of Sydney; Yi Yang,",Style Aggregated Network for Facial Landmark Detection,"Recent advances in facial landmark detection achieve success by learning discriminative features from rich deformation of face shapes and poses. Besides the variance of faces themselves, the intrinsic variance of image styles, e.g., grayscale vs. color images, light vs. dark, intense vs. dull, and so on, has constantly been overlooked. This issue becomes inevitable as increasing web images are collected from various sources for training neural networks. In this work, we propose a style-aggregated approach to deal with the large intrinsic variance of image styles for facial landmark detection. Our method transforms original face images to style-aggregated images by a generative adversarial module. The proposed scheme uses the style-aggregated image to maintain face images that are more robust to environmental changes. Then the original face images accompanying with style-aggregated ones play a duet to train a landmark detector which is complementary to each other. In this way, for each face, our method takes two images as input, i.e., one in its original style and the other in the aggregated style. In experiments, we observe that the large variance of image styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image styles by comparing to a variant of our approach, in which the generative adversarial module is removed, and no style-aggregated images are used. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on benchmark datasets AFLW and 300-W. Code is publicly available on GitHub: https://github.com/D-X-Y/SAN",http://arxiv.org/pdf/1803.04108v4
41,615,Poster,Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision,"Yaojie Liu, Michigan State University; Amin Jourabloo, ; Xiaoming Liu, Michigan State University",Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision,"Face anti-spoofing is the crucial step to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. Then we fuse the estimated depth and rPPG to distinguish live vs. spoof faces. In addition, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experimental results show that our model achieves the state-of-the-art performance on both intra-database and cross-database testing.",http://arxiv.org/pdf/1803.11097v1
42,680,Poster,Deep Cost-Sensitive and Order-Preserving Feature Learning for Cross-Population Age Estimation,"Kai Li, Chinese Academy of Sciences; Junliang Xing, Institute of Automation, Chinese Academy of Sciences; Chi Su, KingSoft; Weiming Hu, ; Yundong Zhang, Vimicro Corporation; Stephen Maybank, Birkbeck University of London",,,
43,683,Poster,First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations,"Guillermo Garcia-Hernando, Imperial College London; Shanxin Yuan, Imperial College London; Seungryul Baek, Imperial College London; Tae-Kyun Kim, Imperial College London",,,
44,1114,Poster,A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking,"M. Saquib Sarfraz, KIT; Arne Schumann, KIT; Andreas Eberle, KIT; Rainer Stiefelhagen, Karlsruhe Institute of Technology",A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking,"Person re identification is a challenging retrieval task that requires matching a person's acquired image across non overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets.   The code is available online at: https://github.com/pse-ecn/pose-sensitive-embedding",http://arxiv.org/pdf/1711.10378v2
45,1230,Poster,Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment,"Amit  Kumar, University of Maryland; Rama Chellappa, University of Maryland, USA",Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment,"Heatmap regression has been used for landmark localization for quite a while now. Most of the methods use a very deep stack of bottleneck modules for heatmap classification stage, followed by heatmap regression to extract the keypoints. In this paper, we present a single dendritic CNN, termed as Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN), where a classification network is followed by a second and modular classification network, trained in an end to end fashion to obtain accurate landmark points. Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches. Extensive experimentation shows that conditioning on pose reduces the localization error by making it agnostic to face pose. The proposed model can be extended to yield variable number of landmark points and hence broadening its applicability to other datasets. Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto $15\%$ reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG.",http://arxiv.org/pdf/1802.06713v3
46,1895,Poster,A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation,"Kang Wang, RPI; Rui Zhao, Rensselaer Polytechnic Institu; Qiang Ji, RPI",,,
47,2097,Poster,MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition,"Yizhou Zhou, Univ of Scienc.&Tech.; of China; Xiaoyan Sun, Microsoft; Zheng-Jun Zha, ; Wenjun Zeng,",,,
48,3736,Poster,Learning to Estimate 3D Human Pose and Shape from a Single Color Image,"Georgios Pavlakos, ; Luyang Zhu, Peking University; Xiaowei Zhou, Zhejiang University; Kostas Daniilidis, University of Pennsylvania",,,
49,182,Poster,Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points,"Fabien Baradel, LIRIS, INSA-Lyon; Christian Wolf, INRIA, INSA-Lyon, CITI, LIRIS; Julien Mille, INSA Val de Loire; Graham Taylor, University of Guelph",Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points,"We propose a method for human activity recognition from RGB data that does not rely on any pose information during test time and does not explicitly calculate pose information internally. Instead, a visual attention module learns to predict glimpse sequences in each frame. These glimpses correspond to interest points in the scene that are relevant to the classified activities. No spatial coherence is forced on the glimpse locations, which gives the module liberty to explore different points at each frame and better optimize the process of scrutinizing visual information. Tracking and sequentially integrating this kind of unstructured data is a challenge, which we address by separating the set of glimpses from a set of recurrent tracking/recognition workers. These workers receive glimpses, jointly performing subsequent motion tracking and activity prediction. The glimpses are soft-assigned to the workers, optimizing coherence of the assignments in space, time and feature space using an external memory module. No hard decisions are taken, i.e. each glimpse point is assigned to all existing workers, albeit with different importance. Our methods outperform state-of-the-art methods on the largest human activity recognition dataset available to-date; NTU RGB+D Dataset, and on a smaller human action recognition dataset Northwestern-UCLA Multiview Action 3D Dataset.",http://arxiv.org/pdf/1802.07898v2
50,892,Poster,Context-aware Deep Feature Compression for High-speed Visual Tracking,"Jongwon Choi, ; Hyung Jin Chang, Imperial College London; Tobias Fischer, Imperial College London; Sangdoo Yun, Seoul National University; Jiyeoup Jeong, Seoul National University; kyuewang Lee, Seoul National University; Yiannis Demiris, ; Jin Choi,",Context-aware Deep Feature Compression for High-speed Visual Tracking,"We propose a new context-aware correlation filter based tracking framework to achieve both high computational speed and state-of-the-art performance among real-time trackers. The major contribution to the high computational speed lies in the proposed deep feature compression that is achieved by a context-aware scheme utilizing multiple expert auto-encoders; a context in our framework refers to the coarse category of the tracking target according to appearance patterns. In the pre-training phase, one expert auto-encoder is trained per category. In the tracking phase, the best expert auto-encoder is selected for a given target, and only this auto-encoder is used. To achieve high tracking performance with the compressed feature map, we introduce extrinsic denoising processes and a new orthogonality loss term for pre-training and fine-tuning of the expert auto-encoders. We validate the proposed context-aware framework through a number of experiments, where our method achieves a comparable performance to state-of-the-art trackers which cannot run in real-time, while running at a significantly fast speed of over 100 fps.",http://arxiv.org/pdf/1803.10537v1
51,1494,Poster,Correlation Tracking via Joint Discrimination and Reliability Learning,"Chong Sun, DalianUniversityofTechnology; Dong Wang, DUT; Huchuan Lu, Dalian University of Technology; Ming-Hsuan Yang, UC Merced",,,
52,1790,Poster,Deep PhaseNet for Video Frame Interpolation,"Simone Meyer, ETH Zurich; Abdelaziz Djelouah, The Walt Disney Company; Christopher Schroers, Disney Research Zurich; Brian McWilliams, ; Alexander Sorkine-Hornung, ; Markus Gross,",PhaseNet for Video Frame Interpolation,"Most approaches for video frame interpolation require accurate dense correspondences to synthesize an in-between frame. Therefore, they do not perform well in challenging scenarios with e.g. lighting changes or motion blur. Recent deep learning approaches that rely on kernels to represent motion can only alleviate these problems to some extent. In those cases, methods that use a per-pixel phase-based motion representation have been shown to work well. However, they are only applicable for a limited amount of motion. We propose a new approach, PhaseNet, that is designed to robustly handle challenging scenarios while also coping with larger motion. Our approach consists of a neural network decoder that directly estimates the phase decomposition of the intermediate frame. We show that this is superior to the hand-crafted heuristics previously used in phase-based methods and also compares favorably to recent deep learning based approaches for video frame interpolation on challenging datasets.",http://arxiv.org/pdf/1804.00884v1
53,569,Poster,The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation,"Pia Bideau, University of Massachusets; Aruni RoyChowdhury, University of Massachusetts; Rakesh Radhakrishnan Menon, University of Massachusetts; Erik Miller,",,,
54,1194,Poster,Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning,"Xingping Dong, Beijing Institute of Technology; Jianbing Shen, Beijing Institute of Technolog; Wenguan Wang, Beijing Institute of Technology; Yu Liu, Beijing Institute of Technology; Ling Shao, University of East Anglia; Fatih Porikli, NICTA, Australia",,,
55,1376,Poster,Scale-Transferrable Object Detection,"Peng Zhou, Sjtu; Bingbing Ni, ; Cong Geng, sjtu; jianguo Hu, Minivision; Yi Xu, Shanghai Jiao Tong University",,,
56,3502,Poster,A Prior-Less Method for Multi-Face Tracking in Unconstrained Videos,"CHUNG-CHING LIN, IBM Research; Ying Hung, Rutgers University",,,
57,1264,Poster,End-to-end Flow Correlation Tracking with Spatial-temporal Attention,"Zheng Zhu, Institute of Automation, CAS; Wei Wu, ; Wei Zou, ; Junjie Yan,",End-to-end Flow Correlation Tracking with Spatial-temporal Attention,"Discriminative correlation filters (DCF) with deep convolutional features have achieved favorable performance in recent tracking benchmarks. However, most of existing DCF trackers only consider appearance features of current frame, and hardly benefit from motion and inter-frame information. The lack of temporal information degrades the tracking performance during challenges such as partial occlusion and deformation. In this work, we focus on making use of the rich flow information in consecutive frames to improve the feature representation and the tracking accuracy. Firstly, individual components, including optical flow estimation, feature extraction, aggregation and correlation filter tracking are formulated as special layers in network. To the best of our knowledge, this is the first work to jointly train flow and tracking task in a deep learning framework. Then the historical feature maps at predefined intervals are warped and aggregated with current ones by the guiding of flow. For adaptive aggregation, we propose a novel spatial-temporal attention mechanism. Extensive experiments are performed on four challenging tracking datasets: OTB2013, OTB2015, VOT2015 and VOT2016, and the proposed method achieves superior results on these benchmarks.",http://arxiv.org/pdf/1711.01124v4
58,77,Poster,Deep Texture Manifold for Ground Terrain Recognition,"Jia Xue, Rutgers; Hang Zhang, Rutgers University; Kristin Dana,",Deep Texture Manifold for Ground Terrain Recognition,"We present a texture network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition. Recognition of ground terrain is an important task in establishing robot or vehicular control parameters, as well as for localization within an outdoor environment. The architecture of DEP integrates orderless texture details and local spatial information and the performance of DEP surpasses state-of-the-art methods for this task. The GTOS database (comprised of over 30,000 images of 40 classes of ground terrain in outdoor scenes) enables supervised recognition. For evaluation under realistic conditions, we use test images that are not from the existing GTOS dataset, but are instead from hand-held mobile phone videos of similar terrain. This new evaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground terrain such as grass, gravel, asphalt and sand. The resultant network shows excellent performance not only for GTOS-mobile, but also for more general databases (MINC and DTD). Leveraging the discriminant features learned from this network, we build a new texture manifold called DEP-manifold. We learn a parametric distribution in feature space in a fully supervised manner, which gives the distance relationship among classes and provides a means to implicitly represent ambiguous class boundaries. The source code and database are publicly available.",http://arxiv.org/pdf/1803.10896v2
59,102,Poster,Learning Superpixels with Segmentation-Aware Affinity Loss,"Wei-Chih Tu, National Taiwan University; Ming-Yu Liu, NVIDIA; Varun Jampani, NVIDIA Research; Deqing Sun, NVIDIA; Shao-Yi Chien, National Taiwan University; Ming-Hsuan Yang, UC Merced; Jan Kautz, NVIDIA",,,
60,147,Poster,Interactive Image Segmentation with Latent Diversity,"Zhuwen Li, Intel Labs; Qifeng Chen, Intel Labs; Vladlen Koltun, Intel Labs",,,
61,299,Poster,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,"Richard Zhang, UC Berkeley; Phillip Isola, UC Berkeley; Alexei Efros, UC Berkeley; Eli Shechtman, Adobe Research; Oliver Wang, Adobe",The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,"While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ""perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",http://arxiv.org/pdf/1801.03924v1
62,368,Poster,Optimizing Local Feature Descriptors for Nearest Neighbor Matching,"Kun He, Boston University; Yan Lu, ; Stan Sclaroff, Boston University",3D Scan Registration using Curvelet Features in Planetary Environments,"Topographic mapping in planetary environments relies on accurate 3D scan registration methods. However, most global registration algorithms relying on features such as FPFH and Harris-3D show poor alignment accuracy in these settings due to the poor structure of the Mars-like terrain and variable resolution, occluded, sparse range data that is hard to register without some a-priori knowledge of the environment. In this paper, we propose an alternative approach to 3D scan registration using the curvelet transform that performs multi-resolution geometric analysis to obtain a set of coefficients indexed by scale (coarsest to finest), angle and spatial position. Features are detected in the curvelet domain to take advantage of the directional selectivity of the transform. A descriptor is computed for each feature by calculating the 3D spatial histogram of the image gradients, and nearest neighbor based matching is used to calculate the feature correspondences. Correspondence rejection using Random Sample Consensus identifies inliers, and a locally optimal Singular Value Decomposition-based estimation of the rigid-body transformation aligns the laser scans given the re-projected correspondences in the metric space. Experimental results on a publicly available data-set of planetary analogue indoor facility, as well as simulated and real-world scans from Neptec Design Group's IVIGMS 3D laser rangefinder at the outdoor CSA Mars yard demonstrates improved performance over existing methods in the challenging sparse Mars-like terrain.",http://arxiv.org/pdf/1509.07075v1
63,695,Poster,Recovering Realistic Texture in Image Super-resolution by Spatial Feature Modulation,"Xintao Wang, CUHK University; Ke Yu, CUHK; Chao Dong, Sensetime Co. Ltd ; Chen-Change Loy, the Chinese University of Hong Kong",Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,"Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.",http://arxiv.org/pdf/1804.02815v1
64,899,Poster,Deep Extreme Cut: From Extreme Points to Object Segmentation,"Kevis-Kokitsi Maninis, ETH Zurich; Sergi Caelles, ETH Zurich; Jordi Pont-Tuset, ETHZ; Luc Van Gool, KTH",Deep Extreme Cut: From Extreme Points to Object Segmentation,"This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/.",http://arxiv.org/pdf/1711.09081v2
65,1446,Poster,Learning to Parse Wireframes in Images of Man-Made Environments,"Kun Huang, Shanghaitech University; Yifan Wang, ShanghaiTech University; Zihan Zhou, Penn State University; Tianjiao Ding, ; Shenghua Gao, ShanghaiTech University; Yi Ma, EECS, UC Berkeley",,,
66,1800,Poster,Occlusion-Aware  Rolling Shutter Rectification of 3D Scenes,"Subeesh Vasu, IIT Madras; Mahesh Mohan M R, IIT Madras; A.N. Rajagopalan, IIT Madras",,,
67,2102,Poster,Content-Sensitive Supervoxels via Uniform Tessellations on Video Manifolds,"Ran Yi, Tsinghua University; Yong-Jin Liu, ; Yu-Kun Lai, Cardiff University",,,
68,2451,Poster,Intrinsic Image Transformation via Scale Space Decomposition,"Lechao Cheng, ; Chengyi Zhang, Zhejiang University; Zicheng Liao,",,,
69,2834,Poster,Learned Shape-Tailored Descriptors for Segmentation,"Naeemullah Khan, KAUST; Ganesh Sundaramoorthi,",,,
70,3286,Poster,PAD-Net: Multi-Tasks Guided Prediciton-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing,"Dan Xu, ; Wanli Ouyang, The University of Sydney; Xiaogang Wang, Chinese University of Hong Kong; Nicu Sebe, University of Trento",,,
71,3629,Poster,Multi-Image Semantic Matching by Mining Consistent Features,"Qianqian Wang, Zhejiang University; Xiaowei Zhou, Zhejiang University; Kostas Daniilidis, University of Pennsylvania",Multi-Image Semantic Matching by Mining Consistent Features,"This work proposes a multi-image matching method to estimate semantic correspondences across multiple images. In contrast to the previous methods that optimize all pairwise correspondences, the proposed method identifies and matches only a sparse set of reliable features in the image collection. In this way, the proposed method is able to prune nonrepeatable features and also highly scalable to handle thousands of images. We additionally propose a low-rank constraint to ensure the geometric consistency of feature correspondences over the whole image collection. Besides the competitive performance on multi-graph matching and semantic flow benchmarks, we also demonstrate the applicability of the proposed method to reconstruct object-class models and discover object-class landmarks from images without any annotation.",http://arxiv.org/pdf/1711.07641v1
72,48,Poster,Density-aware Single Image De-raining using a Multi-stream Dense Network,"He Zhang, Rutgers; Vishal Patel,",Density-aware Single Image De-raining using a Multi-stream Dense Network,"Single image rain streak removal is an extremely challenging problem due to the presence of non-uniform rain densities in images. We present a novel density-aware multi-stream densely connected convolutional neural network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method enables the network itself to automatically determine the rain-density information and then efficiently remove the corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with different scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and used to train the proposed density-aware network. Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method. Code can be found at: https://github.com/hezhangsprinter",http://arxiv.org/pdf/1802.07412v1
73,851,Poster,Joint Cuts and Matching of Partitions in One Graph,"Tianshu Yu, Arizona State University; Junchi Yan, Shanghai Jiao Tong University; Jieyi Zhao, University of Texas Health Science Center at Houston; Baoxin Li, Arizona State University",Joint Cuts and Matching of Partitions in One Graph,"As two fundamental problems, graph cuts and graph matching have been investigated over decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their correspondence i.e. graph matching. Then we develop an optimization algorithm by updating matching and cutting alternatively, provided with theoretical analysis. The efficacy of our algorithm is verified on both synthetic dataset and real-world images containing similar regions or structures.",http://arxiv.org/pdf/1711.09584v1
74,1235,Poster,Progressive Attention Guided Recurrent Network for Salient Object Detection,"Xiaoning Zhang, Dalian University of Technolog; TIANTIAN WANG, Dalian University of Technolog; Jinqing Qi, ; Huchuan Lu, Dalian University of Technology",,,
75,1315,Poster,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,"Zheng Hui, Xidian university; Xiumei Wang, Xidian university; Xinbo Gao,",Fast and Accurate Single Image Super-Resolution via Information Distillation Network,"Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few numbers of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance.",http://arxiv.org/pdf/1803.09454v1
76,1335,Poster,Imagination-IQA: No-reference Image Quality Assessment via Adversarial Learning,"Kwan-Yee Lin, Peking University",,,
77,135,Poster,NAG: Network for Adversary Generation,"Konda Reddy Mopuri, Indian Institute of Science; Utkarsh Ojha, MNNIT Allahabad; Utsav Garg, Nanyang Technological University; Venkatesh Babu Radhakrishnan, Indian Institute of Science",NAG: Network for Adversary Generation,"Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.",http://arxiv.org/pdf/1712.03390v2
78,319,Poster,Dynamic-Structured Semantic Propagation Network,"Xiaodan Liang, Carnegie Mellon University; Hongfei Zhou, ; Eric Xing, Carnegie Mellon University",Dynamic-structured Semantic Propagation Network,"Semantic concept hierarchy is still under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into dense prediction. This lack of modeling semantic correlations also makes prior works must tune highly-specified models for each task due to the label discrepancy across datasets. It severely limits the generalization capability of segmentation models for open set concept vocabulary and annotation utilization. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph by explicitly incorporating the semantic concept hierarchy into network construction. Each neuron represents the instantiated module for recognizing a specific type of entity such as a super-class (e.g. food) or a specific concept (e.g. pizza). During training, DSSPN performs the dynamic-structured neuron computation graph by only activating a sub-graph of neurons for each image in a principled way. A dense semantic-enhanced neural block is proposed to propagate the learned knowledge of all ancestor neurons into each fine-grained child neuron for feature evolving. Another merit of such semantic explainable structure is the ability of learning a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of our DSSPN over state-of-the-art segmentation models. Moreoever, we demonstrate a universal segmentation model that is jointly trained on diverse datasets can surpass the performance of the common fine-tuning scheme for exploiting multiple domain knowledge.",http://arxiv.org/pdf/1803.06067v1
79,408,Poster,Cross-Domain Self-supervised Multi-task Feature Learning Using Synthetic Game Imagery,"Zhongzheng Ren, UC Davis; Yong Jae Lee, UC Davis",,,
80,473,Poster,A Two-Step Disentanglement Method,"Naama Hadad, Tel Aviv University; Lior Wolf, Tel Aviv University, Israel; Moni  Shahar, Tel Aviv University",A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning,"This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.",http://arxiv.org/pdf/1710.05741v2
81,978,Poster,Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network,"Daniel Merget, Technical University of Munich; Matthias Rock, TUM; Rigoll Gerhard, TUM",,,
82,1134,Poster,Decorrelated Batch Normalization,"Lei Huang, BeiHang university; Dawei Yang, University of Michigan; Bo Lang, Beihang University; Jia Deng,",Regularizing CNNs with Locally Constrained Decorrelations,"Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.",http://arxiv.org/pdf/1611.01967v2
83,1185,Poster,Learning to Sketch with Shortcut Cycle Consistency,"Jifei Song, Queen Mary, Uni. of London; Kaiyue Pang, QMUL; Yi-Zhe Song, ; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh",,,
84,1462,Poster,Towards a Mathematical Understanding of the Difficulty in Learning with Feedforward Neural Networks,"Hao Shen, Fortiss GmbH",Towards a Mathematical Understanding of the Difficulty in Learning with Feedforward Neural Networks,"Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations. This work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. By assuming exact learning of finite samples, sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. Furthermore, a state of the art algorithm, known as the Generalised Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate Newton's algorithm, which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning.",http://arxiv.org/pdf/1611.05827v3
85,2021,Poster,ID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis,"Yujun Shen, Dept. of IE, CUHK; Ping Luo, The Chinese University of Hong Kong; Junjie Yan, ; Xiaogang Wang, Chinese University of Hong Kong; Xiaoou Tang, Chinese University of Hong Kong",,,
86,2135,Poster,A Constrained Deep Neural Network for Ordinal Regression,"Yanzhu Liu, Nanyang Technological Universi; Adams  Kong, NTU Singapore ; Chi Keong Goh, Rolls-Royce Advanced Technology Centre",,,
87,2542,Poster,Modulated Convolutional Networks,"Xiaodi Wang, Beihang University; Baochang Zhang, ; Ce Li, CUMTB; Rongrong Ji, ; jungong han, ; Xianbin Cao, Beihang University; jianzhuang liu,",Merging and Evolution: Improving Convolutional Neural Networks for Mobile Applications,"Compact neural networks are inclined to exploit ""sparsely-connected"" convolutions such as depthwise convolution and group convolution for employment in mobile applications. Compared with standard ""fully-connected"" convolutions, these convolutions are more computationally economical. However, ""sparsely-connected"" convolutions block the inter-group information exchange, which induces severe performance degradation. To address this issue, we present two novel operations named merging and evolution to leverage the inter-group information. Our key idea is encoding the inter-group information with a narrow feature map, then combining the generated features with the original network for better representation. Taking advantage of the proposed operations, we then introduce the Merging-and-Evolution (ME) module, an architectural unit specifically designed for compact networks. Finally, we propose a family of compact neural networks called MENet based on ME modules. Extensive experiments on ILSVRC 2012 dataset and PASCAL VOC 2007 dataset demonstrate that MENet consistently outperforms other state-of-the-art compact networks under different computational budgets. For instance, under the computational budget of 140 MFLOPs, MENet surpasses ShuffleNet by 1% and MobileNet by 1.95% on ILSVRC 2012 top-1 accuracy, while by 2.3% and 4.1% on PASCAL VOC 2007 mAP, respectively.",http://arxiv.org/pdf/1803.09127v1
88,3214,Poster,Learning Steerable Filters for Rotation Equivariant CNNs,"Maurice Weiler, Heidelberg University; Fred Hamprecht, Heidelberg University, Germany; Martin Storath,",Learning Steerable Filters for Rotation Equivariant CNNs,"In many machine learning tasks it is desirable that a model's prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He's weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge.",http://arxiv.org/pdf/1711.07289v3
89,3409,Poster,Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++,"David Acuna, University of Toronto; Huan Ling, UofT; Amlan Kar, University of Toronto; Sanja Fidler,",Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++,"Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.",http://arxiv.org/pdf/1803.09693v1
90,3827,Poster,SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels,"Matthias Fey, TU Dortmund; Jan Lenssen, TU Dortmund; Frank Weichert, TU Dortmund; Heinrich Müller, TU Dortmund",,,
91,4208,Poster,GAGAN: Geometry Aware Generative Adverserial Networks,"Jean Kossaifi, Imperial College London; Linh Tran, Imperial College London; Yannis Panagakis, ; Maja Pantic, Imperial College London, UK",,,
92,261,Poster,On the Robustness of Semantic Segmentation Models to Adversarial Attacks,"Anurag Arnab, University of Oxford; Ondrej Miksik, University of Oxford; Phil Torr, Oxford",On the Robustness of Semantic Segmentation Models to Adversarial Attacks,"Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.",http://arxiv.org/pdf/1711.09856v2
93,618,Poster,Feedback-prop: Convolutional Neural Network Inference under Partial Evidence,"Tianlu Wang, 1994; Kota Yamaguchi, CyberAgent, Inc.; Vicente Ordonez, University of Virginia",Feedback-prop: Convolutional Neural Network Inference under Partial Evidence,"We propose an inference procedure for deep convolutional neural networks (CNNs) when partial evidence is available. Our method consists of a general feedback-based propagation approach (feedback-prop) that boosts the prediction accuracy for an arbitrary set of unknown target labels when the values for a non-overlapping arbitrary set of target labels are known. We show that existing models trained in a multi-label or multi-task setting can readily take advantage of feedback-prop without any retraining or fine-tuning. Our feedback-prop inference procedure is general, simple, reliable, and works on different challenging visual recognition tasks. We present two variants of feedback-prop based on layer-wise and residual iterative updates. We experiment using several multi-task models and show that feedback-prop is effective in all of them. Our results unveil a previously unreported but interesting dynamic property of deep CNNs. We also present an associated technical approach that takes advantage of this property for inference under partial evidence in general visual recognition tasks.",http://arxiv.org/pdf/1710.08049v2
94,1035,Poster,Super-Resolving Very Low-Resolution Face Images with Supplementary Attributes,"Xin Yu, Australian National University; Basura Fernando, ANU Canberra Australia; Richard Hartley, Australian National University Australia; Fatih Porikli, NICTA, Australia",,,
95,19,Poster,Frustum PointNets for 3D Object Detection from RGB-D Data,"Charles R. Qi, Stanford University; Wei Liu, ; Chenxia Wu, ; hao Su, ; Leonidas J. Guibas,",,,
96,165,Poster,W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection,"Yongqiang Zhang, Harbin institute of Technology/KAUST; Yancheng Bai, Kaust/Iscas; Mingli Ding, ; Yongqiang Li, ; Bernard Ghanem,",,,
97,179,Poster,3D Object Detection with Latent Support Surfaces,"Zhile Ren, Brown University; Erik Sudderth, UC Irvine",,,
98,475,Poster,Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization,"Peihua Li, ; Jiangtao Xie, ; Qilong Wang, ; Zilin Gao, Dalian University of Technology",Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization,"Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-to-end training of global covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By finetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks. The source code and network models will be available at http://www.peihuali.org/iSQRT-COV",http://arxiv.org/pdf/1712.01034v2
99,534,Poster,Recurrent Scene Parsing with Perspective Understanding in the Loop,"Shu Kong, University of California, Irvine; Charless Fowlkes, University of California, Irvine, USA",Recurrent Scene Parsing with Perspective Understanding in the Loop,"Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution. We propose a depth-aware gating module that adaptively selects the pooling field size in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby. The depth gating signal is provided by stereo disparity or estimated directly from monocular input. We integrate this depth-aware gating into a recurrent convolutional neural network to perform semantic segmentation. Our recurrent module iteratively refines the segmentation results, leveraging the depth and semantic predictions from the previous iterations.   Through extensive experiments on four popular large-scale RGB-D datasets, we demonstrate this approach achieves competitive semantic segmentation performance with a model which is substantially more compact. We carry out extensive analysis of this architecture including variants that operate on monocular RGB but use depth as side-information during training, unsupervised gating as a generic attentional mechanism, and multi-resolution gating. We find that gated pooling for joint semantic segmentation and depth yields state-of-the-art results for quantitative monocular depth estimation.",http://arxiv.org/pdf/1705.07238v2
100,620,Poster,Improving Occlusion and Hard Negative Handling for Single-Stage Object Detectors,"Junhyug Noh, Seoul National University; Soochan Lee, ; Beomsu Kim, ; Gunhee Kim, Carnegie Mellon University",,,
101,631,Poster,Learning to Act Properly: Predicting and Explaining Affordances from Images,"Ching-Yao Chuang, University of Toronto; Jiaman Li, University of Toronto; Antonio Torralba, MIT; Sanja Fidler,",Learning to Act Properly: Predicting and Explaining Affordances from Images,"We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent's actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which contains annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.",http://arxiv.org/pdf/1712.07576v1
102,649,Poster,Point-wise Convolutional Neural Networks,"Binh-Son Hua, SUTD; Khoi Tran, SUTD; Sai-Kit Yeung,",Weakly Supervised Object Detection with Pointwise Mutual Information,"In this work a novel approach for weakly supervised object detection that incorporates pointwise mutual information is presented. A fully convolutional neural network architecture is applied in which the network learns one filter per object class. The resulting feature map indicates the location of objects in an image, yielding an intuitive representation of a class activation map. While traditionally such networks are learned by a softmax or binary logistic regression (sigmoid cross-entropy loss), a learning approach based on a cosine loss is introduced. A pointwise mutual information layer is incorporated in the network in order to project predictions and ground truth presence labels in a non-categorical embedding space. Thus, the cosine loss can be employed in this non-categorical representation. Besides integrating image level annotations, it is shown how to integrate point-wise annotations using a Spatial Pyramid Pooling layer. The approach is evaluated on the VOC2012 dataset for classification, point localization and weakly supervised bounding box localization. It is shown that the combination of pointwise mutual information and a cosine loss eases the learning process and thus improves the accuracy. The integration of coarse point-wise localizations further improves the results at minimal annotation costs.",http://arxiv.org/pdf/1801.08747v1
103,948,Poster,Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification,"Weijian Deng, University of Chinese Academy; Liang Zheng, University of Texas at San Ant; GUOLIANG KANG, UTS; Yi Yang, ; Qixiang Ye, ; Jianbin Jiao,",Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification,"Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a ""learning via translation"" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation.   Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of a Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.",http://arxiv.org/pdf/1711.07027v2
104,1019,Poster,Imagine it for me: Generative Adversarial  Approach for Zero-Shot Learning from Noisy Texts,"Yizhe Zhu, ; Mohamed Elhoseiny, FAIR; Bingchen Liu, Rutgers; Ahmed Elgammal,",Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts,"Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.",http://arxiv.org/pdf/1712.01381v2
105,1219,Poster,Visual Relationship Learning with a Factorization-based Prior,"SEONG JAE HWANG, University of Wisconsin - Madison; Zirui Tao , University of Wisconsin - Madi; Vikas Singh, University of Wisconsin-Madison; Hyunwoo Kim, Amazon Lab 126; Sathya Ravi, University of Wisconsin-Madison; Maxwell Collins,",,,
106,1369,Poster,Transductive Unbiased Embedding for Zero-Shot Learning,"Jie Song, Zhejiang University; Chengchao Shen, Zhejiang University; Yezhou Yang, Arizona State University; Yang Liu, ; Mingli Song, Zhejiang University",Transductive Unbiased Embedding for Zero-Shot Learning,"Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem, in which instances of unseen (target) classes tend to be categorized as one of the seen (source) classes. So they yield poor performance after being deployed in the generalized ZSL settings. In this paper, we propose a straightforward yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate the bias problem. Our method follows the way of transductive learning, which assumes that both the labeled source images and unlabeled target images are available for training. In the semantic embedding space, the labeled source images are mapped to several fixed points specified by the source categories, and the unlabeled target images are forced to be mapped to other points specified by the target categories. Experiments conducted on AwA2, CUB and SUN datasets demonstrate that our method outperforms existing state-of-the-art approaches by a huge margin of 9.3~24.5% following generalized ZSL settings, and by a large margin of 0.2~16.2% following conventional ZSL settings.",http://arxiv.org/pdf/1803.11320v1
107,2307,Poster,Hierarchical Novelty Detection for Visual Object Recognition,"Kibok Lee, University of Michigan; Kimin Lee, KAIST; Kyle Min, University of Michigan; Yuting Zhang, University of Michigan; Jinwoo Shin, KAIST; Honglak Lee, University of Michigan, USA",Hierarchical Novelty Detection for Visual Object Recognition,"Deep neural networks have achieved impressive success in large-scale visual object recognition tasks with a predefined set of classes. However, recognizing objects of novel classes unseen during training still remains challenging. The problem of detecting such novel classes has been addressed in the literature, but most prior works have focused on providing simple binary or regressive decisions, e.g., the output would be ""known,"" ""novel,"" or corresponding confidence intervals. In this paper, we study more informative novelty detection schemes based on a hierarchical classification framework. For an object of a novel class, we aim for finding its closest super class in the hierarchical taxonomy of known classes. To this end, we propose two different approaches termed top-down and flatten methods, and their combination as well. The essential ingredients of our methods are confidence-calibrated classifiers, data relabeling, and the leave-one-out strategy for modeling novel classes under the hierarchical taxonomy. Furthermore, our method can generate a hierarchical embedding that leads to improved generalized zero-shot learning performance in combination with other commonly-used semantic embeddings.",http://arxiv.org/pdf/1804.00722v1
108,2517,Poster,Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Networks,"Long Chen, ZJU; Hanwang Zhang, Columbia University; Jun Xiao, ZJU; Wei Liu, ; Shih-Fu Chang,",Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Networks,"We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem --- semantic loss --- in the prevailing family of embedding-based ZSL, where some semantics would be discarded during training if they are non-discriminative for training classes, but could become critical for recognizing test classes. Specifically, SP-AEN prevents the semantic loss by introducing an independent visual-to-semantic space embedder which disentangles the semantic space into two subspaces for the two arguably conflicting objectives: classification and reconstruction. Through adversarial learning of the two subspaces, SP-AEN can transfer the semantics from the reconstructive subspace to the discriminative one, accomplishing the improved zero-shot recognition of unseen classes. Comparing with prior works, SP-AEN can not only improve classification but also generate photo-realistic images, demonstrating the effectiveness of semantic preservation. On four popular benchmarks: CUB, AWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art methods by an absolute performance difference of 12.2\%, 9.3\%, 4.0\%, and 3.6\% in terms of harmonic mean values",http://arxiv.org/pdf/1712.01928v2
109,2813,Poster,Learning Rich Features for Image Manipulation Detection,"Peng Zhou, University of Maryland, Colleg; Xintong Han, University of Maryland; Vlad Morariu, University of Maryland; Larry Davis, University of Maryland, USA",,,
110,2828,Poster,Human Semantic Parsing for Person Re-identification,"Mahdi Kalayeh, UCF; Emrah Basaran, ; Mubarak Shah, UCF",Human Semantic Parsing for Person Re-identification,"Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually, local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that by employing a \textit{simple} yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by ~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1.",http://arxiv.org/pdf/1804.00216v1
111,2916,Poster,Fully Convolutional Attention Network for Multimodal Reasoning,"Haoqi Fan, Carnegie Mellon University; Jiatong Zhou,",Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization,"We propose a technique for producing ""visual explanations"" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a ""stronger"" deep network from a ""weaker"" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.",http://arxiv.org/pdf/1610.02391v3
112,3392,Poster,R-FCN-3000 at 30fps: Decoupling Detection and Classification,"Bharat Singh, ; Hengduo  Li, ; Abhishek Sharma, ; Larry Davis, University of Maryland, USA",,,
113,3727,Poster,CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes,"Yuhong Li, Beijing Univ. of Posts & Tels; Xiaofan Zhang, UIUC; deming Chen, UIUC",CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes,"We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.",http://arxiv.org/pdf/1802.10062v3
114,3739,Poster,Revisiting knowledge transfer for training object class detectors,"Jasper Uijlings, Google; Stefan Popov, Google; Vitto Ferrari,",Revisiting knowledge transfer for training object class detectors,"We propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images, helped by a set of source classes with bounding-box annotations. We present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes, organized in a semantic hierarchy. This generates proposals with scores at multiple levels in the hierarchy, which we use to explore knowledge transfer over a broad range of generality, ranging from class-specific (bicycle to motorbike) to class-generic (objectness to any class). Experiments on the 200 object classes in the ILSVRC 2013 detection dataset show that our technique: (1) leads to much better performance on the target classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline which uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2) delivers target object detectors reaching 80% of the mAP of their fully supervised counterparts. (3) outperforms the best reported transfer learning results on this dataset (+41% CorLoc and +3% mAP over [18, 46], +16.2% mAP over [32]). Moreover, we also carry out several across-dataset knowledge transfer experiments [27, 24, 35] and find that (4) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5x-1.9x, establishing its general applicability.",http://arxiv.org/pdf/1708.06128v3
115,1672,Poster,Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons,"Edward Kim, ; Darryl Hannan, ; Garrett Kenyon,",Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons,"Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous in virtually all machine learning and computer vision challenges; however, advancements in CNNs have arguably reached an engineering saturation point where incremental novelty results in minor performance gains. Although there is evidence that object classification has reached human levels on narrowly defined tasks, for general applications, the biological visual system is far superior to that of any computer. Research reveals there are numerous missing components in feed-forward deep neural networks that are critical in mammalian vision. The brain does not work solely in a feed-forward fashion, but rather all of the neurons are in competition with each other; neurons are integrating information in a bottom up and top down fashion and incorporating expectation and feedback in the modeling process. Furthermore, our visual cortex is working in tandem with our parietal lobe, integrating sensory information from various modalities.   In our work, we sought to improve upon the standard feed-forward deep learning model by augmenting them with biologically inspired concepts of sparsity, top-down feedback, and lateral inhibition. We define our model as a sparse coding problem using hierarchical layers. We solve the sparse coding problem with an additional top-down feedback error driving the dynamics of the neural network. While building and observing the behavior of our model, we were fascinated that multimodal, invariant neurons naturally emerged that mimicked, ""Halle Berry neurons"" found in the human brain. Furthermore, our sparse representation of multimodal signals demonstrates qualitative and quantitative superiority to the standard feed-forward joint embedding in common vision and machine learning tasks.",http://arxiv.org/pdf/1711.07998v1
116,3993,Poster,On the convergence of PatchMatch and its variants,"Thibaud EHRET, CMLA, ENS Cachan; Pablo Arias, CMLA, ENS Cachan",,,
117,24,Poster,Rethinking the Faster R-CNN Architecture for Temporal Action Localization,"Yu-Wei Chao, University of Michigan; Sudheendra Vijayanarasimhan, Google Research; Bryan Seybold, Google Research; David Ross, Google Research; Jia Deng, ; Rahul Sukthankar, Google Research",,,
118,224,Poster,MoNet: Deep Motion Exploitation  for Video Object Segmentation,"Huaxin Xiao, Nudt; Jiashi Feng, ; Guosheng Lin, Nanyang Technological Universi; Yu  Liu, NUDT; Maojun Zhang,",,,
119,329,Poster,Video Representation Learning Using Discriminative Pooling,"Jue Wang, ANU; Anoop Cherian, ; Fatih Porikli, NICTA, Australia; Stephen Gould, Australian National University",Video Representation Learning Using Discriminative Pooling,"Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.",http://arxiv.org/pdf/1803.10628v2
120,346,Poster,Recognizing Human Actions as Evolution of Pose Estimation Maps,"Mengyuan Liu, Nanyang Technological University; Junsong Yuan,",,,
121,1036,Poster,Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding,"Dapeng Chen, CUHK; Hongsheng Li, ; Tong Xiao, The Chinese University of HK; Shuai Yi, The Chinese University of Hong Kong; Xiaogang Wang, Chinese University of Hong Kong",,,
122,1085,Poster,Mask-guided Contrastive Attention Model for Person Re-Identification,"Chunfeng Song, CASIA; Yan Huang, ; Wanli Ouyang, ; Liang Wang, unknown",,,
123,1550,Poster,Pixel-Wise Metric Learning for Blazingly Fast Video Object Segmentation,"Yuhua Chen, CVL@ETHZ; Jordi Pont-Tuset, ETHZ; Alberto Montes, ETHZ; Luc Van Gool, KTH",,,
124,431,Poster,Learning to Compare: Relation Network for Few-Shot Learning,"Flood Sung, Independent Researcher; Yongxin Yang, Queen Mary University of London; Li Zhang, Queen Mary University of London; Tao Xiang, Queen Mary University of London; Phil Torr, Oxford; Timothy Hospedales, University of Edinburgh",Learning to Compare: Relation Network for Few-Shot Learning,"We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.",http://arxiv.org/pdf/1711.06025v2
125,735,Poster,COCO-Stuff: Thing and Stuff Classes in Context,"Holger Caesar, University of Edinburgh; Jasper Uijlings, Google; Vitto Ferrari,",COCO-Stuff: Thing and Stuff Classes in Context,"Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.",http://arxiv.org/pdf/1612.03716v4
126,764,Poster,Image Generation from Scene Graphs,"Justin Johnson, Stanford University; Agrim Gupta, Stanford University; Fei-Fei Li, Stanford University",Image Generation from Scene Graphs,"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.",http://arxiv.org/pdf/1804.01622v1
127,1381,Poster,Deep Cauchy Hashing for Hamming Space Retrieval,"Yue Cao, Tsinghua University; Mingsheng Long, Tsinghua University; Bin Liu, Tsinghua University; Jianmin Wang,",,,
128,1598,Poster,Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks,"Dinesh Jayaraman, UT Austin ; Kristen Grauman,",Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks,"It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge. We address the problem of learning to look around: if a visual agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training. As a result, 1) the learned ""look around"" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments. Completion episodes are shown at https://goo.gl/BgWX3W.",http://arxiv.org/pdf/1709.00507v2
129,1620,Poster,Multi-scale Location-aware Kernel Representation for Object Detection,"Hao Wang, Harbin Institute of Technology; Qilong Wang, ; Mingqi Gao, Harbin Institute of Technology; Peihua Li, ; Wangmeng Zuo, Harbin Institute of Technology",Multi-scale Location-aware Kernel Representation for Object Detection,"Although Faster R-CNN and its variants have shown promising performance in object detection, they only exploit simple first-order representation of object proposals for final classification and regression. Recent classification methods demonstrate that the integration of high-order statistics into deep convolutional neural networks can achieve impressive improvement, but their goal is to model whole images by discarding location information so that they cannot be directly adopted to object detection. In this paper, we make an attempt to exploit high-order statistics in object detection, aiming at generating more discriminative representations for proposals to enhance the performance of detectors. To this end, we propose a novel Multi-scale Location-aware Kernel Representation (MLKP) to capture high-order statistics of deep features in proposals. Our MLKP can be efficiently computed on a modified multi-scale feature map using a low-dimensional polynomial kernel approximation.Moreover, different from existing orderless global representations based on high-order statistics, our proposed MLKP is location retentive and sensitive so that it can be flexibly adopted to object detection. Through integrating into Faster R-CNN schema, the proposed MLKP achieves very competitive performance with state-of-the-art methods, and improves Faster R-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0% (AP at IOU=[0.5:0.05:0.95]) on PASCAL VOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at: https://github.com/Hwang64/MLKP.",http://arxiv.org/pdf/1804.00428v1
130,1717,Poster,Clinical Skin Lesion Diagnosis using Representations Inspired by Dermatologist Criteria,"Jufeng Yang, Nankai University; Xiaoxiao Sun, ; Jie Liang, ; Paul Rosin,",,,
131,2318,Poster,Compare and Contrast: Learning Prominent Visual Differences,"Steven Chen, University of Texas at Austin; Kristen Grauman,",Compare and Contrast: Learning Prominent Visual Differences,"Relative attribute models can compare images in terms of all detected properties or attributes, exhaustively predicting which image is fancier, more natural, and so on without any regard to ordering. However, when humans compare images, certain differences will naturally stick out and come to mind first. These most noticeable differences, or prominent differences, are likely to be described first. In addition, many differences, although present, may not be mentioned at all. In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect instance-level annotations of most noticeable differences, and build a model trained on relative attribute features that predicts prominent differences for unseen pairs. We test our model on the challenging UT-Zap50K shoes and LFW10 faces datasets, and outperform an array of baseline methods. We then demonstrate how our prominence model improves two vision tasks, image search and description generation, enabling more natural communication between people and vision systems.",http://arxiv.org/pdf/1804.00112v1
132,2752,Poster,"Multi-Evidence Fusion and Filtering for Weakly Supervised Object Recognition, Detection and Segmentation","Weifeng Ge, The University of Hong Kong; Yizhou Yu, The University of Hong Kong","Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning","Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. In this paper, we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition, detection and semantic segmentation. In this pipeline, we first obtain intermediate object localization and pixel labeling results for the training images, and then use such results to train task-specific deep networks in a fully supervised manner. The entire process consists of four stages, including object localization in the training images, filtering and fusing object instances, pixel labeling for the training images, and task-specific network training. To obtain clean object instances in the training images, we propose a novel algorithm for filtering, fusing and classifying object instances collected from multiple solution mechanisms. In this algorithm, we incorporate both metric learning and density-based clustering to filter detected object instances. Experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on MS-COCO, PASCAL VOC 2007 and PASCAL VOC 2012.",http://arxiv.org/pdf/1802.09129v1
133,313,Poster,HashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GAN,"Yue Cao, Tsinghua University; Mingsheng Long, Tsinghua University; Bin Liu, Tsinghua University; Jianmin Wang,",,,
134,425,Poster,Min-Entropy Latent Model for Weakly Supervised Object Detection,"Fang Wan, UCAS; Pengxu Wei, ; Jianbin Jiao, ; Zhenjun Han, ; Qixiang Ye,",,,
135,594,Poster,MatNet: Modular Attention Network for Referring Expression Comprehension,"Licheng Yu, UNC Chapel Hill; Zhe Lin, Adobe Systems, Inc.; Xiaohui Shen, Adobe Research; Jimei Yang, ; Xin Lu, ; Mohit Bansal, UNC Chapel Hill; Tamara Berg, University on North carolina",,,
136,842,Poster,AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks,"Tao Xu, Lehigh University; Pengchuan Zhang, ; Qiuyuan Huang, ; Han Zhang, Rutgers; Zhe Gan, ; Xiaolei Huang, Lehigh ; Xiaodong He,",AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks,"In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",http://arxiv.org/pdf/1711.10485v1
137,927,Poster,Adversarial Complementary Learning for Weakly Supervised Object Localization,"Xiaolin Zhang, University of Technology Sydey; Yunchao Wei, ; Jiashi Feng, ; Yi Yang, ; Thomas Huang,",,,
138,961,Poster,Conditional Generative Adversarial Network for Structured Domain Adaptation,"Weixiang Hong, Nanyang Technological Universi; Zhenzhen Wang, Nanyang Technological University; Ming Yang, Horizon Robotics Inc.; Junsong Yuan, Nanyang Technological University",Generative Adversarial Learning for Spectrum Sensing,"A novel approach of training data augmentation and domain adaptation is presented to support machine learning applications for cognitive radio. Machine learning provides effective tools to automate cognitive radio functionalities by reliably extracting and learning intrinsic spectrum dynamics. However, there are two important challenges to overcome, in order to fully utilize the machine learning benefits with cognitive radios. First, machine learning requires significant amount of truthed data to capture complex channel and emitter characteristics, and train the underlying algorithm (e.g., a classifier). Second, the training data that has been identified for one spectrum environment cannot be used for another one (e.g., after channel and emitter conditions change). To address these challenges, a generative adversarial network (GAN) with deep learning structures is used to 1)~generate additional synthetic training data to improve classifier accuracy, and 2) adapt training data to spectrum dynamics. This approach is applied to spectrum sensing by assuming only limited training data without knowledge of spectrum statistics. Machine learning classifiers are trained with limited, augmented and adapted training data to detect signals. Results show that training data augmentation increases the classifier accuracy significantly and this increase is sustained with domain adaptation as spectrum conditions change.",http://arxiv.org/pdf/1804.00709v1
139,1186,Poster,GroupCap: Group-based Image Captioning with Structured Relevance and Diversity Constraints,"Fuhai Chen, Xiamen university; Rongrong Ji, ; Xiaoshuai Sun, Harbin Institute of Technology; Jinsong Su, Xiamen university",,,
140,1303,Poster,Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features,"Xiang Wang, Tsinghua University; Shaodi You, Data61; Xi Li, Tsinghua University; Huimin Ma, Tsinghua University",,,
141,1401,Poster,Bootstrapping the Performance of Webly Supervised Semantic Segmentation,"Tong Shen, The University of Adelaide; Guosheng Lin, Nanyang Technological Universi; Chunhua Shen, University of Adelaide; Ian Reid,",,,
142,1564,Poster,DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion,"Zhishuai Zhang, Johns Hopkins University; Cihang Xie, JHU; Jianyu Wang, ; Lingxi Xie, UCLA; Alan Yuille, JHU",DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion,"In this paper, we study the task of detecting semantic parts of an object, e.g., a wheel of a car, under partial occlusion. We propose that all models should be trained without seeing occlusions while being able to transfer the learned knowledge to deal with occlusions. This setting alleviates the difficulty in collecting an exponentially large dataset to cover occlusion patterns and is more essential. In this scenario, the proposal-based deep networks, like RCNN-series, often produce unsatisfactory results, because both the proposal extraction and classification stages may be confused by the irrelevant occluders. To address this, [25] proposed a voting mechanism that combines multiple local visual cues to detect semantic parts. The semantic parts can still be detected even though some visual cues are missing due to occlusions. However, this method is manually-designed, thus is hard to be optimized in an end-to-end manner.   In this paper, we present DeepVoting, which incorporates the robustness shown by [25] into a deep network, so that the whole pipeline can be jointly optimized. Specifically, it adds two layers after the intermediate features of a deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the evidence of local visual cues, and the second layer performs a voting mechanism by utilizing the spatial relationship between visual cues and semantic parts. We also propose an improved version DeepVoting+ by learning visual cues from context outside objects. In experiments, DeepVoting achieves significantly better performance than several baseline methods, including Faster-RCNN, for semantic part detection under occlusion. In addition, DeepVoting enjoys explainability as the detection results can be diagnosed via looking up the voting cues.",http://arxiv.org/pdf/1709.04577v2
143,1653,Poster,Geometry-Aware Scene Text Detection with Instance Transformation Network,"Fangfang Wang, Zhejiang University; Liming Zhao, Zhejiang University; Xi Li, Zhejiang University; Xinchao Wang, ; Dacheng Tao, University of Sydney",,,
144,2566,Poster,Optical Flow Guided Feature: A Motion Representation for Video Action Recognition,"Shuyang Sun, The University of Sydney; Zhanghui Kuang, Sense Time; Wanli Ouyang, The University of Sydney; Lu Sheng, The Chinese University of HK; Wei Zhang,",Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition,"Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatio-temporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0% accuracy on UCF-101. The code will be available online.",http://arxiv.org/pdf/1711.11152v1
145,391,Poster,Motion-Guided Cascaded Refinement Network for Video Object Segmentation,"Ping Hu, ; Gang Wang, ; Xiangfei Kong, Nanyang Technological University; Jason Kuen, NTU, Singapore; Yap-Peng Tan,",,,
146,619,Poster,A Memory Network Approach for Story-based Temporal Summarization of 360° Videos,"Sangho Lee, Seoul National University; Jinyoung Sung, Seoul National University; Youngjae Yu, ; Gunhee Kim, Carnegie Mellon University",,,
147,700,Poster,Cube Padding for Weakly-Supervised Saliency Prediction in 360$^{\circ}$ Videos,"Hsien-Tzu Cheng, National Tsing Hua University; Chun-Hung Chao, ; Jin-Dong Dong, ; Hao-Kai Wen, ; Tyng-Luh Liu, IIS/Academia Sinica; Min Sun, University of Washington",,,
148,2777,Poster,Appearance-and-Relation Networks for Video Classification,"Limin Wang, ETH Zurich; Wei Li, Google; Wen Li, ETH; Luc Van Gool, KTH",Appearance-and-Relation Networks for Video Classification,"Spatiotemporal feature learning in videos is a fundamental and difficult problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods.",http://arxiv.org/pdf/1711.09125v1
149,316,Poster,Excitation Backprop for RNNs,"Sarah Bargal, Boston University; Andrea Zunino, Istituto Italiano di Tecnologia; Donghyun Kim, Boston University; Jianming Zhang, Adobe Research; Vittorio Murino, Istituto Italiano di Tecnologia; Stan Sclaroff, Boston University",Excitation Backprop for RNNs,"Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.",http://arxiv.org/pdf/1711.06778v3
150,1037,Poster,One-shot Action Localization by Sequence Matching Network,"Hongtao  Yang, Australian National University; Xuming He, ShanghaiTech; Fatih Porikli, NICTA, Australia",,,
151,1178,Poster,Structure Preserving Video Prediction,"Xu Jingwei, Shanghai Jiao Tong University; Bingbing Ni, ; Zefan Li, Shanghai Jiaotong University; Shuo Cheng, SJTU; Xiaokang Yang,",Visual Understanding via Multi-Feature Shared Learning with Global Consistency,"Image/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing the attributes has been widely recognized. Multi-feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l_2-norm based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed for better preserving the manifold structure of each feature, such that the label prediction power is much improved through the semi-supervised learning with global label consistency. For convenience, we call the proposed approach Global-Label-Consistent Classifier (GLCC). The merits of the proposed method include: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed; 3) an efficient alternative optimization method is introduced as a fast solver owing to the convex sub-problems. Experiments on several benchmark visual datasets for multimedia understanding, such as the 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, the YouTube & Consumer Videos dataset and the large-scale NUS-WIDE dataset, demonstrate that the proposed approach compares favorably with the state-of-the-art algorithms. An extensive experiment on the deep convolutional activation features also show the effectiveness of the proposed approach. The code is available on http://www.escience.cn/people/lei/index.html",http://arxiv.org/pdf/1505.05233v2
152,1860,Poster,Person Re-identification with Cascaded Pairwise Convolutions,"Yicheng Wang, ; Zhenzhong Chen, Wuhan University; Feng Wu, ; Gang Wang,",,,
153,1110,Poster,On the Importance of Label Quality for Semantic Segmentation,"Aleksandar Zlateski, MIT; ronnachai  Jaroensri, Massachusetts Institute of Technology; Prafull Sharma, MIT; Fredo Durand,",Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs,"The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, key to navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) became popular methods for addressing this type of problem. The available software for training and the integration of CNNs in real robots, however, is quite fragmented and difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a novel framework called Bonnet, which addresses this fragmentation problem. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source framework for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides a C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.",http://arxiv.org/pdf/1802.08960v1
154,1124,Poster,Scalable and Effective Deep CCA via Soft Decorrelation,"Xiaobin Chang, Queen Mary Univ. of London; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh",Scalable and Effective Deep CCA via Soft Decorrelation,"Recently the widely used multi-view learning model, Canonical Correlation Analysis (CCA) has been generalised to the non-linear setting via deep neural networks. Existing deep CCA models typically first decorrelate the feature dimensions of each view before the different views are maximally correlated in a common latent space. This feature decorrelation is achieved by enforcing an exact decorrelation constraint; these models are thus computationally expensive due to the matrix inversion or SVD operations required for exact decorrelation at each training iteration. Furthermore, the decorrelation step is often separated from the gradient descent based optimisation, resulting in sub-optimal solutions. We propose a novel deep CCA model Soft CCA to overcome these problems. Specifically, exact decorrelation is replaced by soft decorrelation via a mini-batch based Stochastic Decorrelation Loss (SDL) to be optimised jointly with the other training objectives. Extensive experiments show that the proposed soft CCA is more effective and efficient than existing deep CCA models. In addition, our SDL loss can be applied to other deep models beyond multi-view learning, and obtains superior performance compared to existing decorrelation losses.",http://arxiv.org/pdf/1707.09669v2
155,1172,Poster,Duplex Generative Adversarial Network for Unsupervised Domain Adaptation,"Lanqing Hu, ICT, CAS; Meina Kan, ; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen,",,,
156,1288,Poster,Edit Probability for Scene Text Recognition,"Fan Bai, Fudan University; Zhanzhan Cheng, Hikvision Research Institute; Yi Niu, Hikvision Research Institute; Shiliang Pu, ; Shuigeng Zhou, Fudan University",,,
157,1552,Poster,Global versus Localized Generative Adversarial Nets,"Guo-Jun Qi, University of Central Florida; Liheng Zhang, University of Central Florida; Hao Hu, University of Central Florida",Global versus Localized Generative Adversarial Nets,"In this paper, we present a novel localized Generative Adversarial Net (GAN) to learn on the manifold of real data. Compared with the classic GAN that {\em globally} parameterizes a manifold, the Localized GAN (LGAN) uses local coordinate charts to parameterize distinct local geometry of how data points can transform at different locations on the manifold. Specifically, around each point there exists a {\em local} generator that can produce data following diverse patterns of transformations on the manifold. The locality nature of LGAN enables local generators to adapt to and directly access the local geometry without need to invert the generator in a global GAN. Furthermore, it can prevent the manifold from being locally collapsed to a dimensionally deficient tangent subspace by imposing an orthonormality prior between tangents. This provides a geometric approach to alleviating mode collapse at least locally on the manifold by imposing independence between data transformations in different tangent directions. We will also demonstrate the LGAN can be applied to train a robust classifier that prefers locally consistent classification decisions on the manifold, and the resultant regularizer is closely related with the Laplace-Beltrami operator. Our experiments show that the proposed LGANs can not only produce diverse image transformations, but also deliver superior classification performances.",http://arxiv.org/pdf/1711.06020v2
158,1622,Poster,MoCoGAN: Decomposing Motion and Content for Video Generation,"Sergey Tulyakov, ; Ming-Yu Liu, NVIDIA; Xiaodong Yang, NVIDIA; Jan Kautz, NVIDIA",MoCoGAN: Decomposing Motion and Content for Video Generation,"Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion.",http://arxiv.org/pdf/1707.04993v2
159,1640,Poster,Recurrent Residual Module for Fast Inference in Videos,"Bowen Pan, Shanghai Jiao Tong University; Wuwei Lin, Shanghai Jiao Tong University; Xiaolin Fang, Zhejiang University; Chaoqin Huang, Shanghai Jiaotong University; Bolei Zhou, Massachuate Institute of Technology; Cewu Lu, Shanghai Jiao Tong University",Recurrent Residual Module for Fast Inference in Videos,"Deep convolutional neural networks (CNNs) have made impressive progress in many video recognition tasks such as video pose estimation and video object detection. However, CNN inference on video is computationally expensive due to processing dense frames individually. In this work, we propose a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks. This framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames, to largely reduce the redundant computation. One unique property of the proposed method compared to previous work is that feature maps of each frame are precisely computed. The experiments show that, while maintaining the similar recognition performance, our RRM yields averagely 2x acceleration on the commonly used CNNs such as AlexNet, ResNet, deep compression model (thus 8-12x faster than the original dense models using the efficient inference engine), and impressively 9x acceleration on some binary networks such as XNOR-Nets (thus 500x faster than the original model). We further verify the effectiveness of the RRM on speeding up CNNs for video pose estimation and video object detection.",http://arxiv.org/pdf/1802.09723v1
160,2285,Poster,Improving Landmark Localization with Semi-Supervised Learning,"Sina Honari, University of Montreal; Pavlo Molchanov, NVIDIA Research; Jan Kautz, NVIDIA; Stephen Tyree, ; Christopher Pal, Ecole Polytechnique de Montreal; Pascal Vincent, University of Montreal",Improving Landmark Localization with Semi-Supervised Learning,"We present two techniques to improve landmark localization in images from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification or regression tasks related to the landmarks are more abundantly available. First, we propose the framework of sequential multitasking and explore it here through an architecture for landmark localization where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that these techniques, improve landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results on two toy datasets and four real datasets, with hands and faces, and report new state-of-the-art on two datasets in the wild, e.g. with only 5\% of labeled images we outperform previous state-of-the-art trained on the AFLW dataset.",http://arxiv.org/pdf/1709.01591v5
161,2313,Poster,Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated Labeled Data,"Arghya Pal, Indian Institute of Technology; Vineeth Balasubramanian, IIT Hyderabad",Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated Labeled Data,"Paucity of large curated hand-labeled training data for every domain-of-interest forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label has given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multi-task learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.",http://arxiv.org/pdf/1803.05137v1
162,2434,Poster,Stochastic Variational Inference with Gradient Linearization,"Tobias Plötz, TU Darmstadt; Anne Wannenwetsch, TU Darmstadt; Stefan Roth,",Stochastic Variational Inference with Gradient Linearization,"Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.",http://arxiv.org/pdf/1803.10586v1
163,2486,Poster,Multi-Label Zero-Shot Learning with Structured Knowledge Graphs,"Chung-Wei Lee, National Taiwan University; Wei Fang, National Taiwan University; Chih-Kuan Yeh, Carnegie Mellon University; Yu-Chiang  Frank Wang, Academia Sinica",Multi-Label Zero-Shot Learning with Structured Knowledge Graphs,"In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance. Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels. Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels. With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks. Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method.",http://arxiv.org/pdf/1711.06526v1
164,2508,Poster,"Fast, Simple, and Effective Resource-Constrained Structure Learning of Deep Networks","Ariel Gordon, Google; Elad Eban, Google; Bo Chen, Google; ofir Nachum, Google; Tien-Ju  Yang, Massachusetts Institute of Technology; Edward  Choi, Georgia Institute of Technology",,,
165,2635,Poster,Deep Adversarial Subspace Clustering,"Pan Zhou, National university of singapo; Yunqing Hou, NUS; Jiashi Feng,",,,
166,3145,Poster,Towards Human-Machine Cooperation: Evolving Active Learning with Self-supervised Process for Object Detection,"Keze Wang, ; Liang Lin, ; Xiaopeng Yan, Sun Yat-sen University; Lei Zhang, The Hong Kong Polytechnic University",,,
167,3264,Poster,Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs,"Emanuel Laude, TUM; Jan-Hendrik Lange, ; Jonas Schuepfer, ; Csaba Domokos, ; Laura Leal-Taixe, Technical University of Munich; Frank Schmidt, BCAI; Bjoern Andres, ; Daniel Cremers,",Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs,"This paper introduces a novel algorithm for transductive inference in higher-order MRFs, where the unary energies are parameterized by a variable classifier. The considered task is posed as a joint optimization problem in the continuous classifier parameters and the discrete label variables. In contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of the objective function into discrete and continuous subproblems and a novel, efficient optimization method related to ADMM. This approach preserves integrality of the discrete label variables and guarantees global convergence to a critical point. We demonstrate the advantages of our approach in several experiments including video object segmentation on the DAVIS data set and interactive image segmentation.",http://arxiv.org/pdf/1705.05020v4
168,3407,Poster,Robust Physical-World Attacks on Deep Learning Visual Classification,"Ivan Evtimov, University of Washington; Kevin Eykholt, University of Michigan; Earlence Fernandes, University of Washington; Tadayoshi Kohno, University of Washington; Bo Li, UC Berkeley; Atul Prakash, University of Michigan; Amir Rahmati, University of Michigan; Chaowei Xiao, University of Michigan; Dawn Song, UC Berkeley",,,
169,3569,Poster,Generating a Fusion Image: One' s Identity and Another's Shape,"DongGyu Joo, KAIST; Doyeon Kim, KAIST; Junmo Kim, KAIST",,,
170,1757,Poster,Learning to Promote Saliency Detectors,"Yu Zeng, Dalian University of Technology; Huchuan Lu, Dalian University of Technology; Lihe Zhang, Dalian University of Technology; Mengyang Feng, DUT, student; Ali Borji, UCF",,,
171,3225,Poster,Image Super-resolution via Dual-state Recurrent Neural Networks,"Wei Han, UIUC; Shiyu Chang, ; Ding Liu, UIUC; Michael Witbrock, ; Thomas Huang,",,,
172,3606,Poster,Deep Back-Projection Networks For Super-Resolution,"Muhammad Haris, Toyota Technological Institute; Greg Shakhnarovich, ; Norimichi Ukita, NAIST",Deep Back-Projection Networks For Super-Resolution,"The feed-forward architectures of recently proposed deep super-resolution networks learn representations of low-resolution inputs, and the non-linear mapping from those to high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and down-sampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually-connected up- and down-sampling stages each of which represents different types of image degradation and high-resolution components. We show that extending this idea to allow concatenation of features across up- and down-sampling stages (Dense DBPN) allows us to reconstruct further improve super-resolution, yielding superior results and in particular establishing new state of the art results for large scaling factors such as 8x across multiple data sets.",http://arxiv.org/pdf/1803.02735v1
173,3869,Poster,Focus Manipulation Detection via Photometric Histogram Analysis,"Can Chen, University of Delaware; Scott McCloskey, Honeywell; Jingyi Yu, University of Delaware, USA",,,
174,4265,Poster,Compassionately Conservative Balanced Cuts for Image Segmentation,"Nathan Cahill, Rochester Institute of Technol; Tyler Hayes, Rochester Institute of Tech; Renee Meinhold, Rochester Institute of Technology; John Hamilton, RIT",Compassionately Conservative Balanced Cuts for Image Segmentation,"The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the B\""uhler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained $\ell_{\tau}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.",http://arxiv.org/pdf/1803.09903v1
175,34,Poster,A High-Quality Denoising Dataset for Smartphone Cameras,"Abdelrahman Abdelhamed, York University; Stephen Lin, Microsoft Research Asia, China; Michael Brown, York University",,,
176,130,Poster,Context-aware Synthesis for Video Frame Interpolation,"Simon Niklaus, Portland State University; Feng Liu, Portland State University",Context-aware Synthesis for Video Frame Interpolation,"Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches.",http://arxiv.org/pdf/1803.10967v1
177,178,Poster,Salient Object Detection Driven by Fixation Prediction,"Wenguan Wang, Beijing Institute of Technology; Jianbing Shen, Beijing Institute of Technolog; Xingping Dong, Beijing Institute of Technology; Ali Borji, UCF",,,
178,493,Poster,Enhancing the Spatial Resolution of Stereo Images using a Parallax Prior,"Daniel S. Jeon, KAIST; Seung-Hwan Baek, KAIST; Inchang Choi, ; Min H. Kim, KAIST",,,
179,1083,Poster,HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification,"Amos Sironi, Prophesee; Manuele Brambilla, Prophesee; Nicolas Bourdis, prophesee; Xavier Lagorce, Prophesee; Ryad Benosman, Universite Pierre et Marie Curie-Paris",HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification,"Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classification algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their frame-based counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classification and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information and build a robust event-based representation. Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments, showing better classification performance and real-time computation.",http://arxiv.org/pdf/1803.07913v1
180,1366,Poster,A Bi-directional Message Passing Model for Salient Object Detection,"Lu Zhang, Dalian University of Technolog; Ju Dai, Dalian University of Technolog; Huchuan Lu, Dalian University of Technology; You He, ; Gang Wang,",,,
181,2450,Poster,Co-Occurrence Template Matching,"Shai Avidan, ; rotal kat, Tel-Aviv University; roy jevnisek, Tel-Aviv University",,,
182,2561,Poster,SeedNet : Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation,"Gwangmo Song, Seoul National University; Heesoo Myeong, Samsung; Kyoung Mu Lee,",,,
183,2767,Poster,Jerk-Aware Video Acceleration Magnification,"Shoichiro Takeda, NTT Media Intelligence Lab.; Kazuki Okami, NTT Media Intelligence Lab.; Dan Mikami, NTT Media Intelligence Lab.; Megumi Isogai, NTT Media Intelligence Lab.; Hideaki Kimata, NTT Media Intelligence Lab.",,,
184,2790,Poster,Defense against adversarial attacks using guided denoiser,"Fangzhou Liao, Tsinghua University; Ming Liang, ; Yinpeng Dong, Tsinghua Univeristy; Tianyu Pang, Tsinghua University; Jun Zhu, Tsinghua University; Xiaolin Hu, Tsinghua University",Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser,"Neural networks are vulnerable to adversarial examples. This phenomenon poses a threat to their applications in security-sensitive systems. It is thus important to develop effective defending methods to strengthen the robustness of neural networks to adversarial attacks. Many techniques have been proposed, but only a few of them are validated on large datasets like the ImageNet dataset. We propose high-level representation guided denoiser (HGD) as a defense for image classification. HGD uses a U-net structure to capture multi-scale information. It serves as a preprocessing step to remove the adversarial noise from the input, and feeds its output to the target model. To train the HGD, we define the loss function as the difference of the target model's outputs activated by the clean image and denoised image. Compared with the traditional denoiser that imposes loss function at the pixel-level, HGD is better at suppressing the influence of adversarial noise. Compared with ensemble adversarial training which is the state-of-the-art defending method, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images, which makes the training much easier on large-scale datasets. Third, HGD can be transferred to defend models other than the one guiding it. We further validated the proposed method in NIPS adversarial examples dataset and achieved state-of-the-art result.",http://arxiv.org/pdf/1712.02976v1
185,2906,Poster,Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal,"Jifeng Wang, NJUST; Xiang Li, NJUST; Jian Yang, Nanjing University of Science and Technology",Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal,"Understanding shadows from a single image spontaneously derives into two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one.",http://arxiv.org/pdf/1712.02478v1
186,3141,Poster,Image Correction via Deep Reciprocating HDR Transformation,"Xin Yang, Dalian University of Technology, City University of Hong Kong; Ke Xu, Dalian University of Technology; City University of Hong Kong; Yibing Song, Tencent AI Lab; Qiang Zhang, Dalian University of Technology; Xiaopeng Wei, Dalian University of Technology; Rynson Lau, City University of Hong Kong",,,
187,3483,Poster,IQAPP: Image Quality Assessment through Pairwise Preference,"Ekta Prashnani, UCSB; Hong Cai, University of California, Santa Barbara; Yasamin Mostofi, UCSB; Pradeep Sen, University of California, Santa Barbara",,,
188,3695,Poster,Normalized Cut Loss for Weakly Supervised CNN Segmentation,"Meng Tang, UWO; Federico Perazzi, Disney Research Zurich; Abdelaziz Djelouah, The Walt Disney Company; Yuri Boykov, University of Western Ontario; Christopher Schroers, Disney Research Zurich",Normalized Cut Loss for Weakly-supervised CNN Segmentation,"Most recent semantic segmentation methods train deep convolutional neural networks with fully annotated masks requiring pixel-accuracy for good quality training. Common weakly-supervised approaches generate full masks from partial input (e.g. scribbles or seeds) using standard interactive segmentation methods as preprocessing. But, errors in such masks result in poorer training since standard loss functions (e.g. cross-entropy) do not distinguish seeds from potentially mislabeled other pixels. Inspired by the general ideas in semi-supervised learning, we address these problems via a new principled loss function evaluating network output with criteria standard in ""shallow"" segmentation, e.g. normalized cut. Unlike prior work, the cross entropy part of our loss evaluates only seeds where labels are known while normalized cut softly evaluates consistency of all pixels. We focus on normalized cut loss where dense Gaussian kernel is efficiently implemented in linear time by fast Bilateral filtering. Our normalized cut loss approach to segmentation brings the quality of weakly-supervised training significantly closer to fully supervised methods.",http://arxiv.org/pdf/1804.01346v1
189,195,Poster,ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing,"Jian Zhang, KAUST; Bernard Ghanem,",,,
190,542,Poster,Fast End-to-End Trainable Guided Filter,"Huikai Wu, CASIA; Shuai Zheng, EBay; Junge Zhang, ; Kaiqi Huang, National Laboratory of Pattern Recognition",Fast End-to-End Trainable Guided Filter,"Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs 10-100 times faster and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks. The code is available at https://github.com/wuhuikai/DeepGuidedFilter.",http://arxiv.org/pdf/1803.05619v1
191,550,Poster,Disentangling Structure and Aesthetics for Content-aware Image Completion,"Andrew Gilbert, University of Surrey; John Collomosse, University of Surrey, UK.; Hailin Jin, ; Brian Price,",,,
192,632,Poster,Learning a Discriminative Feature Network for Semantic Segmentation,"Changqian Yu, HUST; Jingbo Wang, Peking University; Chao Peng, Megvii; Changxin Gao, HUST; Gang Yu, Face++; Nong Sang,",Adversarial Dropout Regularization,"We present a method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by fooling a special domain critic network. However, a drawback of this approach is that the critic simply labels the generated features as in-domain or not, without considering the boundaries between classes. This can lead to ambiguous features being generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), to encourage the generator to output more discriminative features for the target domain. Our key idea is to replace the critic with one that detects non-discriminative features, using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvement over the state of the art. We also show that our approach can be used to train Generative Adversarial Networks for semi-supervised learning.",http://arxiv.org/pdf/1711.01575v3
193,784,Poster,Kernelized Subspace Pooling for Deep Local Descriptors,"Xing Wei, Xi'an Jiaotong University; Yihong Gong, Xi'an Jiaotong University; Yue Zhang, IAIR,Xi'an Jiaotong University; Nanning Zheng, Xi'an Jiaotong University",,,
194,41,Poster,pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment,"Je Hyeong Hong, University of Cambridge; Christopher Zach, Toshiba Research",,,
195,95,Poster,Generative Non-Rigid Shape Completion with Graph Convolutional Autoencoders,"Or Litany, Tel Aviv University; Alex Bronstein, ; Michael Bronstein, ; Ameesh Makadia, Google Research",Deformable Shape Completion with Graph Convolutional Autoencoders,"The availability of affordable and portable depth sensors has made scanning objects and people simpler than ever. However, dealing with occlusions and missing parts is still a significant challenge. The problem of reconstructing a (possibly non-rigidly moving) 3D object from a single or multiple partial scans has received increasing attention in recent years. In this work, we propose a novel learning-based method for the completion of partial shapes. Unlike the majority of existing approaches, our method focuses on objects that can undergo non-rigid deformations. The core of our method is a variational autoencoder with graph convolutional operations that learns a latent space for complete realistic shapes. At inference, we optimize to find the representation in this latent space that best fits the generated shape to the known partial input. The completed shape exhibits a realistic appearance on the unknown part. We show promising results towards the completion of synthetic and real scans of human body and face meshes exhibiting different styles of articulation and partiality.",http://arxiv.org/pdf/1712.00268v4
196,789,Poster,Learning from Millions of 3D Scans for Large-scale 3D Face Recognition,"Syed Zulqarnain Gilani, The University of Western Aust; Ajmal Mian, UWA",Learning from Millions of 3D Scans for Large-scale 3D Face Recognition,"Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.",http://arxiv.org/pdf/1711.05942v2
197,890,Poster,CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles,"Dinesh reddy Narapureddy, Carnegie mellon university; Minh Vo, CMU; Srinivasa Narasimhan, Carnegie Mellon University",,,
198,894,Poster,Deep Material-aware Cross-spectral Stereo Matching,"Tiancheng Zhi, Carnegie Mellon University; Bernardo Pires, CMU; Martial Hebert, Carnegie Mellon University; Srinivasa Narasimhan, Carnegie Mellon University",,,
199,1213,Poster,Augmenting Crowd-Sourced 3D Reconstructions using Semantic Detections,"True Price, UNC Chapel Hill; Johannes Schönberger, ETH Zurich; Zhen Wei, University of North Carolina; Marc Pollefeys, ETH; Jan-Michael Frahm, UNC Chapel Hill",,,
200,1524,Poster,Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers,"Stephan Richter, TU Darmstadt; Stefan Roth,",,,
201,1632,Poster,Triplet-Center Loss for Multi-View 3D Object Retrieval,"Xinwei He, HUST; Yang Zhou, Huazhong University of Science and Technology; Zhichao Zhou, Huazhong University of Science and Technology; Song Bai, HUST; Xiang Bai, Huazhong University of Science and Technology",Triplet-Center Loss for Multi-View 3D Object Retrieval,"Most existing 3D object recognition algorithms focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data, while learning discriminative features with deep metric learning for 3D object retrieval is more or less neglected. In the paper, we study variants of deep metric learning losses for 3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss, are introduced which could learn more discriminative features than traditional classification loss. Then, we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared with the state-of-the-arts.",http://arxiv.org/pdf/1803.06189v1
202,1708,Poster,Learning 3D Shape Completion from Point Clouds with Weak Supervision,"David Stutz, MPI Saarbruecken; Andreas Geiger, MPI Tuebingen / ETH Zuerich",,,
203,2411,Poster,End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching,"Georgios Georgakis, George Mason University; Srikrishna Karanam, Siemens Corporate Technology; Ziyan Wu, Siemens Corporation; Jan Ernst, Siemens Corporation; Jana Kosecka, George Mason Univiversity",End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching,"Finding correspondences between images or 3D scans is at the heart of many computer vision and image retrieval applications and is often enabled by matching local keypoint descriptors. Various learning approaches have been applied in the past to different stages of the matching pipeline, considering detector, descriptor, or metric learning objectives. These objectives were typically addressed separately and most previous work has focused on image data. This paper proposes an end-to-end learning framework for keypoint detection and its representation (descriptor) for 3D depth maps or 3D scans, where the two can be jointly optimized towards task-specific objectives without a need for separate annotations. We employ a Siamese architecture augmented by a sampling layer and a novel score loss function which in turn affects the selection of region proposals. The positive and negative examples are obtained automatically by sampling corresponding region proposals based on their consistency with known 3D pose labels. Matching experiments with depth data on multiple benchmark datasets demonstrate the efficacy of the proposed approach, showing significant improvements over state-of-the-art methods.",http://arxiv.org/pdf/1802.07869v1
204,2640,Poster,"ICE-BA: Efficient, Consistent and Efficient Bundle Adjustment for Visual-Inertial SLAM","Haomin Liu, Baidu; Mingyu Chen, Baidu; Guofeng Zhang, Zhejiang University; Hujun Bao, Zhejiang University; Yingze Bao, Baidu LLC",,,
205,2819,Poster,"GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose","Zhichao Yin, Sensetime Group Limited; Jianping Shi, SenseTime","GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose","We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.",http://arxiv.org/pdf/1803.02276v2
206,3217,Poster,Radially-Distorted Conjugate Translations,"James Pritts, Czech Technical University; Zuzana Kukelova, Czech Technical University in Prague; Viktor Larsson, Lund University; Ondrej Chum, Czech Technical University in Prague",Radially-Distorted Conjugate Translations,"This paper introduces the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, plane rectification using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Grobner-basis method are stable, small and fast.   The proposed solvers are used in a RANSAC-based estimator. Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. Experiments confirm that RANSAC accurately estimates the rectification and radial distortion with very few iterations. The proposed solvers are evaluated against the state-of-the-art for affine rectification and radial distortion estimation.",http://arxiv.org/pdf/1711.11339v2
207,3454,Poster,Deep Ordinal Regression Network for Monocular Depth Estimation,"Huan Fu, The University of Sydney; Mingming Gong, ; Chaohui Wang, Université Paris-Est; Kayhan Batmanghelich, University of Pittsburgh; Dacheng Tao, University of Sydney",,,
208,3508,Poster,Analytical Modeling of Vanishing Points and Curves in Catadioptric Cameras,"Pedro Miraldo, Instituto Superior Técnico, Lisboa; Francisco Girbal Eiras, University of Oxford; Srikumar Ramalingam,",,,
209,3838,Poster,Learning Depth from Monocular Videos using Direct Methods,"Chaoyang Wang, Carnegie Mellon University; Jose Buenaposada, Universidad Rey Juan Carlos; Rui Zhu, Carnegie Mellon University; Simon Lucey,",Learning Depth from Monocular Videos using Direct Methods,"The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.",http://arxiv.org/pdf/1712.00175v1
210,4075,Poster,Salience Guided Depth Calibration for Perceptually Optimized Compressive Light Field 3D Display,"WENJUAN LIAO, NTU, Singapore",,,
211,109,Poster,MegaDepth: Learning Single-View Depth Prediction from Internet Photos,"Zhengqi Li, Cornell University; Noah Snavely, Cornell University / Google",MegaDepth: Learning Single-View Depth Prediction from Internet Photos,"Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization-not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training.",http://arxiv.org/pdf/1804.00607v1
212,409,Poster,LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image,"Chuhang Zou, UIUC; Alex Colburn, Zillow Group Inc.; Qi Shan, Zillow Group; Derek Hoiem,",LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image,"We propose an algorithm to predict room layout from a single image that generalizes across panoramas and perspective images, cuboid layouts and more general layouts (e.g. L-shape room). Our method operates directly on the panoramic image, rather than decomposing into perspective images as do recent works. Our network architecture is similar to that of RoomNet, but we show improvements due to aligning the image based on vanishing points, predicting multiple layout elements (corners, boundaries, size and translation), and fitting a constrained Manhattan layout to the resulting predictions. Our method compares well in speed and accuracy to other existing work on panoramas, achieves among the best accuracy for perspective images, and can handle both cuboid-shaped and more general Manhattan layouts.",http://arxiv.org/pdf/1803.08999v1
213,598,Poster,CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation,"Konstantinos Batsos, Stevens Institute of Technolog; Changjiang Cai, ; Philippos Mordohai, Stevens Institute of Technology",CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation,"Recently, there has been a paradigm shift in stereo matching with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based systems on these datasets has allowed them to surpass the accuracy of conventional approaches based on heuristics and assumptions. Many of these assumptions, however, had been validated extensively and hold for the majority of possible inputs. In this paper, we generate a matching volume leveraging both data with ground truth and conventional wisdom. We accomplish this by coalescing diverse evidence from a bidirectional matching process via random forest classifiers. We show that the resulting matching volume estimation method achieves similar accuracy to purely data-driven alternatives on benchmarks and that it generalizes to unseen data much better. In fact, the results we submitted to the KITTI and ETH3D benchmarks were generated using a classifier trained on the Middlebury 2014 dataset.",http://arxiv.org/pdf/1804.01967v1
214,1078,Poster,Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains,"Jiahao Pang, SenseTime Group Limited; Wenxiu Sun, SenseTime Group Limited; Chengxi Yang, SenseTime Group Limited; Jimmy Ren, SenseTime Group Limited; Ruichao Xiao, ; Jin Zeng, The Hong Kong University of Science and Technology; Liang Lin,",Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains,"Despite the recent success of stereo matching with convolutional neural networks (CNNs), it remains arduous to generalize a pre-trained deep stereo model to a novel domain. A major difficulty is to collect accurate ground-truth disparities for stereo pairs in the target domain. In this work, we propose a self-adaptation approach for CNN training, utilizing both synthetic training data (with ground-truth disparities) and stereo pairs in the new domain (without ground-truths). Our method is driven by two empirical observations. By feeding real stereo pairs of different domains to stereo models pre-trained with synthetic data, we see that: i) a pre-trained model does not generalize well to the new domain, producing artifacts at boundaries and ill-posed regions; however, ii) feeding an up-sampled stereo pair leads to a disparity map with extra details. To avoid i) while exploiting ii), we formulate an iterative optimization problem with graph Laplacian regularization. At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts. We demonstrate the effectiveness of our method in two domains: daily scenes collected by smartphone cameras, and street views captured in a driving car.",http://arxiv.org/pdf/1803.06641v1
215,229,Poster,Exploring Disentangled Feature Representation Beyond Face Identification,"Yu Liu, CUHK; Fangyin Wei, Peking University; Jing Shao, The Sensetime Group Limited; Lu Sheng, The Chinese University of HK; Junjie Yan, ; Xiaogang Wang, Chinese University of Hong Kong",,,
216,237,Poster,Learning Facial Action Units from Web Images with Scalable Weakly Supervised Clustering,"Kaili Zhao, Beijing University of Post & T; Wen-Sheng Chu, Carnegie Mellon University; Aleix  Martinez, The ohio state university",,,
217,342,Poster,Human Pose Estimation with Parsing Induced Learner,"Xuecheng Nie, National University of Singapo; Jiashi Feng, ; Yiming Zuo, Tsinghua University; Shuicheng Yan,",,,
218,468,Poster,Multi-Level Factorisation Net for Person Re-Identification,"Xiaobin Chang, Queen Mary Univ. of London; Timothy Hospedales, University of Edinburgh; Tao Xiang, Queen Mary University of London",Multi-Level Factorisation Net for Person Re-Identification,"Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.",http://arxiv.org/pdf/1803.09132v1
219,1082,Poster,Attention-aware Compositional Network for Person Re-Identification,"Jing Xu, SenseNets Technology Limited; Rui Zhao, SenseNets Technology Limited; Feng Zhu, SenseNets Technology Limited; Huaming Wang, SenseNets Technology Limited; Wanli Ouyang, The University of Sydney",,,
220,1330,Poster,Look at Boundary: A Boundary-Aware Face Alignment Algorithm,"Wayne Wu, SenseTime; Chen Qian, SenseTime; Shuo Yang, ; Quan Wang, SenseTime",,,
221,1387,Poster,Demo2Vec: Reasoning Object Affordances from Online Videos,"Te-Lin Wu, USC; Kuan Fang, Stanford University; Daniel Yang, University of Southern California; Joseph Lim, University of Southern California",,,
222,2235,Poster,Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes,"Elisabeta Marinoiu, IMAR and Lund University; Andrei Zanfir, IMAR and Lund University; Cristian Sminchisescu,",,,
223,2240,Poster,3D Human Pose Reconstruction and Action Classification in Robot Assisted Therapy of Children with Autism,"Elisabeta Marinoiu, IMAR and Lund University; Mihai Zanfir, IMAR and Lund University ; Vlad Olaru, ; Cristian Sminchisescu,",,,
224,2300,Poster,Facial Expression Recognition by De-expression Residue Learning,"Huiyuan Yang, Binghamton University-SUNY; Umur Ciftci, Binghamton University-SUNY; Lijun Yin, Binghamton University State University of New York",,,
225,2628,Poster,A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects,"Yuanlu Xu, University of California, Los Angeles; Lei Qin, Institute of Computing Technology, Chinese Academy of Sciences; Xiaobai Liu, San Diego State University; Song-Chun Zhu,",A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects,"Tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking, because the visibility of the human of interests in videos is unknown and might vary over time. In particular, it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions. In this work, we consider the visibility status of a subject as a fluent variable, whose change is mostly attributed to the subject's interaction with the surrounding, e.g., crossing behind another object, entering a building, or getting into a vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the causal-effect relations between an object's visibility fluent and its activities, and develop a probabilistic graph model to jointly reason the visibility fluent change (e.g., from visible to invisible) and track humans in videos. We formulate this joint task as an iterative search of a feasible causal graph structure that enables fast search algorithm, e.g., dynamic programming method. We apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time. Results with comparisons demonstrate that our method outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions.",http://arxiv.org/pdf/1709.05437v2
226,2848,Poster,Weakly Supervised Facial Action Unit Recognition through Adversarial Training,"Guozhu Peng, USTC; Shangfei Wang,",,,
227,3646,Poster,Non-Linear Temporal Subspace Representations for Activity Recognition,"Anoop Cherian, ; Suvrit Sra, MIT; Stephen Gould, Australian National University; Richard Hartley, Australian National University Australia",Non-Linear Temporal Subspace Representations for Activity Recognition,"Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.",http://arxiv.org/pdf/1803.11064v1
228,216,Poster,Towards Pose Invariant Face Recognition in the Wild,"Jian Zhao, NUS; Yu Cheng, Nanyang Technological University; Yan Xu, Core Technology Group, Learning & Vision, Panasonic R&D; Center Singapore; Lin Xiong, Core Technology Group, Learning & Vision, Panasonic R&D; Center Singapore; Jianshu Li, National University of Singapo; Fang Zhao, National University of Singapore; Karlekar Jayashree, Core Technology Group, Learning & Vision, Panasonic R&D; Center Singapore; Sugiri Pranata, Core Technology Group, Learning & Vision, Panasonic R&D; Center Singapore; Shengmei Shen, Core Technology Group, Learning & Vision, Panasonic R&D; Center Singapore; Junliang Xing, Institute of Automation, Chinese Academy of Sciences; Shuicheng Yan, National University of Singapore; Jiashi Feng,",Towards Large-Pose Face Frontalization in the Wild,"Despite recent advances in face recognition using deep learning, severe accuracy drops are observed for large pose variations in unconstrained environments. Learning pose-invariant features is one solution, but needs expensively labeled large-scale data and carefully designed feature learning algorithms. In this work, we focus on frontalizing faces in the wild under various head poses, including extreme profile views. We propose a novel deep 3D Morphable Model (3DMM) conditioned Face Frontalization Generative Adversarial Network (GAN), termed as FF-GAN, to generate neutral head pose face images. Our framework differs from both traditional GANs and 3DMM based modeling. Incorporating 3DMM into the GAN structure provides shape and appearance priors for fast convergence with less training data, while also supporting end-to-end training. The 3DMM-conditioned GAN employs not only the discriminator and generator loss but also a new masked symmetry loss to retain visual quality under occlusions, besides an identity loss to recover high frequency information. Experiments on face recognition, landmark localization and 3D reconstruction consistently show the advantage of our frontalization method on faces in the wild datasets.",http://arxiv.org/pdf/1704.06244v3
229,250,Poster,Unifying Identification and Context Learning for Person Recognition,"Qingqiu Huang, CUHK; Yu Xiong, CUHK; Dahua Lin, CUHK",Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification,"Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.",http://arxiv.org/pdf/1710.06555v1
230,285,Poster,Improved Human Pose Estimation through Adversarial Data Augmentation,"Zhiqiang Tang, Rutgers; Xi Peng, ; Fei Yang, facebook; Rogerio Feris, IBM; Dimitris Metaxas, Rutgers",,,
231,574,Poster,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,"Zhenhua Feng, University of Surrey; Muhammad Awais, university of surrey; Josef Kittler, ; Patrik Huber, University of Surrey; Xiaojun Wu, Jiangnan University",Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,"We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function.   To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.",http://arxiv.org/pdf/1711.06753v4
232,721,Poster,Multiple Granularity Group Interaction Prediction,"Taiping Yao, Shanghai Jiaotong University; Minsi Wang, Shanghai Jiao Tong University; Huawei Wei, Shanghai Jiao Tong University; Bingbing Ni, ; Xiaokang Yang,",Learning to Forecast Videos of Human Activity with Multi-granularity Models and Adaptive Rendering,"We propose an approach for forecasting video of complex human activity involving multiple people. Direct pixel-level prediction is too simple to handle the appearance variability in complex activities. Hence, we develop novel intermediate representations. An architecture combining a hierarchical temporal model for predicting human poses and encoder-decoder convolutional neural networks for rendering target appearances is proposed. Our hierarchical model captures interactions among people by adopting a dynamic group-based interaction mechanism. Next, our appearance rendering network encodes the targets' appearances by learning adaptive appearance filters using a fully convolutional network. Finally, these filters are placed in encoder-decoder neural networks to complete the rendering. We demonstrate that our model can generate videos that are superior to state-of-the-art methods, and can handle complex human activity scenarios in video forecasting.",http://arxiv.org/pdf/1712.01955v1
233,769,Poster,Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks,"Agrim Gupta, Stanford University; Justin Johnson, Stanford University; Fei-Fei Li, Stanford University; Silvio Savarese, ; Alexandre Alahi, EPFL",Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks,"Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.",http://arxiv.org/pdf/1803.10892v1
234,874,Poster,Deep Group-shuffling Random Walk for Person Re-identification,"Yantao Shen, CUHK; Hongsheng Li, ; Tong Xiao, The Chinese University of HK; Shuai Yi, The Chinese University of Hong Kong; Dapeng Chen, CUHK; Xiaogang Wang, Chinese University of Hong Kong",,,
235,888,Poster,Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification,"Jingya Wang, QMUL; Xiatian  Zhu, Vision Semantics Ltd.; Shaogang Gong, Queen Mary University; Wei Li, Queen Mary University of Lond",Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification,"Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identitydiscriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of-the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.",http://arxiv.org/pdf/1803.09786v1
236,908,Poster,Harmonious Attention Network for Person Re-Identication,"Wei Li, Queen Mary University of Lond; Xiatian  Zhu, Vision Semantics Ltd.; Shaogang Gong, Queen Mary University",,,
237,1177,Poster,Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks,"Shi Xuepeng, ICT; Shiguang Shan, Chinese Academy of Sciences; Meina Kan, ; Shuzhe Wu, Chinese Academy of Sciences; Xilin Chen,",,,
238,1360,Poster,Deep Regression Forests for Age Estimation,"Wei Shen, Shanghai University; Yilu Guo, Shanghai University; Yan Wang, JHU; KAI ZHAO, Nankai University; Bo Wang, HikVision USA Inc.; Alan Yuille,",Deep Regression Forests for Age Estimation,"Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free and fast-converging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.",http://arxiv.org/pdf/1712.07195v1
239,1467,Poster,Weakly-supervised Deep Convolutional Neural Network Learning for Facial Action Unit Intensity Estimation,"Yong Zhang, CASIA; Weiming Dong, ; Bao-Gang Hu, CASIA; Qiang Ji, RPI",,,
240,1493,Poster,Memory Based Online Learning of Deep Representations from Video Streams,"Federico Pernici, MICC University of Florence; federico Bartoli, Micc - University of Florence; Matteo Bruni, Micc - University of Florence; Alberto Del Bimbo, University of Florence",Memory Based Online Learning of Deep Representations from Video Streams,"We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative feature matching solution based on Reverse Nearest Neighbour and a feature forgetting strategy that detect redundant features and discard them appropriately while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information. Code will be publicly available.",http://arxiv.org/pdf/1711.07368v1
241,2457,Poster,Efficient and Deep Person Re-Identification using Multi-Level Similarity,"Yiluan Guo, SUTD; Ngai-Man Cheung,",Efficient and Deep Person Re-Identification using Multi-Level Similarity,"Person Re-Identification (ReID) requires comparing two images of person captured under different conditions. Existing work based on neural networks often computes the similarity of feature maps from one single convolutional layer. In this work, we propose an efficient, end-to-end fully convolutional Siamese network that computes the similarities at multiple levels. We demonstrate that multi-level similarity can improve the accuracy considerably using low-complexity network structures in ReID problem. Specifically, first, we use several convolutional layers to extract the features of two input images. Then, we propose Convolution Similarity Network to compute the similarity score maps for the inputs. We use spatial transformer networks (STNs) to determine spatial attention. We propose to apply efficient depth-wise convolution to compute the similarity. The proposed Convolution Similarity Networks can be inserted into different convolutional layers to extract visual similarities at different levels. Furthermore, we use an improved ranking loss to further improve the performance. Our work is the first to propose to compute visual similarities at low, middle and high levels for ReID. With extensive experiments and analysis, we demonstrate that our system, compact yet effective, can achieve competitive results with much smaller model size and computational complexity.",http://arxiv.org/pdf/1803.11353v2
242,2380,Poster,Multi-Level Fusion based 3D Object Detection from Monocular Images,"Bin Xu, ; Zhenzhong Chen, Wuhan University",,,
243,2618,Poster,A Perceptual Measure for Deep Single Image Camera Calibration,"Yannick Hold-Geoffroy, Université Laval; Kalyan Sunkavalli, Adobe Systems Inc.; Jonathan Eisenmann, Adobe Systems; Matthew Fisher, Adobe; Emiliano Gambaretto, Adobe Systems; Sunil Hadap, ; Jean-Francois Lalonde, Laval University",A Perceptual Measure for Deep Single Image Camera Calibration,"Most current single image camera calibration methods rely on specific image features or user input, and cannot be applied to natural images captured in uncontrolled settings. We propose directly inferring camera calibration parameters from a single image using a deep convolutional neural network. This network is trained using automatically generated samples from a large-scale panorama dataset, and considerably outperforms other methods, including recent deep learning-based approaches, in terms of standard L2 error. However, we argue that in many cases it is more important to consider how humans perceive errors in camera estimation. To this end, we conduct a large-scale human perception study where we ask users to judge the realism of 3D objects composited with and without ground truth camera calibration. Based on this study, we develop a new perceptual measure for camera calibration, and demonstrate that our deep calibration network outperforms other methods on this measure. Finally, we demonstrate the use of our calibration network for a number of applications including virtual object insertion, image retrieval and compositing.",http://arxiv.org/pdf/1712.01259v2
244,1497,Poster,Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks,"Wei Xiong, University of Rochester; Wenhan Luo, Tencent AI Lab; Lin Ma, Tencent AI Lab; Wei Liu, ; Jiebo Luo, University of Rochester",Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks,"Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128\times 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models.",http://arxiv.org/pdf/1709.07592v3
245,2447,Poster,Document Enhancement using Visibility Detection,"Nati Kligler, Technion; Sagi Katz, Technion; Ayellet Tal, Technion","Predicting exoplanet observability in time, contrast, separation and polarization, in scattered light","Polarimetry is one of the keys to enhanced direct imaging of exoplanets. Not only does it deliver a differential observable providing extra contrast, but when coupled with spectroscopy, it also reveals valuable information on the exoplanetary atmospheric composition. Nevertheless, angular separation and contrast ratio to the host-star make for extremely challenging observation. Producing detailed predictions for exactly how the expected signals should appear is of critical importance for the designs and observational strategies of tomorrow's telescopes. We aim at accurately determining the magnitudes and evolution of the main observational signatures for imaging an exoplanet: separation, contrast ratio to the host-star and polarization as a function of the orbital geometry and the reflectance parameters of the exoplanet. These parameters were used to construct polarized-reflectance model based on the input of orbital parameters and two albedo values. The model is able to calculate a variety of observational predictions for exoplanets at any orbital time. The inter-dependency of the three main observational criteria -separation, contrast ratio, polarization- result in a complex time-evolution of the system. They greatly affect the viability of planet observation by direct imaging. We introduce a new generic display of the main observational criteria, which enables an observer to determine whether an exoplanet is within detection limits: the Separation-POlarization-Contrast diagrams (SPOC). We explore the complex effect of orbital and albedo parameters on the visibility of an exoplanet. The code we developed is available for public use and collaborative improvement on the python package index, together with its documentation. It is another step towards a full comprehensive simulation tool for predicting and interpreting the results of future observational exoplanetary discovery campaigns.",http://arxiv.org/pdf/1505.03082v1
246,3780,Poster,A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos,"Michel Silva, Universidade de Minas Gerais; Washington Luis Ramos, Universidade Federal de Minas Gerais; João Pedro Ferreira, Universidade Federal de Minas Gerais; Felipe Chamone, Universidade Federal de Minas Gerais; Mario F Campos, Universidade Federal de Minas Gerais; Erickson Nascimento, Universidade Federal de Minas Gerais",A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos,"Thanks to the advances in the technology of low-cost digital cameras and the popularity of the self-recording culture, the amount of visual data on the Internet is going to the opposite side of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched in a computer folder or website. In this work, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem, which combined with a smoothing frame transition method accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. The experiments show that our method is able to fast-forward videos to retain as much relevant information and smoothness as the state-of-the-art techniques in less time. We also present a new 80-hour multimodal (RGB-D, IMU, and GPS) dataset of first-person videos with annotations for recorder profile, frame scene, activities, interaction, and attention.",http://arxiv.org/pdf/1802.08722v3
247,1276,Oral,Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation,"Henghui Ding, Nanyang Technological University; Xudong Jiang, Nanyang Technological University; Bing Shuai, ; Ai Qun Liu, Nanyang Technological University; Gang Wang,",,,
248,435,Oral,Deep Layer Aggregation,"Fisher Yu, UC Berkeley; Dequan Wang, UC Berkeley; Evan Shelhamer, UC Berkeley; Trevor Darrell, UC Berkeley, USA",Unsupervised Semantic-based Aggregation of Deep Convolutional Features,"In this paper, we propose a simple but effective semantic-based aggregation (SBA) method. The proposed SBA utilizes the discriminative filters of deep convolutional layers as semantic detectors. Moreover, we propose the effective unsupervised strategy to select some semantic detectors to generate the ""probabilistic proposals"", which highlight certain discriminative pattern of objects and suppress the noise of background. The final global SBA representation could then be acquired by aggregating the regional representations weighted by the selected ""probabilistic proposals"" corresponding to various semantic content. Our unsupervised SBA is easy to generalize and achieves excellent performance on various tasks. We conduct comprehensive experiments and show that our unsupervised SBA outperforms the state-of-the-art unsupervised and supervised aggregation methods on image retrieval, place recognition and cloud classification.",http://arxiv.org/pdf/1804.01422v1
249,1724,Oral,Convolutional Neural Networks with Alternately Updated Clique,"Yibo Yang, Peking Univ.; Zhisheng Zhong, ; Tiancheng Shen, ; Zhouchen Lin, Peking University, China",Convolutional Neural Networks with Alternately Updated Clique,"Improving information flow in deep networks helps to ease the training difficulties and utilize parameters more efficiently. Here we propose a new convolutional neural network architecture with alternately updated clique (CliqueNet). In contrast to prior networks, there are both forward and backward connections between any two layers in the same block. The layers are constructed as a loop and are updated alternately. The CliqueNet has some unique properties. For each layer, it is both the input and output of any other layer in the same block, so that the information flow among layers is maximized. During propagation, the newly updated layers are concatenated to re-update previously updated layer, and parameters are reused for multiple times. This recurrent feedback structure is able to bring higher level visual information back to refine low-level filters and achieve spatial attention. We analyze the features generated at different stages and observe that using refined features leads to a better result. We adopt a multi-scale feature strategy that effectively avoids the progressive growth of parameters. Experiments on image recognition datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet show that our proposed models achieve the state-of-the-art performance with fewer parameters.",http://arxiv.org/pdf/1802.10419v3
250,181,Oral,Practical Block-wise Neural Network Architecture Generation,"Zhao Zhong, Institute of Automation,CAS; Junjie Yan, ; Wei Wu, ; Jing Shao, The Sensetime Group Limited; cheng-lin Liu,",Practical Block-wise Neural Network Architecture Generation,"Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.",http://arxiv.org/pdf/1708.05552v2
251,3602,Spotlight,xUnit: Learning a Spatial Activation Function for Efficient Image Restoration,"Idan Kligvasser, Technion; Tamar Rott Shaham, Technion; Tomer Michaeli, Technion",xUnit: Learning a Spatial Activation Function for Efficient Image Restoration,"In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.",http://arxiv.org/pdf/1711.06445v3
252,630,Spotlight,Learning a Toolchain for Image Restoration,"Ke Yu, CUHK; Chao Dong, Sensetime Co. Ltd ; Chen-Change Loy, the Chinese University of Hong Kong",,,
253,2937,Spotlight,Deformation Aware Image Compression,"Tamar Rott Shaham, Technion; Tomer Michaeli, Technion",,,
254,3394,Spotlight,Distributable Consistent Multi-Graph Matching,"Nan Hu, Stanford Unviversity; Boris Thibert, ; Leonidas J. Guibas,",Distributable Consistent Multi-Graph Matching,"In this paper we propose an optimization-based framework to multiple graph matching. The framework takes as input maps computed between pairs of graphs, and outputs maps that 1) are consistent among all pairs of graphs, and 2) preserve edge connectivity between pairs of graphs. The central idea of our approach is to divide the input graph into overlapping sub-graphs and enforce consistency among sub-graphs. This leads to a distributed formulation, which is scalable to large-scale datasets. We also present an equivalence condition between this decoupled scheme and the original scheme. Experiments on both synthetic and real-world datasets show that our framework is competent against state-of-the-art global optimization-based techniques.",http://arxiv.org/pdf/1611.07191v2
255,1329,Spotlight,Residual Dense Network for Image Super-Resolution,"Yulun Zhang, Northeastern University; Yapeng Tian, University of rochester; Yu Kong, Northeastern University; Bineng Zhong, Huaqiao University; Yun Fu, Northeastern University",Residual Dense Network for Image Super-Resolution,"A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.",http://arxiv.org/pdf/1802.08797v2
256,1520,Spotlight,Attentive Generative Adversarial Network for Raindrop Removal from A Single Image,"Rui Qian, Peking University; Robby Tan, Yale-NUS College Also, Electrical and Computer Engineering, NUS; Wenhan Yang, Peking University; Jiajun Su, Peking University; Jiaying Liu, Peking University",Attentive Generative Adversarial Network for Raindrop Removal from a Single Image,"Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively.",http://arxiv.org/pdf/1711.10098v3
257,1859,Spotlight,FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors,"Yu Chen, NUST; Ying Tai, Tencent; Xiaoming Liu, Michigan State University; Chunhua Shen, University of Adelaide; Jian Yang, Nanjing University of Science and Technology",FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors,"Face Super-Resolution (SR) is a domain-specific super-resolution problem. The specific facial prior knowledge could be leveraged for better super-resolving face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes full use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To further generate realistic faces, we propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Moreover, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive benchmark experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively. Code will be made available upon publication.",http://arxiv.org/pdf/1711.10703v1
258,3761,Spotlight,Burst Denoising with Kernel Prediction Networks,"Ben Mildenhall, UC Berkeley; Jiawen Chen, Google; Jonathan Barron, Google; Robert Carroll, Google; Dillon Sharlet, ; Ren Ng, Berkeley",Burst Denoising with Kernel Prediction Networks,"We present a technique for jointly denoising bursts of images taken from a handheld camera. In particular, we propose a convolutional neural network architecture for predicting spatially varying kernels that can both align and denoise frames, a synthetic data generation approach based on a realistic noise formation model, and an optimization guided by an annealed loss function to avoid undesirable local minima. Our model matches or outperforms the state-of-the-art across a wide range of noise levels on both real and synthetic data.",http://arxiv.org/pdf/1712.02327v2
259,4304,Spotlight,Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution,"Ying Qu, The University of Tennessee; Hairong Qi, University of Tennessee; Chiman Kwan,",,,
260,1193,Spotlight,Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks,"Jiawei Zhang, City University of Hong Kong; Jinshan Pan, UC Merced; Jimmy Ren, SenseTime Group Limited; Yibing Song, Tencent AI Lab; Linchao Bao, Tencent AI Lab; Rynson Lau, City University of Hong Kong; Ming-Hsuan Yang, UC Merced",,,
261,326,Oral,SPLATNet: Sparse Lattice Networks for Point Cloud Processing,"Hang Su, University of Massachusetts, Amherst; Varun Jampani, NVIDIA Research; Deqing Sun, NVIDIA; Evangelos Kalogerakis, UMass; Subhransu Maji, ; Ming-Hsuan Yang, UC Merced; Jan Kautz, NVIDIA",SPLATNet: Sparse Lattice Networks for Point Cloud Processing,"We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.",http://arxiv.org/pdf/1802.08275v3
262,3318,Oral,Surface Networks,"Ilya Kostrikov, NYU; Joan Bruna, New York University; Daniele Panozzo, NYU; Denis Zorin, NYU",Factor Models for High-Dimensional Dynamic Networks: with Application to International Trade Flow Time Series 1981-2015,"Dynamic network analysis has found an increasing interest in the literature because of the importance of different kinds of dynamic social networks, biological networks, and economic networks. Most available probability and statistical models for dynamic network data are deduced from random graph theory where the networks are characterized on the node and edge level. They are often very restrictive for applications and unscalable to high-dimensional dynamic network data which is very common nowadays. In this paper, we take a different perspective: The evolving sequence of networks are treated as a time series of network matrices. We adopt a matrix factor model where the observed surface dynamic network is assumed to be driven by a latent dynamic network with lower dimensions. The linear relationship between the surface network and the latent network is characterized by unknown but deterministic loading matrices. The latent network and the corresponding loadings are estimated via an eigenanalysis of a positive definite matrix constructed from the auto-cross-covariances of the network times series, thus capturing the dynamics presenting in the network. The proposed method is able to unveil the latent dynamic structure and achieve the objective of dimension reduction. Different from other dynamic network analytical methods that build on latent variables, our approach imposes neither any distributional assumptions on the underlying network nor any parametric forms on its covariance function. The latent network is learned directly from the data with little subjective input. We applied the proposed method to the monthly international trade flow data from 1981 to 2015. The results unveil an interesting evolution of the latent trading network and the relations between the latent entities and the countries.",http://arxiv.org/pdf/1710.06325v1
263,3099,Oral,Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250Hz,"Ayush Tewari, MPI Informatics; Michael Zollhöfer, MPI Informatics; Pablo Garrido, ; Florian Bernard, ; Hyeongwoo Kim, MPII; Patrick Perez, Technicolor Research; Christian Theobalt, MPI Informatics",,,
264,3124,Oral,"CodeSLAM --- Learning a Compact, Optimisable Representation for Dense Visual SLAM","Michael Bloesch, Imperial College London; Jan Czarnowski, Imperial College London; Ronald Clark, Imperial College London; Stefan Leutenegger, Imperial College London; Andrew Davison, Imperial College London UK","CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM","The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.   We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM.",http://arxiv.org/pdf/1804.00874v1
265,967,Spotlight,SGPN: Similarity Group Proposal Network for 3D  Point Cloud Instance Segmentation,"Weiyue Wang, USC; Ronald Yu, ; Qiangui Huang, U of Southern CA; Ulrich Neumann, USC",SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation,"We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.",http://arxiv.org/pdf/1711.08588v1
266,1584,Spotlight,PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image,"Chen Liu, WUSTL; Jimei Yang, ; Duygu Ceylan, ; Ersin Yumer, Argo AI; Yasutaka Furukawa,",,,
267,2565,Spotlight,Deep Parametric Continuous Convolutional Neural Networks,"Shenlong Wang, ; Shun Da Suo, ; Wei-Chiu Ma, MIT; Raquel Urtasun, University of Toronto",SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels,"We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. As a main advantage, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence.",http://arxiv.org/pdf/1711.08920v1
268,2589,Spotlight,FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis,"Nitika Verma, INRIA; Edmond Boyer, ; Jakob Verbeek,",FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis,"Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.",http://arxiv.org/pdf/1706.05206v2
269,3025,Spotlight,Image Collection Pop-up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories,"Antonio Agudo, IRI (CSIC-UPC); Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC)",,,
270,168,Spotlight,MapNet: Geometry-Aware Learning of Maps for Camera Localization,"Samarth Brahmbhatt, Georgia Tech; Jinwei Gu, NVIDIA; Kihwan Kim, NVIDIA Research; James Hays, Georgia Tech; Jan Kautz, NVIDIA",Geometry-Aware Learning of Maps for Camera Localization,"Maps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes dataset and the outdoor Oxford RobotCar dataset show significant performance improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au.",http://arxiv.org/pdf/1712.03342v3
271,2058,Spotlight,Recurrent Slice Networks for 3D Segmentation on Point Clouds,"Qiangui Huang, U of Southern CA; Weiyue Wang, USC; Ulrich Neumann, USC",Recurrent Slice Networks for 3D Segmentation of Point Clouds,"Point clouds are an efficient data format for 3D data. However, existing 3D segmentation methods for point clouds either do not model local dependencies \cite{pointnet} or require added computations \cite{kd-net,pointnet2}. This work presents a novel 3D segmentation framework, RSNet\footnote{Codes are released here https://github.com/qianguih/RSNet}, to efficiently model local structures in point clouds. The key component of the RSNet is a lightweight local dependency module. It is a combination of a novel slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice pooling layer is designed to project features of unordered points onto an ordered sequence of feature vectors so that traditional end-to-end learning algorithms (RNNs) can be applied. The performance of RSNet is validated by comprehensive experiments on the S3DIS\cite{stanford}, ScanNet\cite{scannet}, and ShapeNet \cite{shapenet} datasets. In its simplest form, RSNets surpass all previous state-of-the-art methods on these benchmarks. And comparisons against previous state-of-the-art methods \cite{pointnet, pointnet2} demonstrate the efficiency of RSNets.",http://arxiv.org/pdf/1802.04402v2
272,155,Spotlight,3D Hand Pose Estimation: From Current Achievements to Future Goals,"Shanxin Yuan, Imperial College London; Guillermo Garcia-Hernando, Imperial College London; Bjorn Stenger, ; Tae-Kyun Kim, Imperial College London; Gyeongsik Moon, Seoul National University; Ju Yong Chang, Kwangwoon University; Kyoung Mu Lee, ; Pavlo Molchanov, NVIDIA Research; Liuhao Ge, NTU; Junsong Yuan, Nanyang Technological University; Xinghao Chen, Tsinghua University; Guijin Wang, Tsinghua University; Fan Yang, Nara institute of science and technology; Kai Akiyama, Nara Institute of Science and Technology; Yang Wu, Nara Institute of Science and Technology; Qingfu Wan, Fudan University; Meysam Madadi, Autonomus University of Barcelona and Computer Vision Center, Barcelona, Spain; Sergio Escalera, University of Barcelona; Shile Li, Technical University of Munich; Dongheui Lee, Technical University of Munich; Iason Oikonomidis, FORTH; Antonis Argyros, FORTH",Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals,"In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints.",http://arxiv.org/pdf/1712.03917v2
273,1101,Spotlight,SobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-rigid Motion,"Miroslava Slavcheva, Siemens AG; Maximilian Baust, TUM; Slobodan Ilic, Siemens AG",,,
274,2583,Spotlight,AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation,"Jogendra Kundu, Indian Institute of Science; Phani Krishna Uppala, Indian Institute of Science; Anuj Pahuja, Indian Institute of Science; Venkatesh Babu Radhakrishnan, Indian Institute of Science",AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation,"Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.",http://arxiv.org/pdf/1803.01599v1
275,1453,Oral,Learning to Find Good Correspondences,"Kwang Moo Yi, EPFL; Eduard Trulls, ; Yuki Ono, Sony; Vincent Lepetit, TU Graz; Mathieu Salzmann, EPFL; Pascal Fua,",The Advantage of Doubling: A Deep Reinforcement Learning Approach to Studying the Double Team in the NBA,"During the 2017 NBA playoffs, Celtics coach Brad Stevens was faced with a difficult decision when defending against the Cavaliers: ""Do you double and risk giving up easy shots, or stay at home and do the best you can?"" It's a tough call, but finding a good defensive strategy that effectively incorporates doubling can make all the difference in the NBA. In this paper, we analyze double teaming in the NBA, quantifying the trade-off between risk and reward. Using player trajectory data pertaining to over 643,000 possessions, we identified when the ball handler was double teamed. Given these data and the corresponding outcome (i.e., was the defense successful), we used deep reinforcement learning to estimate the quality of the defensive actions. We present qualitative and quantitative results summarizing our learned defensive strategy for defending. We show that our policy value estimates are predictive of points per possession and win percentage. Overall, the proposed framework represents a step toward a more comprehensive understanding of defensive strategies in the NBA.",http://arxiv.org/pdf/1803.02940v1
276,3289,Oral,OATM: Occlusion Aware Template Matching by Consensus Set Maximization,"Simon Korman, Weizmann Institute; Mark Milam, NGC; Stefano Soatto, UCLA",OATM: Occlusion Aware Template Matching by Consensus Set Maximization,"We present a novel approach to template matching that is efficient, can handle partial occlusions, and comes with provable performance guarantees. A key component of the method is a reduction that transforms the problem of searching a nearest neighbor among $N$ high-dimensional vectors, to searching neighbors among two sets of order $\sqrt{N}$ vectors, which can be found efficiently using range search techniques. This allows for a quadratic improvement in search complexity, and makes the method scalable in handling large search spaces. The second contribution is a hashing scheme based on consensus set maximization, which allows us to handle occlusions. The resulting scheme can be seen as a randomized hypothesize-and-test algorithm, which is equipped with guarantees regarding the number of iterations required for obtaining an optimal solution with high probability. The predicted matching rates are validated empirically and the algorithm shows a significant improvement over the state-of-the-art in both speed and robustness to occlusions.",http://arxiv.org/pdf/1804.02638v1
277,1830,Oral,Deep Learning of Graph Matching,"Andrei Zanfir, IMAR and Lund University; Cristian Sminchisescu,",Linking ImageNet WordNet Synsets with Wikidata,The linkage of ImageNet WordNet synsets to Wikidata items will leverage deep learning algorithm with access to a rich multilingual knowledge graph. Here I will describe our on-going efforts in linking the two resources and issues faced in matching the Wikidata and WordNet knowledge graphs. I show an example on how the linkage can be used in a deep learning setting with real-time image classification and labeling in a non-English language and discuss what opportunities lies ahead.,http://arxiv.org/pdf/1803.04349v1
278,932,Oral,Unsupervised Discovery of Object Landmarks as Structural Representations,"Yuting Zhang, University of Michigan; Yijie Guo, University of Michigan; Yixin Jin, ; Yijun Luo, University of Michigan; Zhiyuan He, University of Michigan; Honglak Lee, University of Michigan, USA",,,
279,777,Spotlight,Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,"Benoit Jacob, Google; Skirmantas Kligys, Google; Bo Chen, Google; Matthew Tang, Google; Menglong Zhu, ; Andrew Howard, Google; Dmitry Kalenichenko, Google; Hartwig Adam, Google",Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,"The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.",http://arxiv.org/pdf/1712.05877v1
280,1324,Spotlight,Lean Multiclass Crowdsourcing,"Grant van Horn, California Institute of Technology; Pietro Perona, California Institute of Technology, USA; Serge Belongie,",,,
281,1363,Spotlight,Partial Transfer Learning with Selective Adversarial Networks,"Zhangjie Cao, Tsinghua University; Mingsheng Long, Tsinghua University; Jianmin Wang,",Partial Transfer Learning with Selective Adversarial Networks,"Adversarial learning has been successfully embedded into deep networks to learn transferable features, which reduce distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully shared label space across domains. In the presence of big data, there is strong motivation of transferring both classification and representation models from existing big domains to unknown small domains. This paper introduces partial transfer learning, which relaxes the shared label space assumption to that the target label space is only a subspace of the source label space. Previous methods typically match the whole source domain to the target domain, which are prone to negative transfer for the partial transfer problem. We present Selective Adversarial Network (SAN), which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Experiments demonstrate that our models exceed state-of-the-art results for partial transfer learning tasks on several benchmark datasets.",http://arxiv.org/pdf/1707.07901v1
282,2485,Spotlight,Self-Supervised Feature Learning by Learning to Spot Artifacts,"Simon Jenni, Universität Bern; Paolo Favaro, Bern University, Switzerland",,,
283,3254,Spotlight,LDMNet: Low Dimensional Manifold Regularized Neural Networks,"Wei Zhu, Duke University; Qiang Qiu, ; Jiaji Huang, Baidu Silicon Valley AI Lab; Robert Calderbank, Duke University; Guillermo Sapiro, Duke; Ingrid Daubechies, Duke University",LDMNet: Low Dimensional Manifold Regularized Neural Networks,"Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent, and their efficacy is often limited when the training set is very small. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly and often requires human input of tangent vectors. These methods typically only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose a new framework, the Low-Dimensional-Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. We demonstrate two benefits of LDMNet in the experiments. First, we show that LDMNet significantly outperforms widely-used network regularizers such as weight decay and DropOut. Second, we show that LDMNet can be designed to extract common features of an object imaged via different modalities, which proves to be very useful in real-world applications such as cross-spectral face recognition.",http://arxiv.org/pdf/1711.06246v1
284,3656,Spotlight,CondenseNet: An Efficient DenseNet using Learned Group Convolutions,"Gao Huang, ; Shichen Liu, Tsinghua University; Laurens van der Maaten, Facebook; Kilian Weinberger, Cornell University",CondenseNet: An Efficient DenseNet using Learned Group Convolutions,"Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity between layers with a mechanism to remove unused connections. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard grouped convolutions - allowing for efficient computation in practice. Our experiments demonstrate that CondenseNets are much more efficient than stateof-the-art compact convolutional networks such as MobileNets and ShuffleNets.",http://arxiv.org/pdf/1711.09224v1
285,3673,Spotlight,Learning Deep Descriptors with Scale-Aware Triplet Networks,"Michel Keller, ETH Zürich; Zetao Chen, ETH Zurich; Fabiola Maffra, ETH Zürich; Patrik Schmuck, ETH Zurich; Margarita Chli, ETH Zurich",,,
286,3916,Spotlight,Unsupervised Person Image Synthesis in Arbitrary Poses,"Albert Pumarola, IRI (CSIC-UPC); Antonio Agudo, IRI (CSIC-UPC); Alberto Sanfeliu, IRI (CSIC-UPC); Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC)",,,
287,4133,Spotlight,Decoupled Networks,"Weiyang Liu, Georgia Tech; Zhen Liu, ; Zhiding Yu, Carnegie Mellon University; Bo Dai, ; Yisen Wang, Tsinghua University; Thomas Breuel, ; James Rehg, Georgia Institute of Technology; Jan Kautz, NVIDIA; Le Song, Georgia Institute of Technology",TCP Decoupling for Next Generation Communication System,"In traditional networks, interfaces of network nodes are duplex. But, emerging communication technologies such as visible light communication, millimeter-wave communications, can only provide a unidirectional interface when cost is limited. It's urgent to find effective solutions to utilize such new unidirectional communication skills. Decoupling implies separating one single resource to two independent resources. This idea can be applied at physical layer, link layer, network layer, even transport layer. TCP decoupling is an end to end solution provided at transport layer. With decoupled TCP, two distinct unidirectional path can be created to meet the requirements of reliable information transfer. However, it is not an easy task to decouple a bidirectional logical path at transport layer. In this paper, we dwell on the idea of TCP decoupling. Advantages of decoupling at transport layer are analyzed also. In addition, an experiment is carried out to figure out how to implement a decouple TCP. Our results show decoupling at transport layer is possible and the modified protocol is available.",http://arxiv.org/pdf/1804.02723v1
288,350,Spotlight,Deep Adversarial Metric Learning,"Yueqi Duan, Tsinghua University; Wenzhao Zheng, Tsinghua University; Xudong Lin, Tsinghua University; Jiwen Lu, Tsinghua University; Jie Zhou,",Attacking the Madry Defense Model with $L_1$-based Adversarial Examples,"The Madry Lab recently hosted a competition designed to test the robustness of their adversarially trained MNIST model. Attacks were constrained to perturb each pixel of the input image by a scaled maximal $L_\infty$ distortion $\epsilon$ = 0.3. This discourages the use of attacks which are not optimized on the $L_\infty$ distortion metric. Our experimental results demonstrate that by relaxing the $L_\infty$ constraint of the competition, the elastic-net attack to deep neural networks (EAD) can generate transferable adversarial examples which, despite their high average $L_\infty$ distortion, have minimal visual distortion. These results call into question the use of $L_\infty$ as a sole measure for visual distortion, and further demonstrate the power of EAD at generating robust adversarial examples.",http://arxiv.org/pdf/1710.10733v3
289,1248,Poster,PU-Net: Point Cloud Upsampling Network,"Lequan Yu, The Chinese University of Hong; XIANZHI LI, CUHK; Chi-Wing Fu, ; Daniel  Cohen-Or, ; Pheng-Ann Heng,",,,
290,1308,Poster,Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer,"Amir Atapour-Abarghouei, Durham University; Toby Breckon, Durham University",,,
291,2403,Poster,Learning Deep Correspondence through Prior and Posterior Feature Constancy,"Zhengfa Liang, NUDT; Yiliu Feng, NUDT; Yulan Guo, NUDT; Hengzhu Liu, NUDT; Wei Chen, ; Linbo Qiao, ; Li Zhou, NUDT; Jianfeng Zhang, NUDT",,,
292,2703,Poster,DeepMVS: Learning Multi-View Stereopsis,"Po-Han Huang, University of Illinois, U-C; Kevin Matzen, Facebook; Johannes Kopf, Facebook; Narendra Ahuja, University of Illinois at Urbana-Champaign, USA; Jia-Bin Huang, Virginia Tech",DeepMVS: Learning Multi-view Stereopsis,"We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures.",http://arxiv.org/pdf/1804.00650v1
293,2967,Poster,Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View,"Albert Pumarola, IRI (CSIC-UPC); Antonio Agudo, IRI (CSIC-UPC); Lorenzo Porzi, Mapillary Research; Alberto Sanfeliu, IRI (CSIC-UPC); Vincent Lepetit, University of Bordeaux; Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC)",,,
294,3446,Poster,Self-calibrating polarising radiometric calibration,"Daniel Teo, SUTD; Boxin Shi, Peking University; Yinqiang Zheng, National Institute of Informatics, Japan; Sai-Kit Yeung,",,,
295,3531,Poster,Coding Kendall's Shape Trajectories for 3D Action Recognition,"Amor Ben Tanfous, IMT Lille Douai; Hassen Drira, IMT Lille Douai; Boulbaba Ben Amor, IMT Lille Douai",,,
296,3870,Poster,"Efficient, sparse representation of manifold distance matrices for classical scaling","Alexander Huth, University of Texas at Austin; Javier Turek, Intel Corporation","Efficient, sparse representation of manifold distance matrices for classical scaling","Geodesic distance matrices can reveal shape properties that are largely invariant to non-rigid deformations, and thus are often used to analyze and represent 3-D shapes. However, these matrices grow quadratically with the number of points. Thus for large point sets it is common to use a low-rank approximation to the distance matrix, which fits in memory and can be efficiently analyzed using methods such as multidimensional scaling (MDS). In this paper we present a novel sparse method for efficiently representing geodesic distance matrices using biharmonic interpolation. This method exploits knowledge of the data manifold to learn a sparse interpolation operator that approximates distances using a subset of points. We show that our method is 2x faster and uses 20x less memory than current leading methods for solving MDS on large point sets, with similar quality. This enables analyses of large point sets that were previously infeasible.",http://arxiv.org/pdf/1705.10887v2
297,4172,Poster,Motion Segmentation by Exploiting Complementary Geometric Models,"Xun Xu, National University of Singapore; Loong Fah Cheong, National University of Singapore; Zhuwen Li, Intel Labs",Motion Segmentation by Exploiting Complementary Geometric Models,"Many real-world sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-view spectral clustering framework that synergistically combines multiple models together. We show that the performance can be substantially improved in this way. We perform extensive testing on existing motion segmentation datasets, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.",http://arxiv.org/pdf/1804.02142v1
298,4239,Poster,"Estimation of Camera Locations in Highly Corrupted Scenarios: All About the Base, No Shape Trouble","Yunpeng Shi, University of Minnesota; Gilad Lerman, University of Minnesota","Estimation of Camera Locations in Highly Corrupted Scenarios: All About that Base, No Shape Trouble","We propose a strategy for improving camera location estimation in structure from motion. Our setting assumes highly corrupted pairwise directions (i.e., normalized relative location vectors), so there is a clear room for improving current state-of-the-art solutions for this problem. Our strategy identifies severely corrupted pairwise directions by using a geometric consistency condition. It then selects a cleaner set of pairwise directions as a preprocessing step for common solvers. We theoretically guarantee the successful performance of a basic version of our strategy under a synthetic corruption model. Numerical results on artificial and real data demonstrate the significant improvement obtained by our strategy.",http://arxiv.org/pdf/1804.02591v1
299,345,Poster,4D Human Body Correspondences from Panoramic Depth Maps,"Zhong Li, University of Delaware; Minye Wu, ShanghaiTech; Wangyiteng Zhou, ShanghaiTech University; Jingyi Yu, University of Delaware, USA",,,
300,491,Poster,Reconstructing Thin Structures of Manifold Surfaces by Integrating Spatial Curves,"Shiwei Li, HKUST; Yao Yao, HKUST; Tian Fang, HKUST; Long Quan, The Hong Kong University of  Science and Technology, Hong Kong",,,
301,759,Poster,Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction,"Shubham Tulsiani, UC Berkeley; Alexei Efros, UC Berkeley; Jitendra Malik,",Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction,We present a framework for learning single-view shape and pose prediction without using direct supervision for either. Our approach allows leveraging multi-view observations from unknown poses as supervisory signal during training. Our proposed training setup enforces geometric consistency between the independently predicted shape and pose from two views of the same instance. We consequently learn to predict shape in an emergent canonical (view-agnostic) frame along with a corresponding pose predictor. We show empirical and qualitative results using the ShapeNet dataset and observe encouragingly competitive performance to previous techniques which rely on stronger forms of supervision. We also demonstrate the applicability of our framework in a realistic setting which is beyond the scope of existing techniques: using a training dataset comprised of online product images where the underlying shape and pose are unknown.,http://arxiv.org/pdf/1801.03910v1
302,1443,Poster,Probabilistic Plant Modeling via Multi-View Image-to-Image Translation,"Takahiro Isokane, Osaka university; Fumio Okura, Osaka University; Ayaka Ide, Osaka University; Yasuyuki Matsushita, Osaka University; Yasushi Yagi, Osaka University",,,
303,1704,Poster,Deep Marching Cubes: Learning Explicit Surface Representations,"Yiyi Liao, Zhejiang University; Simon Donné, Ghent University; Andreas Geiger, MPI Tuebingen / ETH Zuerich",,,
304,1893,Poster,Tags2Parts: Discovering Semantic Regions from Shape Tags,"Sanjeev Muralikrishnan, IIT Bombay; Vladimir Kim, Adobe Research; Siddhartha Chaudhuri, IIT Bombay",Tags2Parts: Discovering Semantic Regions from Shape Tags,"We propose a novel method for discovering shape regions that strongly correlate with user-prescribed tags. For example, given a collection of chairs tagged as either ""has armrest"" or ""lacks armrest"", our system correctly highlights the armrest regions as the main distinctive parts between the two chair types. To obtain point-wise predictions from shape-wise tags we develop a novel neural network architecture that is trained with tag classification loss, but is designed to rely on segmentation to predict the tag. Our network is inspired by U-Net, but we replicate shallow U structures several times with new skip connections and pooling layers, and call the resulting architecture ""WU-Net"". We test our method on segmentation benchmarks and show that even with weak supervision of whole shape tags, our method can infer meaningful semantic regions, without ever observing shape segmentations. Further, once trained, the model can process shapes for which the tag is entirely unknown. As a bonus, our architecture is directly operational under full supervision and performs strongly on standard benchmarks. We validate our method through experiments with many variant architectures and prior baselines, and demonstrate several applications.",http://arxiv.org/pdf/1708.06673v2
305,1990,Poster,Uncalibrated Photometric Stereo under Natural Illumination,"Zhipeng Mo, ; Boxin Shi, Peking University; Feng Lu, U. Tokyo; Sai-Kit Yeung, ; Yasuyuki Matsushita, Osaka University",,,
306,2443,Poster,Robust Depth Estimation from Auto Bracketed Images,"Sunghoon Im, KAIST; Hae-Gon Jeon, KAIST; In So Kweon, KAIST",Robust Depth Estimation from Auto Bracketed Images,"As demand for advanced photographic applications on hand-held devices grows, these electronics require the capture of high quality depth. However, under low-light conditions, most devices still suffer from low imaging quality and inaccurate depth acquisition. To address the problem, we present a robust depth estimation method from a short burst shot with varied intensity (i.e., Auto Bracketing) or strong noise (i.e., High ISO). We introduce a geometric transformation between flow and depth tailored for burst images, enabling our learning-based multi-view stereo matching to be performed effectively. We then describe our depth estimation pipeline that incorporates the geometric transformation into our residual-flow network. It allows our framework to produce an accurate depth map even with a bracketed image sequence. We demonstrate that our method outperforms state-of-the-art methods for various datasets captured by a smartphone and a DSLR camera. Moreover, we show that the estimated depth is applicable for image quality enhancement and photographic editing.",http://arxiv.org/pdf/1803.07702v1
307,2501,Poster,Free supervision from video games,"Philipp Krahenbuhl,",Shared Autonomy via Deep Reinforcement Learning,"In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user's policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action, with task reward as the only form of supervision. Controlled studies with users (n = 16) and synthetic pilots playing a video game and flying a real quadrotor demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user's private information through observations, but receives a reward signal and user input that both depend on the user's intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user's input. This allows the assisted user to complete the task more effectively than the user or an autonomous agent could on their own. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable flexible and practical assistive systems.",http://arxiv.org/pdf/1802.01744v1
308,2647,Poster,Planar Shape Detection at Structural Scales,"Hao Fang, Inria; Florent Lafarge, ; Mathieu Desbrun, Caltech",A dwarf galaxy remnant in Canis Major: the fossil of an in-plane accretion onto the Milky Way,"We present an analysis of the asymmetries in the population of Galactic M-giant stars present in the 2MASS All Sky catalogue. Several large-scale asymmetries are detected, the most significant of which is a strong elliptical-shaped stellar over-density, close to the Galactic plane at (l=240, b=-8), in the constellation of Canis Major. A small grouping of globular clusters (NGC 1851, NGC 1904, NGC 2298, and NGC 2808), coincident in position and radial velocity, surround this structure, as do a number of open clusters. The population of M-giant stars in this over-density is similar in number to that in the core of the Sagittarius dwarf galaxy. We argue that this object is the likely dwarf galaxy progenitor of the ring-like structure that has recently been found at the edge of the Galactic disk. A numerical study of the tidal disruption of an accreted dwarf galaxy is presented. The simulated debris fits well the extant position, distance and velocity information on the ``Galactic Ring'', as well as that of the M-giant over-densities, suggesting that all these structures are the consequence of a single accretion event. The disrupted dwarf galaxy stream orbits close to the Galactic Plane, with a pericentre at approximately the Solar circle, an orbital eccentricity similar to that of stars in the Galactic thick disk, as well as a vertical scale height similar to that of the thick disk. This finding strongly suggests that the Canis Major dwarf galaxy is a building block of the Galactic thick disk, that the thick disk is continually growing, even up to the present time, and that thick disk globular clusters were accreted onto the Milky Way from dwarf galaxies in co-planar orbits.",http://arxiv.org/pdf/astro-ph/0311010v1
309,2715,Poster,Pix3D: Dataset and Methods for 3D Object Modeling from a Single Image,"Xingyuan Sun, Shanghai Jiao Tong University; Jiajun Wu, MIT; Xiuming Zhang, MIT; Zhoutong Zhang, MIT; Tianfan Xue, Google; Joshua Tenenbaum, ; William Freeman, MIT/Google",,,
310,2936,Poster,Camera Pose Estimation with Unknown Principal Point,"Viktor Larsson, Lund University; Zuzana Kukelova, Czech Technical University in Prague; Yinqiang Zheng, National Institute of Informatics, Japan",,,
311,2988,Poster,Inverse Composition Discriminative Optimization for Point Cloud Registration,"Jayakorn Vongkulbhisal, Carnegie Mellon University; Beñat Irastorza Ugalde, ; Fernando de la Torre, ; João Costeira,",,,
312,711,Poster,SurfConv: Bridging 3D and 2D Convolution for RGBD Images,"Hang Chu, University of Toronto; Wei-Chiu Ma, MIT; Kaustav Kundu, University of Toronto; Raquel Urtasun, University of Toronto; Sanja Fidler,",,,
313,762,Poster,A Fast Resection-Intersection Method for the Known Rotation Problem,"Qianggong Zhang, The University of Adelaide; Tat-Jun Chin, ; Huu Le, The University of Adelaide",,,
314,1914,Poster,3D Pose Estimation and 3D Model Retrieval for Objects in the Wild,"Alexander Grabner, Graz University of Technology; Peter Roth, Graz University of Technology; Vincent Lepetit, TU Graz",3D Pose Estimation and 3D Model Retrieval for Objects in the Wild,"We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.",http://arxiv.org/pdf/1803.11493v1
315,2723,Poster,Structure from Recurrent Motion: From Rigidity to Recurrency,"Xiu Li, Tsinghua University; Hongdong Li, Australian National University; Hanbyul Joo, CMU; Yebin Liu, Tsinghua University; Yaser Sheikh,",,,
316,3027,Poster,Spanning Patches: Deep Patch Selection for Fast Multi-View Stereo,"Alex Poms, Carnegie Mellon University; Shoou-I Yu, Oculus; Chenglei Wu, Oculus; Yaser Sheikh,",,,
317,3219,Poster,Progressively Complementarity-aware Fusion Network for RGB-D Salient Object Detection,"Hao Chen, City University of Hong Kong; You fu Li, City University of Hong Kong",,,
318,3826,Poster,"Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction","Daeyun Shin, UC Irvine; Charless Fowlkes, University of California, Irvine, USA; Derek Hoiem,",,,
319,816,Poster,Learning Dual Convolutional Neural Networks for Low-Level Vision,"Jinshan Pan, UC Merced; Sifei Liu, ; Deqing Sun, NVIDIA; Jiawei Zhang, City University of Hong Kong; Yang Liu, DUT; Jimmy Ren, SenseTime Group Limited; Zechao Li, Nanjing University of Science and Technology ; Jinhui Tang, ; Huchuan Lu, Dalian University of Technology; Yu-Wing Tai, Tencent YouTu; Ming-Hsuan Yang, UC Merced",,,
320,1133,Poster,Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network,"Wenda Zhao, Dalian University of Technolog; Dong Wang, DUT; Huchuan Lu, Dalian University of Technology",,,
321,1251,Poster,PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection,"Nian Liu, Northwestern Polytechnical University; Junwei Han, Northwestern Polytechnical U.; Ming-Hsuan Yang, UC Merced",PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection,"Contexts play an important role in the saliency detection task. However, given a context region, not all contextual information is helpful for the final task. In this paper, we propose a novel pixel-wise contextual attention network, i.e., the PiCANet, to learn to selectively attend to informative context locations for each pixel. Specifically, for each pixel, it can generate an attention map in which each attention weight corresponds to the contextual relevance at each context location. An attended contextual feature can then be constructed by selectively aggregating the contextual information. We formulate the proposed PiCANet in both global and local forms to attend to global and local contexts, respectively. Both models are fully differentiable and can be embedded into CNNs for joint training. We also incorporate the proposed models with the U-Net architecture to detect salient objects. Extensive experiments show that the proposed PiCANets can consistently improve saliency detection performance. The global and local PiCANets facilitate learning global contrast and homogeneousness, respectively. As a result, our saliency model can detect salient objects more accurately and uniformly, thus performing favorably against the state-of-the-art methods.",http://arxiv.org/pdf/1708.06433v2
322,1508,Poster,Curve Reconstruction via the Global Statistics of Natural Curves,"Ehud Barnea, Ben-Gurion University; Ohad Ben-Shahar, Ben-Gurion University",Curve Reconstruction via the Global Statistics of Natural Curves,"Reconstructing the missing parts of a curve has been the subject of much computational research, with applications in image inpainting, object synthesis, etc. Different approaches for solving that problem are typically based on processes that seek visually pleasing or perceptually plausible completions. In this work we focus on reconstructing the underlying physically likely shape by utilizing the global statistics of natural curves. More specifically, we develop a reconstruction model that seeks the mean physical curve for a given inducer configuration. This simple model is both straightforward to compute and it is receptive to diverse additional information, but it requires enough samples for all curve configurations, a practical requirement that limits its effective utilization. To address this practical issue we explore and exploit statistical geometrical properties of natural curves, and in particular, we show that in many cases the mean curve is scale invariant and oftentimes it is extensible. This, in turn, allows to boost the number of examples and thus the robustness of the statistics and its applicability. The reconstruction results are not only more physically plausible but they also lead to important insights on the reconstruction problem, including an elegant explanation why certain inducer configurations are more likely to yield consistent perceptual completions than others.",http://arxiv.org/pdf/1711.03172v2
323,1838,Poster,What do Deep Networks Like to See?,"Sebastian Palacio, DFKI; Joachim Folz, DFKI; Andreas Dengel, DFKI; Jörn Hees, DFKI; Federico Raue, DFKI",Complexity Theory and its Applications in Linear Quantum Optics,"This thesis is intended in part to summarize and also to contribute to the newest developments in passive linear optics that have resulted, directly or indirectly, from the somewhat shocking discovery in 2010 that the BosonSampling problem is likely hard for a classical computer to simulate. In doing so, I hope to provide a historic context for the original result, as well as an outlook on the future of technology derived from these newer developments. An emphasis is made in each section to provide a broader conceptual framework for understanding the consequences of each result in light of the others. This framework is intended to be comprehensible even without a deep understanding of the topics themselves.   The first three chapters focus more closely on the BosonSampling result itself, seeking to understand the computational complexity aspects of passive linear optical networks, and what consequences this may have. Some effort is spent discussing a number of issues inherent in the BosonSampling problem that limit the scope of its applicability, and that are still active topics of research. Finally, we describe two other linear optical settings that inherit the same complexity as BosonSampling. The final chapters focus on how an intuitive understanding of BosonSampling has led to developments in optical metrology and other closely related fields. These developments suggest the exciting possibility that quantum sensors may be viable in the next few years with only marginal improvements in technology. Lastly, some open problems are presented which are intended to lay out a course for future research that would allow for a more complete picture of the scalability of the architecture developed in these chapters.",http://arxiv.org/pdf/1607.02991v1
324,2253,Poster,Zero-Shot Super-Resolution using Deep Internal Learning,"Assaf Shocher, Weizmann institut of Science; Michal Irani, Weizmann Institute of Science; Nadav Cohen, Institute for Advanced Study",,,
325,2268,Poster,"Detect globally, refine locally: A novel approach to saliency detection","TIANTIAN WANG, Dalian University of Technolog; Lihe Zhang, Dalian University of Technology; Huchuan Lu, Dalian University of Technology; Ali Borji, UCF",,,
326,2605,Poster,Beyond the Pixel-Wise Loss for Topology-Aware Delineation,"Agata  Mosinska, EPFL; Pablo Marquez Neila, EPFL; Mateusz Kozinski, ; Pascal Fua,",Beyond the Pixel-Wise Loss for Topology-Aware Delineation,"Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary cross-entropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological impact of mistakes in the final prediction. We propose a new loss term that is aware of the higher-order topological features of linear structures. We also introduce a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step while keeping the number of parameters and the complexity of the model constant.   When combined with the standard pixel-wise loss, both our new loss term and our iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained with the binary cross-entropy alone. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images.",http://arxiv.org/pdf/1712.02190v1
327,2644,Poster,KIPPI: KInetic Polygonal Partitioning of Images,"Jean-Philippe Bauchet, Inria; Florent Lafarge,",,,
328,2954,Poster,Image Blind Denoising With Generative Adversarial Network Based Noise Modeling,"Jingwen Chen, Sun Yat-sen University; Jiawei Chen, Sun Yat-sen University; Hongyang Chao, Sun Yat-sen University; Ming Yang,",,,
329,3269,Poster,Multi-Scale Weighted Nuclear Norm Image Restoration,"Noam Yair, Technion; Tomer Michaeli, Technion",,,
330,3442,Poster,MoNet: Moments Embedding Network,"Mengran Gou, Northeastern University; Fei Xiong, University of Southern California ; Octavia Camps, Northeastern University, USA; Mario Sznaier,",MoNet: Moments Embedding Network,"Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks. Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion. However, a serious drawback of this family of pooling layers is their dimensionality explosion. Approximate pooling methods with compact properties have been explored towards resolving this weakness. Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information. However, combining compact pooling with matrix normalization and other order information has not been explored until now. In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix. In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods. Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet. Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions.",http://arxiv.org/pdf/1802.07303v2
331,3798,Poster,Active Fixation Control to Predict Saccade Sequences,"Calden Wloka, York University; Iuliia Kotseruba, York University; John Tsotsos, York University Canada",,,
332,49,Poster,Densely Connected Pyramid Dehazing Network,"He Zhang, Rutgers; Vishal Patel,",Densely Connected Pyramid Dehazing Network,"We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods. Code will be made available at: https://github.com/hezhangsprinter",http://arxiv.org/pdf/1803.08396v1
333,512,Poster,Universal Denoising Networks : A Novel CNN-based Network Architecture for Image Denoising,"Stamatios Lefkimmiatis, Skolkovo Institute of Science",Universal Denoising Networks : A Novel CNN Architecture for Image Denoising,"We design a novel network architecture for learning discriminative image models that are employed to efficiently tackle the problem of grayscale and color image denoising. Based on the proposed architecture, we introduce two different variants. The first network involves convolutional layers as a core component, while the second one relies instead on non-local filtering layers and thus it is able to exploit the inherent non-local self-similarity property of natural images. As opposed to most of the existing deep network approaches, which require the training of a specific model for each considered noise level, the proposed models are able to handle a wide range of noise levels using a single set of learned parameters, while they are very robust when the noise degrading the latent image does not match the statistics of the noise used during training. The latter argument is supported by results that we report on publicly available images corrupted by unknown noise and which we compare against solutions obtained by competing methods. At the same time the introduced networks achieve excellent results under additive white Gaussian noise (AWGN), which are comparable to those of the current state-of-the-art network, while they depend on a more shallow architecture with the number of trained parameters being one order of magnitude smaller. These properties make the proposed networks ideal candidates to serve as sub-solvers on restoration methods that deal with general inverse imaging problems such as deblurring, demosaicking, superresolution, etc.",http://arxiv.org/pdf/1711.07807v2
334,673,Poster,Learning Convolutional Networks for Content-weighted Image Compression,"Mu LI, PolyU; Wangmeng Zuo, Harbin Institute of Technology; Shuhang Gu, ; debin Zhao, ; David Zhang, Hong Kong Polytechnic University",Learning Convolutional Networks for Content-weighted Image Compression,"Lossy image compression is generally formulated as a joint rate-distortion optimization to learn encoder, quantizer, and decoder. However, the quantizer is non-differentiable, and discrete entropy estimation usually is required for rate control. These make it very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that the bit rate of the different parts of the image should be adapted to local content. And the content aware bit rate is allocated under the guidance of a content-weighted importance map. Thus, the sum of the importance map can serve as a continuous alternative of discrete entropy estimation to control compression rate. And binarizer is adopted to quantize the output of encoder due to the binarization scheme is also directly defined by the importance map. Furthermore, a proxy function is introduced for binary operation in backward propagation to make it differentiable. Therefore, the encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner by using a subset of the ImageNet database. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.",http://arxiv.org/pdf/1703.10553v2
335,823,Poster,Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation,"Younghyun Jo, Yonsei University; Seoung Wug Oh, Yonsei Univeristy; JaeYeon Kang, Yonsei Univ.; Seon Joo Kim, Yonsei University",,,
336,1118,Poster,Erase or Fill? Deep Joint Recurrent Rain Removal and Reconstruction in Videos,"Jiaying Liu, Peking University; Wenhan Yang, Peking University; Shuai Yang, Peking University; Zongming Guo,",,,
337,1226,Poster,Flow Guided Recurrent Neural Encoder for Video Salient Object Detection,"Guanbin Li, ; Yuan Xie, ; Tianhao Wei, ; Liang Lin,",,,
338,1252,Poster,Gated Fusion Network for Single Image Dehazing,"Wenqi Ren, Chinese Academy of Sciences; Lin Ma, Tencent AI Lab; Jiawei Zhang, City University of Hong Kong; Jinshan Pan, UC Merced; Xiaochun Cao, Chinese Academy of Sciences; Wei Liu, ; Ming-Hsuan Yang, UC Merced",Gated Fusion Network for Single Image Dehazing,"In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale approach such that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.",http://arxiv.org/pdf/1804.00213v1
339,1440,Poster,Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,"Kai Zhang, Harbin Institute of Technology; Wangmeng Zuo, Harbin Institute of Technology; Lei Zhang, The Hong Kong Polytechnic University",Learning a Single Convolutional Super-Resolution Network for Multiple Degradations,"Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to deal with multiple degradations. To address these issues, we propose a dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input. Consequently, the proposed super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications.",http://arxiv.org/pdf/1712.06116v1
340,1792,Poster,Non-blind Deblurring: Handling Kernel Uncertainty with CNNs,"Subeesh Vasu, IIT Madras; Venkatesh Reddy Maligireddy, IIT Madras; A.N. Rajagopalan, IIT Madras",,,
341,1939,Poster,Boundary Flow: A Siamese Network that Predicts Boundary Motion without Training on Motion,"Peng Lei, Oregon State University; Fuxin Li, Oregon State University; Sinisa Todorovic,",Boundary Flow: A Siamese Network that Predicts Boundary Motion without Training on Motion,"Using deep learning, this paper addresses the problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize objects spatial extents, and the flow indicates objects motions and interactions. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet-based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPMFlow but on the augmented input with our boundary-flow matches, and achieve significant performance improvement on the Sintel benchmark.",http://arxiv.org/pdf/1702.08646v3
342,1981,Poster,Learning to See in the Dark,"Chen Chen, UIUC; Qifeng Chen, Intel Labs; Jia Xu, Tencent AI Lab; Vladlen Koltun, Intel Labs",Interpreting Deep Classifier by Visual Distillation of Dark Knowledge,"Interpreting black box classifiers, such as deep networks, allows an analyst to validate a classifier before it is deployed in a high-stakes setting. A natural idea is to visualize the deep network's representations, so as to ""see what the network sees"". In this paper, we demonstrate that standard dimension reduction methods in this setting can yield uninformative or even misleading visualizations. Instead, we present DarkSight, which visually summarizes the predictions of a classifier in a way inspired by notion of dark knowledge. DarkSight embeds the data points into a low-dimensional space such that it is easy to compress the deep classifier into a simpler one, essentially combining model compression and dimension reduction. We compare DarkSight against t-SNE both qualitatively and quantitatively, demonstrating that DarkSight visualizations are more informative. Our method additionally yields a new confidence measure based on dark knowledge by quantifying how unusual a given vector of predictions is.",http://arxiv.org/pdf/1803.04042v1
343,170,Poster,BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning,"Ziming Zhang, MERL; Yuanwei Wu, University of Kansas; Guanghui Wang, University of Kansas",BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning,"Understanding the global optimality in deep learning (DL) has been attracting more and more attention recently. Conventional DL solvers, however, have not been developed intentionally to seek for such global optimality. In this paper we propose a novel approximation algorithm, BPGrad, towards optimizing deep models globally via branch and pruning. Our BPGrad algorithm is based on the assumption of Lipschitz continuity in DL, and as a result it can adaptively determine the step size for current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that, by repeating such branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation.",http://arxiv.org/pdf/1711.06959v1
344,200,Poster,Perturbative Neural Networks: Rethinking Convolution in CNNs,"Felix Juefei-Xu, Carnegie Mellon University; Vishnu Naresh Boddeti, Michigan State University; Marios Savvides, Carnegie Mellon University",,,
345,506,Poster,Unsupervised CCA,"Yedid Hoshen, Facebook AI Research (FAIR); Lior Wolf, Tel Aviv University, Israel",Unsupervised Correlation Analysis,"Linking between two data sources is a basic building block in numerous computer vision problems. In this paper, we set to answer a fundamental cognitive question: are prior correspondences necessary for linking between different domains?   One of the most popular methods for linking between domains is Canonical Correlation Analysis (CCA). All current CCA algorithms require correspondences between the views. We introduce a new method Unsupervised Correlation Analysis (UCA), which requires no prior correspondences between the two domains. The correlation maximization term in CCA is replaced by a combination of a reconstruction term (similar to autoencoders), full cycle loss, orthogonality and multiple domain confusion terms. Due to lack of supervision, the optimization leads to multiple alternative solutions with similar scores and we therefore introduce a consensus-based mechanism that is often able to recover the desired solution. Remarkably, this suffices in order to link remote domains such as text and images. We also present results on well accepted CCA benchmarks, showing that performance far exceeds other unsupervised baselines, and approaches supervised performance in some cases.",http://arxiv.org/pdf/1804.00347v1
346,1103,Poster,A Biresolution Spectral framework for Product Quantization,"Lopamudra Mukherjee, University of Wisc Whitewater; Sathya Ravi, University of Wisconsin-Madison; Jiming Peng, University of Houston; Vikas Singh, University of Wisconsin-Madison",,,
347,1320,Poster,Domain Adaptive Faster R-CNN for Object Detection in the Wild,"Yuhua Chen, CVL@ETHZ; Wen Li, ETH; Luc Van Gool, KTH",,,
348,1590,Poster,Low-shot learning with large-scale diffusion,"Matthijs Douze, ; Arthur Szlam, Facebook AI Research; Bharath Hariharan, Cornell University; Herve Jegou, Facebook AI Research",Low-shot learning with large-scale diffusion,"This paper considers the problem of inferring image labels for which only a few labelled examples are available at training time. This setup is often referred to as low-shot learning in the literature, where a standard approach is to re-train the last few layers of a convolutional neural network learned on separate classes. We consider a semi-supervised setting in which we exploit a large collection of images to support label propagation. This is made possible by leveraging the recent advances on large-scale similarity graph construction. We show that despite its conceptual simplicity, scaling up label propagation to up hundred millions of images leads to state of the art accuracy in the low-shot learning regime.",http://arxiv.org/pdf/1706.02332v2
349,1630,Poster,Joint Pose and Expression Modeling for Facial Expression Recognition,"Feifei Zhang, Jiangsu University; Tianzhu Zhang, CASIA; Qirong Mao, Department of Computer Science and Communication Engineering, Jiangsu University; Changsheng Xu,",Motion deblurring of faces,"Face analysis is a core part of computer vision, in which remarkable progress has been observed in the past decades. Current methods achieve recognition and tracking with invariance to fundamental modes of variation such as illumination, 3D pose, expressions. Notwithstanding, a much less standing mode of variation is motion deblurring, which however presents substantial challenges in face analysis. Recent approaches either make oversimplifying assumptions, e.g. in cases of joint optimization with other tasks, or fail to preserve the highly structured shape/identity information. Therefore, we propose a data-driven method that encourages identity preservation. The proposed model includes two parallel streams (sub-networks): the first deblurs the image, the second implicitly extracts and projects the identity of both the sharp and the blurred image in similar subspaces. We devise a method for creating realistic motion blur by averaging a variable number of frames to train our model. The averaged images originate from a 2MF2 dataset with 10 million facial frames, which we introduce for the task. Considering deblurring as an intermediate step, we utilize the deblurred outputs to conduct a thorough experimentation on high-level face analysis tasks, i.e. landmark localization and face verification. The experimental evaluation demonstrates the superiority of our method.",http://arxiv.org/pdf/1803.03330v1
350,1799,Poster,Lightweight Probabilistic Deep Networks,"Jochen Gast, TU Darmstadt; Stefan Roth,",,,
351,2503,Poster,Adversarially Learned One-Class Classifier for Novelty Detection,"Mohammad Sabokrou, Institute for Research in Fundamental Sciences (IPM); Mohammad Khalooie, ; Mahmood Fathi, ; Ehsan Adeli, Stanford University",Adversarially Learned One-Class Classifier for Novelty Detection,"Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods.",http://arxiv.org/pdf/1802.09088v1
352,2722,Poster,Defense against Universal Adversarial Perturbations,"NAVEED AKHTAR, UNIVERSITY OF WESTERN AUSTRALI; Jian Liu, UWA; Ajmal Mian, UWA",Defense against Universal Adversarial Perturbations,"Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to `any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These `Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.",http://arxiv.org/pdf/1711.05929v3
353,2797,Poster,Disentangling Factors of Variation by Mixing Them,"Qiyang HU, University of bern; Attila Szabo, University of Bern; Tiziano Portenier, ; Matthias Zwicker, ; Paolo Favaro, Bern University, Switzerland",Disentangling Factors of Variation by Mixing Them,"We propose an approach to learn image representations that consist of disentangled factors of variation without exploiting any manual labeling or data domain knowledge. A factor of variation corresponds to an image attribute that can be discerned consistently across a set of images, such as the pose or color of objects. Our disentangled representation consists of a concatenation of feature chunks, each chunk representing a factor of variation. It supports applications such as transferring attributes from one image to another, by simply mixing and unmixing feature chunks, and classification or retrieval based on one or several attributes, by considering a user-specified subset of feature chunks. We learn our representation without any labeling or knowledge of the data domain, using an autoencoder architecture with two novel training objectives: first, we propose an invariance objective to encourage that encoding of each attribute, and decoding of each chunk, are invariant to changes in other attributes and chunks, respectively; second, we include a classification objective, which ensures that each chunk corresponds to a consistently discernible attribute in the represented image, hence avoiding degenerate feature mappings where some chunks are completely ignored. We demonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA datasets.",http://arxiv.org/pdf/1711.07410v2
354,3247,Poster,Deformable GANs for Pose-based Human Image Generation,"Aliaksandr Siarohin , DISI, University of Trento; Enver Sangineto, University of Trento; Stéphane Lathuilière, Inria; Nicu Sebe, University of Trento",Deformable GANs for Pose-based Human Image Generation,"In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.",http://arxiv.org/pdf/1801.00055v2
355,3315,Poster,Hierarchical Recurrent Attention Networks for  Structured Online Maps,"Namdar Homayounfar, Uber ATG; Wei-Chiu Ma, MIT; Shrinidhi Kowshika Lakshmikanth, Uber ATG; Raquel Urtasun, University of Toronto",,,
356,3352,Poster,Sliced Wasserstein Distance for Learning Gaussian Mixture Models,"Soheil Kolouri, HRL Laboratories, LLC; Gustavo Rohde, University Virginia ; Heiko Hoffmann, HRL Laboratories, LLC",Sliced Wasserstein Distance for Learning Gaussian Mixture Models,"Gaussian mixture models (GMM) are powerful parametric tools with many applications in machine learning and computer vision. Expectation maximization (EM) is the most popular algorithm for estimating the GMM parameters. However, EM guarantees only convergence to a stationary point of the log-likelihood function, which could be arbitrarily worse than the optimal solution. Inspired by the relationship between the negative log-likelihood function and the Kullback-Leibler (KL) divergence, we propose an alternative formulation for estimating the GMM parameters using the sliced Wasserstein distance, which gives rise to a new algorithm. Specifically, we propose minimizing the sliced-Wasserstein distance between the mixture model and the data distribution with respect to the GMM parameters. In contrast to the KL-divergence, the energy landscape for the sliced-Wasserstein distance is more well-behaved and therefore more suitable for a stochastic gradient descent scheme to obtain the optimal GMM parameters. We show that our formulation results in parameter estimates that are more robust to random initializations and demonstrate that it can estimate high-dimensional data distributions more faithfully than the EM algorithm.",http://arxiv.org/pdf/1711.05376v2
357,3383,Poster,Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation,"Zhen Zhang, WASHINGTON UNIVERSITY IN ST.LO; Mianzhi Wang, WASHINGTON UNIVERSITY IN ST.LOUIS; Yan Huang, ; Arye Nehorai, WASHINGTON UNIVERSITY IN ST.LOUIS",,,
358,3412,Poster,CLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition,"Jedrzej Kozerawski, UCSB; Matthew Turk, UC Santa Barbara USA",,,
359,3558,Poster,Local and Global Optimization Techniques in Graph-based Clustering,"Daiki Ikami, The University of Tokyo; Toshihiko Yamasaki, The University of Tokyo; Kiyoharu Aizawa,",Binary Optimization via Mathematical Programming with Equilibrium Constraints,"Binary optimization is a central problem in mathematical optimization and its applications are abundant. To solve this problem, we propose a new class of continuous optimization techniques which is based on Mathematical Programming with Equilibrium Constraints (MPECs). We first reformulate the binary program as an equivalent augmented biconvex optimization problem with a bilinear equality constraint, then we propose two penalization/regularization methods (exact penalty and alternating direction) to solve it. The resulting algorithms seek desirable solutions to the original problem via solving a sequence of linear programming convex relaxation subproblems. In addition, we prove that both the penalty function and augmented Lagrangian function, induced by adding the complementarity constraint to the objectives, are exact, i.e., they have the same local and global minima with those of the original binary program when the penalty parameter is over some threshold. The convergence of both algorithms can be guaranteed, since they essentially reduce to block coordinate descent in the literature. Finally, we demonstrate the effectiveness and versatility of our methods on several important problems, including graph bisection, constrained image segmentation, dense subgraph discovery, modularity clustering and Markov random fields. Extensive experiments show that our methods outperform existing popular techniques, such as iterative hard thresholding, linear programming relaxation and semidefinite programming relaxation.",http://arxiv.org/pdf/1608.04425v4
360,3604,Poster,Multi-task Learning by Maximizing Statistical Dependence,"Youssef Alami Mejjati, University of Bath; Darren Cosker, University of Bath; Kwang In Kim, University of Bath",,,
361,3687,Poster,Robust Classification with Convolutional Prototype Learning,"Hong-Ming Yang, Institute of Automation, Chinese Academy of Sciences; Xu-Yao Zhang, Institute of Automation, Chinese Academy of Sciences; Fei Yin, Institute of Automation, Chinese Academy of Sciences; cheng-lin Liu,",,,
362,3722,Poster,Generative Modeling using the Sliced Wasserstein Distance,"Ishan Deshpande, UIUC; Ziyu Zhang, Snap Research; Alex Schwing,",Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative Model,"In this paper we study generative modeling via autoencoders while using the elegant geometric properties of the optimal transport (OT) problem and the Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE), which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution. In short, we regularize the autoencoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a predefined samplable distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an embarrassingly simple implementation.",http://arxiv.org/pdf/1804.01947v1
363,3809,Poster,Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks,"Tom Veniat, Lip6 - MLIA; Ludovic Denoyer, UPMC",Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks,"We propose to focus on the problem of discovering neural network architectures efficient both in terms of prediction quality and cost. For instance, our approach is able to solve the following tasks: 'learn a neural network able to predict well in less than 100 milliseconds' or 'learn an efficient model that fits in a 50 Mb memory'. Our contribution is a novel family of models called Budgeted Super Networks. They are learned using gradient descent techniques applied on a budgeted learning objective function which integrates a maximum authorized cost where this cost can be of different nature. We present a set of experiments on computer vision problems and analyze the ability of our technique to deal with three different costs: the computation cost, the memory consumption cost, and also a distributed computation cost. We particularly show that our model can discover neural network architectures that have a better accuracy than the ResNet and CNF architectures on CIFAR-10 and CIFAR-100, at a lower cost.",http://arxiv.org/pdf/1706.00046v3
364,3867,Poster,Cross-View Image Synthesis using Conditional Generative Adversarial Nets,"Krishna Regmi, Ucf; Ali Borji, UCF",,,
365,85,Poster,"Sparse, Smart Contours to Represent and Edit Images","Tali Dekel, Google; Dilip Krishnan, Google; Chuang Gan, Tsinghua University; Ce Liu, Google, Cambridge, USA; William Freeman, Google","Smart, Sparse Contours to Represent and Edit Images","We study the problem of reconstructing an image from information stored at sparse contour locations. Existing contour-based image reconstruction methods struggle to balance contour sparsity and reconstruction fidelity. Therefore, denser contours are needed to capture subtle texture information even though contours were not meant for textures. We propose a novel image representation where image content is characterized by contours with gradient information via an encoder-decoder network, while image details are modeled by a conditional generative adversarial network. We show that high-quality reconstructions with high fidelity to the source image can be obtained from extremely sparse input, e.g., comprising less than 6% of image pixels. Our model synthesizes texture, details and fine structures in regions where no input information is provided. The semantic knowledge encoded into our model and the sparsity of the input allows using contours as an intuitive interface for semantically-aware image manipulation: local edits in contour domain such as scaling, translation and erasing, translate to long-range and coherent changes in the pixel space. Experiments on a variety of datasets verify the versatility and convenience afforded by our models.",http://arxiv.org/pdf/1712.08232v1
366,494,Poster,Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB,"Tomoyuki Suzuki, Keio University; Hirokatsu Kataoka, AIST; Yoshimitsu Aoki, Keio University; Yutaka Satoh, AIST",Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB,"In this paper, we propose a novel approach for traffic accident anticipation through (i) Adaptive Loss for Early Anticipation (AdaLEA) and (ii) a large-scale self-annotated incident database for anticipation. The proposed AdaLEA allows a model to gradually learn an earlier anticipation as training progresses. The loss function adaptively assigns penalty weights depending on how early the model can an- ticipate a traffic accident at each epoch. Additionally, we construct a Near-miss Incident DataBase for anticipation. This database contains an enormous number of traffic near- miss incident videos and annotations for detail evaluation of two tasks, risk anticipation and risk-factor anticipation. In our experimental results, we found our proposal achieved the highest scores for risk anticipation (+6.6% better on mean average precision (mAP) and 2.36 sec earlier than previous work on the average time-to-collision (ATTC)) and risk-factor anticipation (+4.3% better on mAP and 0.70 sec earlier than previous work on ATTC).",http://arxiv.org/pdf/1804.02675v1
367,1020,Poster,A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds,"Tolga Birdal, Technical University of Munich; Benjamin Busam, Framos; Nassir Navab, Technical University of Munich; Slobodan Ilic, Siemens AG; Peter Sturm, INRIA Rhone-Alpes",A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds,"This paper proposes a segmentation-free, automatic and efficient procedure to detect general geometric quadric forms in point clouds, where clutter and occlusions are inevitable. Our everyday world is dominated by man-made objects which are designed using 3D primitives (such as planes, cones, spheres, cylinders, etc.). These objects are also omnipresent in industrial environments. This gives rise to the possibility of abstracting 3D scenes through primitives, thereby positions these geometric forms as an integral part of perception and high level 3D scene understanding.   As opposed to state-of-the-art, where a tailored algorithm treats each primitive type separately, we propose to encapsulate all types in a single robust detection procedure. At the center of our approach lies a closed form 3D quadric fit, operating in both primal & dual spaces and requiring as low as 4 oriented-points. Around this fit, we design a novel, local null-space voting strategy to reduce the 4-point case to 3. Voting is coupled with the famous RANSAC and makes our algorithm orders of magnitude faster than its conventional counterparts. This is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes. Results on synthetic and real datasets support the validity of our method.",http://arxiv.org/pdf/1803.07191v1
368,1170,Poster,Facelet-Bank for Fast Portrait Manipulation,"Ying-Cong Chen, CUHK; Lin Huaijia, the Chinese University of Hong Kong; Ruiyu Li, CUHK; Michelle Shu, ; Xin Tao, CUHK; Yangang Ye, Tencent; Xiaoyong Shen, CUHK; Jiaya Jia, Chinese University of Hong Kong",Facelet-Bank for Fast Portrait Manipulation,"Digital face manipulation has become a popular and fascinating way to touch images with the prevalence of smartphones and social networks. With a wide variety of user preferences, facial expressions, and accessories, a general and flexible model is necessary to accommodate different types of facial editing. In this paper, we propose a model to achieve this goal based on an end-to-end convolutional neural network that supports fast inference, edit-effect control, and quick partial-model update. In addition, this model learns from unpaired image sets with different attributes. Experimental results show that our framework can handle a wide range of expressions, accessories, and makeup effects. It produces high-resolution and high-quality results in fast speed.",http://arxiv.org/pdf/1803.05576v3
369,1539,Poster,Visual to Sound: Generating Natural Sound for Videos in the Wild,"Yipin Zhou, UNC-Chapel Hill; Zhaowen Wang, Adobe; Chen Fang, Adobe Research; Trung Bui, ; Tamara Berg, University on North carolina",Visual to Sound: Generating Natural Sound for Videos in the Wild,"As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.",http://arxiv.org/pdf/1712.01393v1
370,1128,Oral,3D-RCNN: Instance-level 3D Scene Understanding via Render-and-Compare,"Abhijit Kundu, Georgia Institute of Technology; Yin Li, Georgia Tech; James Rehg, Georgia Institute of Technology",,,
371,3013,Oral,"Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net","Wenjie Luo, Uber ATG.; UofT; Bin Yang, Uber ATG, UofT; Raquel Urtasun, University of Toronto",,,
372,552,Oral,An Analysis of Scale Invariance in Object Detection - SNIP,"Bharat Singh, ; Larry Davis, University of Maryland, USA",An Analysis of Scale Invariance in Object Detection - SNIP,"An analysis of different techniques for recognizing and detecting objects under extreme scale variation is presented. Scale specific and scale invariant design of detectors are compared by training them with different configurations of input data. To examine if upsampling images is necessary for detecting small objects, we evaluate the performance of different network architectures for classifying small objects on ImageNet. Based on this analysis, we propose a deep end-to-end trainable Image Pyramid Network for object detection which operates on the same image scales during training and inference. Since small and large objects are difficult to recognize at smaller and larger scales respectively, we present a novel training scheme called Scale Normalization for Image Pyramids (SNIP) which selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. On the COCO dataset, our single model performance is 45.7% and an ensemble of 3 networks obtains an mAP of 48.3%. We use ImageNet-1000 pre-trained models and only train with bounding box supervision. Our submission won the Best Student Entry in the COCO 2017 challenge. Code will be made available at http://bit.ly/2yXVg4c.",http://arxiv.org/pdf/1711.08189v1
373,2498,Oral,Relation Networks for Object Detection,"Han Hu, ; Jiayuan Gu, Microsoft; Zheng Zhang, Microsoft; Jifeng Dai, Microsoft Research; Yichen Wei, Microsoft Research Asia",Learning to Detect Human-Object Interactions,"We study the problem of detecting human-object interactions (HOI) in static images, defined as predicting a human and an object bounding box with an interaction class label that connects them. HOI detection is a fundamental problem in computer vision as it provides semantic information about the interactions among the detected objects. We introduce HICO-DET, a new large benchmark for HOI detection, by augmenting the current HICO classification benchmark with instance annotations. To solve the task, we propose Human-Object Region-based Convolutional Neural Networks (HO-RCNN). At the core of our HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the spatial relations between two bounding boxes. Experiments on HICO-DET demonstrate that our HO-RCNN, by exploiting human-object spatial relations through Interaction Patterns, significantly improves the performance of HOI detection over baseline approaches.",http://arxiv.org/pdf/1702.05448v2
374,488,Spotlight,Zero-Shot Sketch-Image Hashing,"Yuming Shen, University of East Anglia; Li Liu, University of East Anglia; Fumin Shen, ; Ling Shao, University of East Anglia",SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval,"We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset. Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval. Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset, we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition. Such superior performances effectively demonstrate the benefit of our sketch-specific design.",http://arxiv.org/pdf/1804.01401v1
375,1597,Spotlight,VizWiz Grand Challenge: Answering Visual Questions from Blind People,"Danna Gurari, University of Texas at Austin; Qing Li, USTC; Abigale Stangl, ; Anhong Guo, ; Chi Lin, ; Kristen Grauman, ; Jiebo Luo, University of Rochester; Jeffrey Bigham,",VizWiz Grand Challenge: Answering Visual Questions from Blind People,"The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting. VizWiz consists of over 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered. Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset. We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people.",http://arxiv.org/pdf/1802.08218v2
376,2726,Spotlight,Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN,"Deepak Babu Sam, Indian Institute of Science; Neeraj Sajjan, Indian Institute of Science; Venkatesh Babu Radhakrishnan, Indian Institute of Science; Mukundhan Srinivasan, NVIDIA",,,
377,2732,Spotlight,Structured Set Matching Networks for One-Shot Part Labeling,"Jonghyun Choi, ; Jayant Krishnamurthy, Semantic Machines; Aniruddha Kembhavi, Allen Institute for Artificial Intelligence; Ali Farhadi,",Structured Set Matching Networks for One-Shot Part Labeling,"Diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning. However, understanding diagrams using natural image understanding approaches requires large training datasets of diagrams, which are very hard to obtain. Instead, this can be addressed as a matching problem either between labeled diagrams, images or both. This problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning. We consider the problem of one-shot part labeling: labeling multiple parts of an object in a target image given only a single source image of that category. For this set-to-set matching problem, we introduce the Structured Set Matching Network (SSMN), a structured prediction model that incorporates convolutional neural networks. The SSMN is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements, while also enforcing a matching constraint between the two sets. The SSMN significantly outperforms several strong baselines on three label transfer scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200 categories; image-to-image, evaluated on a dataset built on top of the Pascal Part Dataset; and image-to-diagram, evaluated on transferring labels across these datasets.",http://arxiv.org/pdf/1712.01867v2
378,2827,Spotlight,Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection,"David Novotny, Oxford University; Samuel Albanie, Oxford University; Diane Larlus, NAVER LABS Europe; Andrea Vedaldi, U Oxford",Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection,"Self-supervision can dramatically cut back the amount of manually-labelled data required to train deep neural networks. While self-supervision has usually been considered for tasks such as image classification, in this paper we aim at extending it to geometry-oriented tasks such as semantic matching and part detection. We do so by building on several recent ideas in unsupervised landmark detection. Our approach learns dense distinctive visual descriptors from an unlabelled dataset of images using synthetic image transformations. It does so by means of a robust probabilistic formulation that can introspectively determine which image regions are likely to result in stable image matching. We show empirically that a network pre-trained in this manner requires significantly less supervision to learn semantic object parts compared to numerous pre-training alternatives. We also show that the pre-trained representation is excellent for semantic object matching.",http://arxiv.org/pdf/1804.01552v1
379,3659,Spotlight,Link and code: Fast indexing with graphs and compact regression codes,"Matthijs Douze, ; Herve Jegou, Facebook AI Research",,,
380,465,Spotlight,Textbook Question Answering under Teacher Guidance with Memory Networks,"Juzheng Li, Tsinghua University; Hang Su, Tsinghua University; Jun Zhu, Tsinghua University; Siyu Wang, ; Bo Zhang,",,,
381,909,Spotlight,Unsupervised Deep Generative Adversarial Hashing Network,"Kamran Ghasedi Dizaji, University of Pittsburgh; Feng Zheng, University of Pittsburgh; Najmeh Sadoughi, University of Texas at Dallas; Heng Huang, University of Pittsburgh",Binary Generative Adversarial Networks for Image Retrieval,"The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107\% in terms of~mAP (See Table tab.res.map.comp) Our anonymous code is available at: https://github.com/htconquer/BGAN.",http://arxiv.org/pdf/1708.04150v1
382,1027,Spotlight,Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments,"Peter Anderson, Australian National University; Qi Wu, University of Adelaide; Damien Teney, Unversity of Adelaide; Jake Bruce, ; Mark Johnson, Macquarie University; Niko Sünderhauf, Queensland University of Technology; Ian Reid, ; Stephen Gould, Australian National University; Anton Van den Hengel, University of Adelaide",Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments,"A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.",http://arxiv.org/pdf/1711.07280v3
383,2022,Spotlight,DenseASPP: Densely Connected Networks for Semantic Segmentation,"Maoke Yang, DeepMotion; Kun Yu, DeepMotion; Kuiyuan Yang, DeepMotion",,,
384,3595,Oral,Efficient Optimization for Rank-based Loss Functions,"Pritish Mohapatra, IIIT Hyderabad; Michal Rolinek, Max Planck Institute for Intelligent Systems, Tuebingen; C.V. Jawahar, IIIT Hyderabad; Vladimir Kolmogorov, Institute of Science and Technology, Austria; M. Pawan Kumar,",Sparse Reduced Rank Regression With Nonconvex Regularization,"In this paper, the estimation problem for sparse reduced rank regression (SRRR) model is considered. The SRRR model is widely used for dimension reduction and variable selection with applications in signal processing, econometrics, etc. The problem is formulated to minimize the least squares loss with a sparsity-inducing penalty considering an orthogonality constraint. Convex sparsity-inducing functions have been used for SRRR in literature. In this work, a nonconvex function is proposed for better sparsity inducing. An efficient algorithm is developed based on the alternating minimization (or projection) method to solve the nonconvex optimization problem. Numerical simulations show that the proposed algorithm is much more efficient compared to the benchmark methods and the nonconvex function can result in a better estimation accuracy.",http://arxiv.org/pdf/1803.07247v1
385,2650,Oral,Wasserstein Introspective Neural Networks,"Kwonjoon Lee, UC San Diego; Weijian Xu, UC San Diego; Fan Fan, UC San Diego; Zhuowen Tu, UCSD, USA",Wasserstein Introspective Neural Networks,"We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.",http://arxiv.org/pdf/1711.08875v5
386,744,Oral,Taskonomy: Disentangling Task Transfer Learning,"Alexander Sax, Stanford University; William Shen, ; Amir Zamir, Stanford, UC Berkeley; Jitendra Malik, ; Silvio Savarese, ; Leonidas J. Guibas,",,,
387,1470,Oral,Maximum Classifier Discrepancy for Unsupervised Domain Adaptation,"Kuniaki Saito, The University of Tokyo; Kohei Watanabe, ; Yoshitaka Ushiku, ; Tatsuya Harada, University of Tokyo",Maximum Classifier Discrepancy for Unsupervised Domain Adaptation,"In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics.   To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at \url{https://github.com/mil-tokyo/MCD_DA}",http://arxiv.org/pdf/1712.02560v4
388,801,Spotlight,Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination,"Zhirong Wu, UC Berkeley; Yuanjun Xiong, Amazon ; Stella Yu, UC Berkeley / ICSI; Dahua Lin, CUHK",,,
389,1589,Spotlight,Multi-Task Adversarial Network for Disentangled Feature Learning,"Yang Liu, University of Cambridge; Zhaowen Wang, Adobe; Hailin Jin, ; Ian Wassell,",Multi-Agent Diverse Generative Adversarial Networks,"We propose an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known mode collapse problem. Firstly, we propose a multi-agent GAN architecture incorporating multiple generators and one discriminator. Secondly, to enforce different generators to capture diverse high probability modes, we modify discriminator's objective function where along with finding the real and fake samples, the discriminator has to identify the generator that generated the fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. Our framework (MAD-GAN) is generalizable in the sense that it can be easily combined with other existing variants of GANs to produce diverse samples. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for the challenging tasks such as image-to-image translation (known to learn delta distribution) and face generation. In addition, we show that MAD-GAN is able to disentangle different modalities even when trained using highly challenging multi-view dataset (mixture of forests, icebergs, bedrooms etc). In the end, we also show its efficacy for the unsupervised feature representation task. In the appendix we introduce a similarity based competing objective which encourages the different generators to generate varied samples judged by a user defined similarity metric. We show extensive evaluations on a 1-D setting of mixture of gaussians for non parametric density estimation. The theoretical proofs back the efficacy of the framework and explains why various generators are pushed towards distinct clusters of modes.",http://arxiv.org/pdf/1704.02906v2
390,1593,Spotlight,Learning from Synthetic Data: Semantic Segmentation using Generative Adversarial Networks,"Swami Sankaranarayanan, University of Maryland; Yogesh Balaji, University of Maryland; Arpit Jain, ; Ser-Nam Lim, GE Global Research; Rama Chellappa, University of Maryland, USA",Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation,"Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.",http://arxiv.org/pdf/1711.06969v2
391,2851,Spotlight,Empirical study of the topology and geometry of deep networks,"Alhussein Fawzi, ; Seyed-Mohsen Moosavi-Dezfooli, ; Pascal Frossard, ; Stefano Soatto, UCLA",Topology and Geometry of Half-Rectified Network Optimization,"The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.   In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important \emph{folklore} facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.   The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors.",http://arxiv.org/pdf/1611.01540v4
392,3548,Spotlight,Boosting Domain Adaptation by Discovering Latent Domains,"Massimiliano Mancini, Sapienza University of Rome; Lorenzo Porzi, Mapillary Research; Samuel Rota Bulò, Mapillary Research; Barbara Caputo, University of Rome La Sapienza, Italy; Elisa Ricci, U. Perugia",,,
393,27,Spotlight,Shape from Shading through Shape Evolution,"Dawei Yang, University of Michigan; Jia Deng,",Shape from Shading through Shape Evolution,"In this paper, we address the shape-from-shading problem by training deep networks with synthetic images. Unlike conventional approaches that combine deep learning and synthetic imagery, we propose an approach that does not need any external shape dataset to render synthetic images. Our approach consists of two synergistic processes: the evolution of complex shapes from simple primitives, and the training of a deep network for shape-from-shading. The evolution generates better shapes guided by the network training, while the training improves by using the evolved shapes. We show that our approach achieves state-of-the-art performance on a shape-from-shading benchmark.",http://arxiv.org/pdf/1712.02961v1
394,974,Spotlight,Weakly Supervised Instance Segmentation using Class Peak Response,"Yanzhao Zhou, UCAS, China; Yi Zhu, UCAS; Qixiang Ye, ; Qiang Qiu, ; Jianbin Jiao,",Weakly Supervised Instance Segmentation using Class Peak Response,"Weakly supervised instance segmentation with image-level labels, instead of expensive pixel-level masks, remains unexplored. In this paper, we tackle this challenging problem by exploiting class peak responses to enable a classification network for instance mask extraction. With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location. We observed that local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside each instance. Motivated by this, we first design a process to stimulate peaks to emerge from a class response map. The emerged peaks are then back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. We refer to the above maps generated from class peak responses as Peak Response Maps (PRMs). PRMs provide a fine-detailed instance-level representation, which allows instance masks to be extracted even with some off-the-shelf methods. To the best of our knowledge, we for the first time report results for the challenging image-level supervised instance segmentation task. Extensive experiments show that our method also boosts weakly supervised pointwise localization as well as semantic segmentation performance, and reports state-of-the-art results on popular benchmarks, including PASCAL VOC 2012 and MS COCO.",http://arxiv.org/pdf/1804.00880v1
395,1410,Spotlight,Collaborative and Adversarial Network for Unsupervised domain adaptation,"Weichen Zhang, The University of Sydney; Wanli Ouyang, The University of Sydney; Dong Xu, ; Wen Li, ETH",,,
396,1643,Spotlight,Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines,"Shuqin Xie, SJTU; Cewu Lu, Shanghai Jiao Tong University; Zitian Chen, Fudan University; Chao Xu, Shanghai Jiao Tong University",,,
397,1919,Spotlight,Teaching Categories to Human Learners with Visual Explanations,"Oisin Mac Aodha, Caltech; Shihan Su, Caltech; Yuxin Chen, Caltech; Pietro Perona, California Institute of Technology, USA; Yisong Yue,",Teaching Categories to Human Learners with Visual Explanations,"We study the problem of computer-assisted teaching with explanations. Conventional approaches for machine teaching typically only provide feedback at the instance level e.g., the category or label of the instance. However, it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student's ability to learn a new concept. To address these existing limitations, we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information. In the case of images, we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label. Experiments on human learners illustrate that, on average, participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods.",http://arxiv.org/pdf/1802.06924v1
398,2354,Oral,Density Adaptive Point Set Registration,"Felix Järemo Lawin, Linköping University; Martin Danelljan, ; Fahad Khan, Computer Vision Laboratory, Linkoping University , Sweden; Per-Erik Forssen, Linkoping University; Michael Felsberg, Link_ping University",Density Adaptive Point Set Registration,"Probabilistic methods for point set registration have demonstrated competitive results in recent years. These techniques estimate a probability distribution model of the point clouds. While such a representation has shown promise, it is highly sensitive to variations in the density of 3D points. This fundamental problem is primarily caused by changes in the sensor location across point sets. We revisit the foundations of the probabilistic registration paradigm. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework. Our density-adaptive registration successfully handles severe density variations commonly encountered in terrestrial Lidar applications. We perform extensive experiments on several challenging real-world Lidar datasets. The results demonstrate that our approach outperforms state-of-the-art probabilistic methods for multi-view registration, without the need of re-sampling.",http://arxiv.org/pdf/1804.01495v1
399,483,Oral,Left-Right Comparative Recurrent Model for Stereo Matching,"Zequn Jie, ; Pengfei Wang, NUS; Yonggen Ling, Tencent; Bo Zhao, ; Jiashi Feng, ; Wei Liu,",Left-Right Comparative Recurrent Model for Stereo Matching,"Leveraging the disparity information from both left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model, demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model.",http://arxiv.org/pdf/1804.00796v1
400,2567,Oral,Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View,"Shuran Song, Princeton ; Andy Zeng, Princeton; Angel Chang, Stanford University; Manolis Savva, ; Silvio Savarese, ; Thomas Funkhouser, Princeton",Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View,"We present Im2Pano3D, a convolutional neural network that generates a dense prediction of 3D structure and a probability distribution of semantic labels for a full 360 panoramic view of an indoor scene when given only a partial observation (<= 50%) in the form of an RGB-D image. To make this possible, Im2Pano3D leverages strong contextual priors learned from large-scale synthetic and real-world indoor scenes. To ease the prediction of 3D structure, we propose to parameterize 3D surfaces with their plane equations and train the model to predict these parameters directly. To provide meaningful training supervision, we use multiple loss functions that consider both pixel level accuracy and global context consistency. Experiments demon- strate that Im2Pano3D is able to predict the semantics and 3D structure of the unobserved scene with more than 56% pixel accuracy and less than 0.52m average distance error, which is significantly better than alternative approaches.",http://arxiv.org/pdf/1712.04569v1
401,2612,Oral,Polarimetric Dense Monocular SLAM,"Luwei Yang, Simon Farser University; Feitong Tan, Simon Fraser University; Ao Li, Simon Fraser University; Zhaopeng Cui, Simon Fraser University; Yasutaka Furukawa, ; Ping Tan,",,,
402,2974,Spotlight,"A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation","Guillermo Gallego, University of Zurich; Henri Rebecq, University of Zurich; Davide Scaramuzza, University of Zurich","A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation","We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.",http://arxiv.org/pdf/1804.01306v1
403,3156,Spotlight,Modeling Facial Geometry using Compositional VAEs,"Timur Bagautdinov, ; Chenglei Wu, Oculus; Jason Saragih, Oculus Research; Pascal Fua, ; Yaser Sheikh,",,,
404,144,Spotlight,Tangent Convolutions for Dense Prediction in 3D,"Maxim Tatarchenko, Freiburg; Jaesik Park, Intel Labs; Qian-Yi Zhou, ABQ Technologies; Vladlen Koltun, Intel Labs",,,
405,1719,Spotlight,RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials,"Despoina Paschalidou, MPI Tuebingen; Carolin Schmitt, MPI Tuebingen; Osman Ulusoy, microsoft corporation; Luc Van Gool, KTH; Andreas Geiger, MPI Tuebingen / ETH Zuerich",,,
406,2792,Spotlight,Neural 3D Mesh Renderer,"Hiroharu Kato, Univ. Tokyo; Tatsuya Harada, University of Tokyo",Self-supervised Learning of Motion Capture,"Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.",http://arxiv.org/pdf/1712.01337v1
407,3491,Spotlight,Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation,"Dan Xu, ; Wei Wang, University of Trento; Hao Tang, University of Trento; Nicu Sebe, University of Trento; Elisa Ricci, U. Perugia",Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation,"Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.",http://arxiv.org/pdf/1803.11029v1
408,1099,Spotlight,Automatic 3D Indoor Scene Modeling from Single Panorama,"Yang Yang, University of Delaware; Shi Jin, ShanghaiTech University; Ruiyang Liu, ; Sing Bing Kang, Microsoft Research; Jingyi Yu, University of Delaware, USA",,,
409,4083,Spotlight,Extreme 3D Face Reconstruction: Looking Past Occlusions,"Anh  Tran, USC; Tal Hassner, Open Univ Israel; Iacopo Masi, USC; Gérard Medioni,",Extreme 3D Face Reconstruction: Seeing Through Occlusions,"Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down.",http://arxiv.org/pdf/1712.05083v2
410,2972,Spotlight,Beyond Gröbner Bases: Basis Selection for Minimal Solvers,"Viktor Larsson, Lund University; Magnus Oskarsson, Lund University Sweden; Kalle Astroem, Lund University; Alge Wallis, ; Zuzana Kukelova, Czech Technical University in Prague; Tomas Pajdla,",Extreme 3D Face Reconstruction: Seeing Through Occlusions,"Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down.",http://arxiv.org/pdf/1712.05083v2
411,3438,Spotlight,"Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images","Silvia Zuffi, IMATI-CNR; Angjoo Kanazawa, University of Maryland; Michael Black, Max Planck Institute for Intelligent Systems",,,
412,1880,Poster,Deep Cocktail Networks: Multi-source Unsupervised Domain Adaptation with Category Shift,"Ruijia Xu, Sun Yat-sen University; Ziliang Chen, Sun Yat-sen University; Wangmeng Zuo, Harbin Institute of Technology; Junjie Yan, ; Liang Lin,",Deep Cocktail Network: Multi-source Unsupervised Domain Adaptation with Category Shift,"Unsupervised domain adaptation (UDA) conventionally assumes labeled source samples coming from a single underlying source distribution. Whereas in practical scenario, labeled data are typically collected from diverse sources. The multiple sources are different not only from the target but also from each other, thus, domain adaptater should not be modeled in the same way. Moreover, those sources may not completely share their categories, which further brings a new transfer challenge called category shift. In this paper, we propose a deep cocktail network (DCTN) to battle the domain and category shifts among multiple sources. Motivated by the theoretical results in \cite{mansour2009domain}, the target distribution can be represented as the weighted combination of source distributions, and, the multi-source unsupervised domain adaptation via DCTN is then performed as two alternating steps: i) It deploys multi-way adversarial learning to minimize the discrepancy between the target and each of the multiple source domains, which also obtains the source-specific perplexity scores to denote the possibilities that a target sample belongs to different source domains. ii) The multi-source category classifiers are integrated with the perplexity scores to classify target sample, and the pseudo-labeled target samples together with source samples are utilized to update the multi-source category classifier and the feature extractor. We evaluate DCTN in three domain adaptation benchmarks, which clearly demonstrate the superiority of our framework.",http://arxiv.org/pdf/1803.00830v1
413,2666,Poster,DOTA: A Large-scale Dataset for Object Detection in Aerial Images,"Gui-Song Xia, Wuhan University; Xiang Bai, Huazhong University of Science and Technology; Jian Ding, Wuhan University; Zhen Zhu, Huazhong University of Science and Technology; Serge Belongie, ; Jiebo Luo, University of Rochester; Mihai Datcu, ; Marcello Pelillo, University of Venice; Liangpei Zhang, Wuhan University",DOTA: A Large-scale Dataset for Object Detection in Aerial Images,"Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect $2806$ aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using $15$ common object categories. The fully annotated DOTA images contains $188,282$ instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.",http://arxiv.org/pdf/1711.10398v2
414,3272,Poster,Finding beans in burgers: Deep semantic-visual embedding with localization,"Patrick Perez, Technicolor Research; Matthieu Cord, ; Louis Chevallier, technicolor; Martin Engilberge, technicolor",Finding beans in burgers: Deep semantic-visual embedding with localization,"Several works have proposed to learn a two-path neural network that maps images and texts, respectively, to a same shared Euclidean space where geometry captures useful semantic relationships. Such a multi-modal embedding can be trained and used for various tasks, notably image captioning. In the present work, we introduce a new architecture of this type, with a visual path that leverages recent space-aware pooling mechanisms. Combined with a textual path which is jointly trained from scratch, our semantic-visual embedding offers a versatile model. Once trained under the supervision of captioned images, it yields new state-of-the-art performance on cross-modal retrieval. It also allows the localization of new concepts from the embedding space into any input image, delivering state-of-the-art result on the visual grounding of phrases.",http://arxiv.org/pdf/1804.01720v2
415,3408,Poster,Feature Super-Resolution: Make Machine See More Clearly,"Weimin Tan, Fudan University; Bo Yan, Fudan University; Bahetiyaer Bare, Fudan University",,,
416,3460,Poster,ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information,"Rodney LaLonde, University of Central Florida; Dong Zhang, University of Central Florida; Mubarak Shah, UCF",ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information,"Object detection in wide area motion imagery (WAMI) has drawn the attention of the computer vision research community for a number of years. WAMI proposes a number of unique challenges including extremely small object sizes, both sparse and densely-packed objects, and extremely large search spaces (large video frames). Nearly all state-of-the-art methods in WAMI object detection report that appearance-based classifiers fail in this challenging data and instead rely almost entirely on motion information in the form of background subtraction or frame-differencing. In this work, we experimentally verify the failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a heatmap-based fully convolutional neural network (CNN), and propose a novel two-stage spatio-temporal CNN which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in WAMI object detection. To reduce the large search space, the first stage (ClusterNet) takes in a set of extremely large video frames, combines the motion and appearance information within the convolutional architecture, and proposes regions of objects of interest (ROOBI). These ROOBI can contain from one to clusters of several hundred objects due to the large video frame size and varying object density in WAMI. The second stage (FoveaNet) then estimates the centroid location of all objects in that given ROOBI simultaneously via heatmap estimation. The proposed method exceeds state-of-the-art results on the WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped objects, as well as being the first proposed method in wide area motion imagery to detect completely stationary objects.",http://arxiv.org/pdf/1704.02694v2
417,3680,Poster,MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features,"Liang-Chieh Chen, ; Alexander Hermans, RWTH Aachen University; George Papandreou, Google Inc.; Florian Schroff, Google Inc.; Peng Wang, Baidu; Hartwig Adam, Google",MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features,"In this work, we tackle the problem of instance segmentation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction prediction. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining semantic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different semantic classes including background, while the direction prediction, estimating each pixel's direction towards its corresponding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incorporating recent successful methods from both segmentation and detection (i.e. atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance segmentation benchmark and shows comparable performance with other state-of-art models.",http://arxiv.org/pdf/1712.04837v1
418,367,Poster,Hashing as Tie-Aware Learning to Rank,"Kun He, Boston University; Fatih Cakir, Boston University; Sarah Bargal, Boston University; Stan Sclaroff, Boston University",Hashing as Tie-Aware Learning to Rank,"Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.",http://arxiv.org/pdf/1705.08562v3
419,449,Poster,Classification Driven Dynamic Image Enhancement,"Vivek Sharma, Karlsruhe Institute of Technology; Ali Diba, ; Davy Neven, KU Leuven; Michael Brown, York University; Luc Van Gool, KTH; Rainer Stiefelhagen, Karlsruhe Institute of Technology",Classification Driven Dynamic Image Enhancement,"Convolutional neural networks rely on image texture and structure to serve as discriminative features to classify the image content. Image enhancement techniques can be used as preprocessing steps to help improve the overall image quality and in turn improve the overall effectiveness of a CNN. Existing image enhancement methods, however, are designed to improve the perceptual quality of an image for a human observer. In this paper, we are interested in learning CNNs that can emulate image enhancement and restoration, but with the overall goal to improve image classification and not necessarily human perception. To this end, we present a unified CNN architecture that uses a range of enhancement filters that can enhance image-specific details via end-to-end dynamic filter learning. We demonstrate the effectiveness of this strategy on four challenging benchmark datasets for fine-grained, object, scene, and texture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD. Experiments using our proposed enhancement show promising results on all the datasets. In addition, our approach is capable of improving the performance of all generic CNN architectures.",http://arxiv.org/pdf/1710.07558v3
420,589,Poster,Knowledge Aided Consistency for Weakly Supervised Phrase Grounding,"Kan Chen, Univ. of Southern California; Jiyang Gao, ; Ram Nevatia,",Knowledge Aided Consistency for Weakly Supervised Phrase Grounding,"Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.",http://arxiv.org/pdf/1803.03879v1
421,603,Poster,Who Let The Dogs Out? Modeling Dog Behavior From Visual Data,"KIANA EHSANI, 1993; Hessam Bagherinezhad, University of Washington; Joe Redmon, University of Washington; Roozbeh Mottaghi, Allen Institute for Artificial Intelligence; Ali Farhadi,",,,
422,910,Poster,Pseudo-Mask Augmented Object Detection,"Xiangyun  Zhao, Northwestern University; Shuang Liang, Tongji University; Yichen Wei, Microsoft Research Asia",Pseudo Mask Augmented Object Detection,"In this work, we present a novel and effective framework to facilitate object detection with the instance-level segmentation information that is only supervised by bounding box annotation. Starting from the joint object detection and instance segmentation network, we propose to recursively estimate the pseudo ground-truth object masks from the instance-level object segmentation network training, and then enhance the detection network with top-down segmentation feedbacks. The pseudo ground truth mask and network parameters are optimized alternatively to mutually benefit each other. To obtain the promising pseudo masks in each iteration, we embed a graphical inference that incorporates the low-level image appearance consistency and the bounding box annotations to refine the segmentation masks predicted by the segmentation network. Our approach progressively improves the object detection performance by incorporating the detailed pixel-wise information learned from the weakly-supervised segmentation network. Extensive evaluation on the detection task in PASCAL VOC 2007 and 2012 [12] verifies that the proposed approach is effective.",http://arxiv.org/pdf/1803.05858v2
423,1271,Poster,Left/Right Asymmetric Layer Skippable Networks,"Changmao Cheng, Fudan University; Yanwei Fu, fudan; Yu-Gang Jiang, Fudan University; Wei Liu, ; wenlian Lu, Fudan; Jianfeng Feng, fudan university; Xiangyang Xue,",,,
424,1342,Poster,Memory Matching Networks for One-Shot Image Recognition,"Qi Cai, University of Science and Technology of China; Yingwei Pan, University of Science and Technology of China; Ting Yao, Microsoft Research Asia; Chenggang Yan, Hangzhou Dianzi University, China; Tao Mei, Microsoft Research Asia",,,
425,1352,Poster,IQA: Visual Question Answering in Interactive Environments,"Daniel Gordon, University of Washington; Ali Farhadi, ; Aniruddha Kembhavi, Allen Institute for Artificial Intelligence; Dieter Fox, University of Washington; Mohammad Rastegari, AI2; Joe Redmon, University of Washington",IQA: Visual Question Answering in Interactive Environments,"We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: ""Are there any apples in the fridge?"" The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98.",http://arxiv.org/pdf/1712.03316v2
426,1432,Poster,Pose Transferrable Person Re-Identification,"Jinxian Liu, Shanghai Jiao Tong University; Yichao Yan, Shanghai Jiao Tong University; Bingbing Ni, ; Peng Zhou, Sjtu; Shuo Cheng, SJTU; jianguo Hu, Minivision",,,
427,1502,Poster,Large Scale Fine-Grained Categorization and the Effectiveness of Domain-Specific Transfer Learning,"Yin Cui, CornellTech; Yang Song, Google; Chen Sun, Google; Andrew Howard, Google; Serge Belongie,",,,
428,1565,Poster,Data Distillation: Towards Omni-Supervised Learning,"Ilija Radosavovic, Facebook AI Research; Piotr Dollar, Facebook AI Research, Menlo Park, USA; Ross Girshick, ; Georgia Gkioxari, Facebook; Kaiming He,",Data Distillation: Towards Omni-Supervised Learning,"We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.",http://arxiv.org/pdf/1712.04440v1
429,1765,Poster,Object Referring in Videos with Language and Human Gaze,"Arun Balajee Vasudevan , ETH Zurich; Dengxin Dai, ETH Zurich; Luc Van Gool, KTH",Object Referring in Videos with Language and Human Gaze,"We investigate the problem of object referring (OR) i.e. to localize a target object in a visual scene coming with a language description. Humans perceive the world more as continued video snippets than as static images, and describe objects not only by their appearance, but also by their spatio-temporal context and motion features. Humans also gaze at the object when they issue a referring expression. Existing works for OR mostly focus on static images only, which fall short in providing many such cues. This paper addresses OR in videos with language and human gaze. To that end, we present a new video dataset for OR, with 30, 000 objects over 5, 000 stereo video sequences annotated for their descriptions and gaze. We further propose a novel network model for OR in videos, by integrating appearance, motion, gaze, and spatio-temporal context into one network. Experimental results show that our method effectively utilizes motion cues, human gaze, and spatio-temporal context. Our method outperforms previousOR methods. For dataset and code, please refer https://people.ee.ethz.ch/~arunv/ORGaze.html.",http://arxiv.org/pdf/1801.01582v2
430,2123,Poster,Feature Selective Networks for Object Detection,"Yao Zhai, University of Science and Technology of China; Jingjing Fu, ; Yan Lu, ; Houqiang Li,",Deep Regionlets for Object Detection,"In this paper, we propose a novel object detection framework named ""Deep Regionlets"" by establishing a bridge between deep neural networks and conventional detection schema for accurate generic object detection. Motivated by the advantages of regionlets on modeling object deformation and multiple aspect ratios, we incorporate regionlet into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network serves as a guidance on where to select regions to learn the features from. The regionlet learning module focuses on local feature selection and transformation to alleviate local variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we further design a ""gating network"" within the regionlet leaning module to enable soft regionlet selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We perform ablation studies on its behavior and conduct extensive experiments on the PASCAL VOC and Microsoft COCO dataset. The proposed framework outperforms state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.",http://arxiv.org/pdf/1712.02408v2
431,2312,Poster,Learning a Discriminative Filter Bank within a CNN for Fine-grained Recognition,"Yaming Wang, University of Maryland; Vlad Morariu, University of Maryland; Larry Davis, University of Maryland, USA",,,
432,2337,Poster,Grounding Referring Expressions in Images by Variational Context,"Hanwang Zhang, Columbia University; Yulei Niu, Renmin University of China; Shih-Fu Chang,",Grounding Referring Expressions in Images by Variational Context,"We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., ""largest elephant standing behind baby elephant"". This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context --- visual attributes (e.g., ""largest"", ""baby"") and relationships (e.g., ""behind"") that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences the estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced, resulting in better localization of referent. We develop a novel cue-specific language-vision embedding network that learns this reciprocity model end-to-end. We also extend the model to the unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings.",http://arxiv.org/pdf/1712.01892v2
433,2764,Poster,Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams,"Daesik Kim, Seoul National University; YoungJoon Yoo, ; JeeSoo Kim, Seoul national university; SangKuk Lee, Seoul National University; Nojun Kwak, Seoul National University",Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams,"In this work, we introduce a new algorithm for analyzing a diagram, which contains visual and textual information in an abstract and integrated way. Whereas diagrams contain richer information compared with individual image-based or language-based data, proper solutions for automatically understanding them have not been proposed due to their innate characteristics of multi-modality and arbitrariness of layouts. To tackle this problem, we propose a unified diagram-parsing network for generating knowledge from diagrams based on an object detector and a recurrent neural network designed for a graphical structure. Specifically, we propose a dynamic graph-generation network that is based on dynamic memory and graph theory. We explore the dynamics of information in a diagram with activation of gates in gated recurrent unit (GRU) cells. On publicly available diagram datasets, our model demonstrates a state-of-the-art result that outperforms other baselines. Moreover, further experiments on question answering shows potentials of the proposed method for various applications.",http://arxiv.org/pdf/1711.09528v1
434,3120,Poster,A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation,"Lukas Rahmann, ; Riccardo Roveri, ETH Zurich; Cengiz Oztireli, ; Markus Gross,",,,
435,3583,Poster,Towards dense object tracking in a 2D honeybee hive,"Katarzyna Bozek, Okinawa Institute of Science a; Laetitia Hebert, ; Alexander Mikheyev, ; Greg Stephens, OIST Graduate University and Vrije Universiteit Amsterdam",Towards dense object tracking in a 2D honeybee hive,"From human crowds to cells in tissue, the detection and efficient tracking of multiple objects in dense configurations is an important and unsolved problem. In the past, limitations of image analysis have restricted studies of dense groups to tracking a single or subset of marked individuals, or to coarse-grained group-level dynamics, all of which yield incomplete information. Here, we combine convolutional neural networks (CNNs) with the model environment of a honeybee hive to automatically recognize all individuals in a dense group from raw image data. We create new, adapted individual labeling and use the segmentation architecture U-Net with a loss function dependent on both object identity and orientation. We additionally exploit temporal regularities of the video recording in a recurrent manner and achieve near human-level performance while reducing the network size by 94% compared to the original U-Net architecture. Given our novel application of CNNs, we generate extensive problem-specific image data in which labeled examples are produced through a custom interface with Amazon Mechanical Turk. This dataset contains over 375,000 labeled bee instances across 720 video frames at 2 FPS, representing an extensive resource for the development and testing of tracking methods. We correctly detect 96% of individuals with a location error of ~7% of a typical body dimension, and orientation error of 12 degrees, approximating the variability of human raters. Our results provide an important step towards efficient image-based dense object tracking by allowing for the accurate determination of object location and orientation across time-series image data efficiently within one network architecture.",http://arxiv.org/pdf/1712.08324v1
436,3887,Poster,Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty,"Apratim Bhattacharyya, MPI Informatics; Bernt Schiele, MPI Informatics Germany; Mario Fritz, MPI, Saarbrucken, Germany",Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty,"Progress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.",http://arxiv.org/pdf/1711.09026v1
437,5,Poster,Single-Shot Refinement Neural Network for Object Detection,"Shifeng Zhang, CBSR, NLPR, CASIA; Longyin Wen, GE Global Research Center; Xiao Bian, ; Zhen Lei, Chinese Academy of Sciences ; Stan Li,",Single-Shot Refinement Neural Network for Object Detection,"For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multi-task loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet",http://arxiv.org/pdf/1711.06897v3
438,7,Poster,Video Captioning via Hierarchical Reinforcement Learning,"Xin Wang, UCSB; Wenhu Chen, ; Jiawei Wu, UCSB; Yuan-Fang Wang, UCSB; William Yang Wang, UCSB",Video Captioning via Hierarchical Reinforcement Learning,"Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.",http://arxiv.org/pdf/1711.11135v3
439,21,Poster,Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge,"Damien Teney, Unversity of Adelaide; Peter Anderson, Australian National University; Xiaodong He, ; Anton Van den Hengel, University of Adelaide",Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge,"This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.",http://arxiv.org/pdf/1708.02711v1
440,47,Poster,Learning to Segment Every Thing,"Ronghang Hu, UC Berkeley; Piotr Dollar, Facebook AI Research, Menlo Park, USA; Kaiming He, ; Trevor Darrell, UC Berkeley, USA; Ross Girshick,",Learning to Segment Every Thing,"Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.",http://arxiv.org/pdf/1711.10370v2
441,124,Poster,Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval,"Chao Li, Xidian University; Cheng Deng, Xidian University; Ning Li, Xidian University; Wei Liu, ; Dacheng Tao, University of Sydney; Xinbo Gao,",Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval,"Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (\textbf{SSAH}) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods.",http://arxiv.org/pdf/1804.01223v1
442,234,Poster,Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries,"Bohan Zhuang, The University of Adelaide; Qi Wu, University of Adelaide; Chunhua Shen, University of Adelaide; Ian Reid, ; Anton Van den Hengel, University of Adelaide",Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries,"Recognising objects according to a pre-defined fixed set of class labels has been well studied in the Computer Vision. There are a great many practical applications where the subjects that may be of interest are not known beforehand, or so easily delineated, however. In many of these cases natural language dialog is a natural way to specify the subject of interest, and the task achieving this capability (a.k.a, Referring Expression Comprehension) has recently attracted attention. To this end we propose a unified framework, the ParalleL AttentioN (PLAN) network, to discover the object in an image that is being referred to in variable length natural expression descriptions, from short phrases query to long multi-round dialogs. The PLAN network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates. Furthermore, the attention mechanisms are recurrent, making the referring process visualizable and explainable. The attended information from these dual sources are combined to reason about the referred object. These two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input, such as RefCOCO, RefCOCO+ and GuessWhat?!.",http://arxiv.org/pdf/1711.06370v1
443,397,Poster,Zigzag Learning for Weakly Supervised Object Detection,"Xiaopeng Zhang, National University of Singapore; Jiashi Feng, ; Hongkai Xiong, Shanghai Jiao Tong University; Qi Tian,",,,
444,505,Poster,Attentive Fashion Grammar Network for Fashion Landmark Detection and Clothing Category Classification,"Wenguan Wang, Beijing Institute of Technology; Yuanlu Xu, University of California, Los Angeles; Jianbing Shen, Beijing Institute of Technolog; Song-Chun Zhu,",,,
445,4099,Poster,A Robust Generative Framework for Generalized Zero-Shot Learning,"Vinay Verma, IIT Kanpur; Gundeep Arora, IIT Kanpur; Ashish Mishra, IIT MADRAS; Piyush  Rai, IIT Kanpur",,,
446,4112,Poster,Partially Shared Multi-Task Convolutional Neural Network with Local Constraint for Face Attribute Learning,"Jiajiong Cao, ; Yingming Li, Zhejiang University; Zhongfei Zhang,",,,
447,4227,Poster,SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks,"Julian Faraone, University of Sydney; Nicholas  Fraser, Xilinx; Michaela Blott, Xilinx; Philip Leong,",,,
448,83,Poster,DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems,"Florian Bernard, ; Christian Theobalt, MPI Informatics; Michael Moeller, University of Siegen",,,
449,304,Poster,Deep Mutual Learning,"Ying Zhang, QMUL; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh; Huchuan Lu, Dalian University of Technology",Information Planning for Text Data,"Information planning enables faster learning with fewer training examples. It is particularly applicable when training examples are costly to obtain. This work examines the advantages of information planning for text data by focusing on three supervised models: Naive Bayes, supervised LDA and deep neural networks. We show that planning based on entropy and mutual information outperforms random selection baseline and therefore accelerates learning.",http://arxiv.org/pdf/1802.03360v3
450,588,Poster,Coupled End-to-end Transfer Learning with Generalized Fisher Information,"Shixing Chen, Wayne State University; Caojin Zhang, Wayne State University; Ming Dong,",,,
451,963,Poster,Residual Parameter Transfer for Deep Domain Adaptation,"Artem Rozantsev, EPFL; Mathieu Salzmann, EPFL; Pascal Fua,",Efficient parametrization of multi-domain deep neural networks,"A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal, fixed feature extractors that, used as the first stage of any deep network, work well for several tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks.   To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but differing only by a small number of parameters. We study different designs for such parametrizations, including series and parallel residual adapters, joint adapter compression, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.",http://arxiv.org/pdf/1803.10082v1
452,1126,Poster,High-order tensor regularization with application to attribute ranking,"Kwang In Kim, University of Bath; Juhyun Park, Lancaster University; James Tompkin, Brown University",,,
453,1294,Poster,Learning to Localize Sound Source in Visual Scenes,"Arda Senocak, KAIST; Junsik Kim, Korea Advanced Institute of Science and Technology (KAIST); Tae-Hyun Oh, MIT; Ming-Hsuan Yang, UC Merced; In So Kweon, KAIST",Learning to Localize Sound Source in Visual Scenes,"Visual events are usually accompanied by sounds in our daily lives. We pose the question: Can the machine learn the correspondence between visual scene and the sound, and localize the sound source only by observing sound and visual scene pairs like human? In this paper, we propose a novel unsupervised algorithm to address the problem of localizing the sound source in visual scenes. A two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization. Moreover, although our network is formulated within the unsupervised learning framework, it can be extended to a unified architecture with a simple modification for the supervised and semi-supervised learning settings as well. Meanwhile, a new sound source dataset is developed for performance evaluation. Our empirical evaluation shows that the unsupervised method eventually go through false conclusion in some cases. We show that even with a few supervision, false conclusion is able to be corrected and the source of sound in a visual scene can be localized effectively.",http://arxiv.org/pdf/1803.03849v1
454,1296,Poster,Dynamic Few-Shot Visual Learning without Forgetting,"Spyros Gidaris, Ecole des Ponts ParisTech ; Nikos Komodakis,",,,
455,1716,Poster,A General Two-Step Quantization Approach for Low-bit Neural Networks with High Accuracy,"Peisong Wang, CASIA; Qinghao Hu, Chinese Academy of Sciences; Yifan Zhang, CASIA; Jian Cheng, Chinese Academy of Sciences",,,
456,1904,Poster,Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks,"Nick Johnston, Google; Damien Vincent, google.com; David Minnen, google.com; Michele Covell, google.com; Saurabh Singh, Univ. of Illinois at Urbana-Champaign; Sung Jin Hwang, google.com; George Toderici, Google; Troy Chinen, google.com; Joel Shor, google.com",Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks,"We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.",http://arxiv.org/pdf/1703.10114v1
457,2172,Poster,Conditional Probability Models for Deep Image Compression,"Eirikur Agustsson, ETH Zurich; Fabian Mentzer, ETHZ Zürich; Michael Tschannen, ETH Zurich; Radu Timofte, ETH Zurich; Luc Van Gool, KTH",Conditional Probability Models for Deep Image Compression,"Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state of the art in image compression. The key challenge in learning such networks is twofold: to deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: a 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach yields a state-of-the-art image compression system based on a simple convolutional auto-encoder.",http://arxiv.org/pdf/1801.04260v1
458,2257,Poster,Deep Diffeomorphic Transformer Networks,"Nicki Skafte Detlefsen, DTU; Oren Freifeld, Ben-Gurion University; Soren Hauberg, Technical University of Denmark","Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations","The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.",http://arxiv.org/pdf/1706.03078v3
459,2291,Poster,The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks,"Maxim Berman, ESAT-PSI, KU Leuven; Amal Rannen Triki, KU Leuven; Matthew Blaschko, KU Leuven","Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations","The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.",http://arxiv.org/pdf/1706.03078v3
460,2387,Poster,Generative Adversarial Perturbations,"Omid Poursaeed, Cornell University; Isay Katsman, Cornell University; Bicheng Gao, Shanghai Jiao Tong University; Serge Belongie,",Detecting Adversarial Perturbations with Saliency,"In this paper we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data. In the case of image classification model, saliency simply explain how the model make decisions by identifying significant pixels for prediction. A model shows wrong classification output always learns wrong features and shows wrong saliency as well. Our approach shows good performance on detecting adversarial perturbations. We quantitatively evaluate generalization ability of the detector, showing that detectors trained with strong adversaries perform well on weak adversaries.",http://arxiv.org/pdf/1803.08773v1
461,2742,Poster,Learning Strict Identity Mappings in Deep Residual Networks,"Xin Yu, University of Utah; Srikumar Ramalingam, ; Zhiding Yu, Carnegie Mellon University",Learning Strict Identity Mappings in Deep Residual Networks,"A family of super deep networks, referred to as residual networks or ResNet, achieved record-beating performance in various visual tasks such as image recognition, object detection, and semantic segmentation. The ability to train very deep networks naturally pushed the researchers to use enormous resources to achieve the best performance. Consequently, in many applications super deep residual networks were employed for just a marginal improvement in performance. In this paper, we propose epsilon-ResNet that allows us to automatically discard redundant layers, which produces responses that are smaller than a threshold epsilon, with a marginal or no loss in performance. The epsilon-ResNet architecture can be achieved using a few additional rectified linear units in the original ResNet. Our method does not use any additional variables nor numerous trials like other hyper-parameter optimization techniques. The layer selection is achieved using a single training process and the evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. In some instances, we achieve about 80% reduction in the number of parameters.",http://arxiv.org/pdf/1804.01661v2
462,2844,Poster,Geometric robustness of deep networks: analysis and improvement,"Can Kanbak, EPFL; Seyed-Mohsen Moosavi-Dezfooli, ; Pascal Frossard,",Geometric robustness of deep networks: analysis and improvement,"Deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations. However, there is no systematic method to measure the invariance properties of deep networks to such transformations. We propose ManiFool as a simple yet scalable algorithm to measure the invariance of deep networks. In particular, our algorithm measures the robustness of deep networks to geometric transformations in a worst-case regime as they can be problematic for sensitive applications. Our extensive experimental results show that ManiFool can be used to measure the invariance of fairly complex networks on high dimensional datasets and these values can be used for analyzing the reasons for it. Furthermore, we build on Manifool to propose a new adversarial training scheme and we show its effectiveness on improving the invariance properties of deep neural networks.",http://arxiv.org/pdf/1711.09115v1
463,2859,Poster,View Extrapolation of Human Body from a Single Image,"Hao Zhu, Nanjing University; hao Su, ; Peng Wang, Baidu; Xun Cao, EE Department, Nanjing Univ; Ruigang Yang, University of Kentucky",,,
464,3057,Poster,Geometry Aware Optimization for Deep Learning: The Good Practice,"SOUMAVA KUMAR ROY, AUSTRALIAN NATIONAL UNIVERSITY; Zakaria Mhammedi, Data61, CSIRO; Mehrtash Harandi, Australian National University",,,
465,3164,Poster,PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition,"Mikaela Angelina Uy, NUS; Gim Hee Lee, National University of SIngapore",,,
466,3303,Poster,An Efficient and Provable Approach for Mixture Proportion Estimation Using Linear Independence Assumption,"Xiyu Yu, The University of Sydney; Tongliang Liu, The University of Sydney; Mingming Gong, ; Kayhan Batmanghelich, University of Pittsburgh; Dacheng Tao, University of Sydney",,,
467,3333,Poster,VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,"Yin Zhou, Lawrence Berkeley National Lab; Oncel Tuzel,",VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,"Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.",http://arxiv.org/pdf/1711.06396v1
468,3335,Poster,Image to Image Translation for Domain Adaptation,"Zak Murez, UCSD; Soheil Kolouri, HRL Laboratories, LLC; David Kriegman, University of California at San Diego; Ravi Ramamoorthi, University of California, San Diego; Kyungnam Kim, HRL Laboratories",Iterative Inversion of Deformation Vector Fields with Feedback Control,"Purpose: Often, the inverse deformation vector field (DVF) is needed together with the corresponding forward DVF in 4D reconstruction and dose calculation, adaptive radiation therapy, and simultaneous deformable registration. This study aims at improving both accuracy and efficiency of iterative algorithms for DVF inversion, and advancing our understanding of divergence and latency conditions. Method: We introduce a framework of fixed-point iteration algorithms with active feedback control for DVF inversion. Based on rigorous convergence analysis, we design control mechanisms for modulating the inverse consistency (IC) residual of the current iterate, to be used as feedback into the next iterate. The control is designed adaptively to the input DVF with the objective to enlarge the convergence area and expedite convergence. Three particular settings of feedback control are introduced: constant value over the domain throughout the iteration; alternating values between iteration steps; and spatially variant values. We also introduce three spectral measures of the displacement Jacobian for characterizing a DVF. These measures reveal the critical role of what we term the non-translational displacement component (NTDC) of the DVF. We carry out inversion experiments with an analytical DVF pair, and with DVFs associated with thoracic CT images of 6 patients at end of expiration and end of inspiration. Results: NTDC-adaptive iterations are shown to attain a larger convergence region at a faster pace compared to previous non-adaptive DVF inversion iteration algorithms. By our numerical experiments, alternating control yields smaller IC residuals and inversion errors than constant control. Spatially variant control renders smaller residuals and errors by at least an order of magnitude, compared to other schemes, in no more than 10 steps. Inversion results also show remarkable quantitative agreement with analysis-based predictions. Conclusion: Our analysis captures properties of DVF data associated with clinical CT images, and provides new understanding of iterative DVF inversion algorithms with a simple residual feedback control. Adaptive control is necessary and highly effective in the presence of non-small NTDCs. The adaptive iterations or the spectral measures, or both, may potentially be incorporated into deformable image registration methods.",http://arxiv.org/pdf/1610.08589v4
469,3427,Poster,"Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation","Mark Sandler, Google; Andrew Howard, Google; Menglong Zhu, ; Andrey  Zhmoginov, Google; Liang-Chieh Chen,",,,
470,3941,Poster,Im2Struct: Recovering 3D Shape Structure from a Single RGB Image,"Chengjie Niu, National University of Defense Technology; Jun Li, ; Kai Xu, NUDT & Princeton Univeristy",,,
471,4110,Poster,Trust your Model: Light Field Depth Estimation with inline Occlusion Handling,"Hendrik Schilling, Universität Heidelberg; Maximilian Diebold, Heidelberg University; Carsten Rother, University of Heidelberg; Bernd Jähne, University of Heidelberg",,,
472,67,Poster,Baseline Desensitizing In Translation Averaging,"Bingbing Zhuang, National University of Singapore; Loong Fah Cheong, National University of Singapore; Gim Hee Lee, National University of SIngapore",,,
473,1573,Poster,Neighbors Do Help: Deeply Exploiting Local Structures of Point Clouds,"Yiru Shen, Clemson University; Chen Feng, MERL; Yaoqing Yang, Carnegie Mellon University; Dong Tian, Mitsubishi Electric Research Laboratories",,,
474,1915,Poster,Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs,"Loic Landrieu, IGN; Martin Simonovsky, Universite Paris Est, ENPC",Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs,"We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).",http://arxiv.org/pdf/1711.09869v2
475,2018,Poster,Very Large-Scale Global SfM by Distributed Motion Averaging,"Siyu Zhu, HKUST; Runze Zhang, HKUST; Lei Zhou, HKUST; Tianwei Shen, HKUST; Tian Fang, HKUST; Ping Tan, ; Long Quan, The Hong Kong University of  Science and Technology, Hong Kong",,,
476,2835,Poster,ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans,"Angela Dai, ; Daniel Ritchie, Brown University; Martin Bokeloh, Google; Scott Reed, Google; Juergen Sturm, Google; Matthias Nießner, Technical University of Munich",ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans,"We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin.",http://arxiv.org/pdf/1712.10215v2
477,3373,Poster,Composing Two Objects of Interest for Flying Camera Photography,"ZIQUAN LAN, NUS; David Hsu, NUS; Gim Hee Lee, National University of SIngapore",,,
478,1428,Poster,Reflection Removal for Large-Scale 3D Point Clouds,"Jae-Seong Yun, UNIST; Jae-Young Sim, UNIST",,,
479,100,Poster,Attentional ShapeContextNet for Point Could Recognition,"Saining Xie, UCSD; Sainan Liu, UCSD; Zeyu Chen, UCSD; Zhuowen Tu, UCSD, USA",,,
480,2437,Poster,Geometry-aware Deep Network for Single-Image Novel View Synthesis,"Miaomiao Liu, Data61,CSIRO; Xuming He, ShanghaiTech; Mathieu Salzmann, EPFL",,,
481,2599,Poster,InverseFaceNet: Deep Monocular Inverse Face Rendering at over 250 Hz,"Hyeongwoo Kim, MPII; Michael Zollhöfer, MPI Informatics; Ayush Tewari, MPI Informatics; Justus Thies, Technical University of Munich; Christian Richardt, University of Bath; Christian Theobalt, MPI Informatics",,,
482,2633,Poster,Sparse Photometric 3D Face Reconstruction Guided by Morphable Models,"Xuan Cao, ShanghaiTech University; Zhang Chen, ShanghaiTech University; jingyi Yu, Shanghai Tech University; Anpei Chen,",Sparse Photometric 3D Face Reconstruction Guided by Morphable Models,"We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and latest advances on face registration/modeling from a single image. We observe that 3D morphable faces approach provides a reasonable geometry proxy for light position calibration. Specifically, we develop a robust optimization technique that can calibrate per-pixel lighting direction and illumination at a very high precision without assuming uniform surface albedos. Next, we apply semantic segmentation on input images and the geometry proxy to refine hairy vs. bare skin regions using tailored filters. Experiments on synthetic and real data show that by using a very small set of images, our technique is able to reconstruct fine geometric details such as wrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing movie quality productions.",http://arxiv.org/pdf/1711.10870v1
483,3350,Poster,Texture Mapping for 3D Reconstruction with RGB-D Sensor,"Yanping Fu, WuHan University; Qingan Yan, JD.com; Long Yang, Northwest A&F; University; Jie Liao , WuHan University; Chunxia Xiao, Wuhan University",An Integrated Platform for Live 3D Human Reconstruction and Motion Capturing,"The latest developments in 3D capturing, processing, and rendering provide means to unlock novel 3D application pathways. The main elements of an integrated platform, which target tele-immersion and future 3D applications, are described in this paper, addressing the tasks of real-time capturing, robust 3D human shape/appearance reconstruction, and skeleton-based motion tracking. More specifically, initially, the details of a multiple RGB-depth (RGB-D) capturing system are given, along with a novel sensors' calibration method. A robust, fast reconstruction method from multiple RGB-D streams is then proposed, based on an enhanced variation of the volumetric Fourier transform-based method, parallelized on the Graphics Processing Unit, and accompanied with an appropriate texture-mapping algorithm. On top of that, given the lack of relevant objective evaluation methods, a novel framework is proposed for the quantitative evaluation of real-time 3D reconstruction systems. Finally, a generic, multiple depth stream-based method for accurate real-time human skeleton tracking is proposed. Detailed experimental results with multi-Kinect2 data sets verify the validity of our arguments and the effectiveness of the proposed system and methodologies.",http://arxiv.org/pdf/1712.03084v1
484,3599,Poster,Learning Less is More - 6D Camera Localization via 3D Surface Regression,"Eric Brachmann, TU Dresden; Carsten Rother, University of Heidelberg",Learning Less is More - 6D Camera Localization via 3D Surface Regression,"Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints.",http://arxiv.org/pdf/1711.10228v2
485,1773,Poster,Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images,"Mahdi Rad, TUG; Markus Oberweger, ; Vincent Lepetit, TU Graz",Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images,"We propose a simple and efficient method for exploiting synthetic images when training a Deep Network to predict a 3D pose from an image. The ability of using synthetic images for training a Deep Network is extremely valuable as it is easy to create a virtually infinite training set made of such images, while capturing and annotating real images can be very cumbersome. However, synthetic images do not resemble real images exactly, and using them for training can result in suboptimal performance. It was recently shown that for exemplar-based approaches, it is possible to learn a mapping from the exemplar representations of real images to the exemplar representations of synthetic images. In this paper, we show that this approach is more general, and that a network can also be applied after the mapping to infer a 3D pose: At run time, given a real image of the target object, we first compute the features for the image, map them to the feature space of synthetic images, and finally use the resulting features as input to another network which predicts the 3D pose. Since this network can be trained very effectively by using synthetic images, it performs very well in practice, and inference is faster and more accurate than with an exemplar-based approach. We demonstrate our approach on the LINEMOD dataset for 3D object pose estimation from color images, and the NYU dataset for 3D hand pose estimation from depth maps. We show that it allows us to outperform the state-of-the-art on both datasets.",http://arxiv.org/pdf/1712.03904v2
486,3630,Poster,Indoor RGB-D Compass from a Single Line and Plane,"Pyojin Kim, Seoul National University; Brian Coltin, NASA Ames Research Center; H. Jin Kim,",,,
487,3026,Poster,Sim2Real View Invariant Visual Servoing by Recurrent Control,"Fereshteh Sadeghi, University of Washington; Alexander Toshev, Google; Sergey Levine, UC Berkeley",Sim2Real View Invariant Visual Servoing by Recurrent Control,"Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints and angles, even in the presence of optical distortions. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we study how viewpoint-invariant visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. The problem that must be solved by this controller is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing system must use its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to most visual servoing methods, which either assume known dynamics or require a calibration phase. We show how we can learn this recurrent controller using simulated data and a reinforcement learning objective. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: https://fsadeghi.github.io/Sim2RealViewInvariantServo",http://arxiv.org/pdf/1712.07642v1
488,3349,Poster,DocUNet: Document Image Unwarping via A Stacked U-Net,"Ke Ma, Stony Brook University; Zhixin Shu, Stony Brook University; Xue Bai, Megvii Inc; Jue Wang, Megvii; Dimitris Samaras,",,,
489,3882,Poster,Analysis of Hand Segmentation in the Wild,"Aisha Urooj, University of Central Florida; Ali Borji, UCF",Analysis of Hand Segmentation in the Wild,"A large number of works in egocentric vision have concentrated on action and object recognition. Detection and segmentation of hands in first-person videos, however, has less been explored. For many applications in this domain, it is necessary to accurately segment not only hands of the camera wearer but also the hands of others with whom he is interacting. Here, we take an in-depth look at the hand segmentation problem. In the quest for robust hand segmentation methods, we evaluated the performance of the state of the art semantic segmentation methods, off the shelf and fine-tuned, on existing datasets. We fine-tune RefineNet, a leading semantic segmentation method, for hand segmentation and find that it does much better than the best contenders. Existing hand segmentation datasets are collected in the laboratory settings. To overcome this limitation, we contribute by collecting two new datasets: a) EgoYouTubeHands including egocentric videos containing hands in the wild, and b) HandOverFace to analyze the performance of our models in presence of similar appearance occlusions. We further explore whether conditional random fields can help refine generated hand segmentations. To demonstrate the benefit of accurate hand maps, we train a CNN for hand-based activity recognition and achieve higher accuracy when a CNN was trained using hand maps produced by the fine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for fine-grained action recognition and show that an accuracy of 58.6% can be achieved by just looking at a single hand pose which is much better than the chance level (12.5%).",http://arxiv.org/pdf/1803.03317v2
490,4023,Poster,Automatic Map Inference from Aerial Images,"Favyen Bastani, MIT CSAIL; Songtao He, MIT CSAIL; Mohammad Alizadeh, MIT CSAIL; Hari Balakrishnan, MIT CSAIL; Sam Madden, MIT CSAIL; Sanjay Chawla, Qatar Computing Research Institute; Sofiane Abbar, Qatar Computing Research Institute; David DeWitt, MIT CSAIL",Unthule: An Incremental Graph Construction Process for Robust Road Map Extraction from Aerial Images,"The availability of highly accurate maps has become crucial due to the increasing importance of location-based mobile applications as well as autonomous vehicles. However, mapping roads is currently an expensive and human-intensive process. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates (poor precision) because noisy CNN outputs are difficult to correct.   We propose a novel approach, Unthule, to construct highly accurate road maps from aerial images. In contrast to prior work, Unthule uses an incremental search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We train the CNN to output the direction of roads traversing a supplied point in the aerial imagery, and then use this CNN to incrementally construct the graph. We compare our approach with a segmentation method on fifteen cities, and find that Unthule has a 45% lower error rate in identifying junctions across these cities.",http://arxiv.org/pdf/1802.03680v1
491,4286,Poster,Alternating-Stereo VINS: Observability Analysis and Performance Evaluation,"Mrinal Kanti Paul, Google; Stergios Roumeliotis, Google",,,
492,310,Poster,Soccer on Your Tabletop,"Konstantinos Rematas, University of Washington; Ira Kemelmacher, ; Brian Curless, Washington; Steve Seitz, Washington/Google",,,
493,1113,Poster,EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation by Using Epipolar Geometry,"Changha Shin, Yonsei Univ; Hae-Gon Jeon, KAIST; Youngjin Yoon , ; InSo Kweon, ; Seon Joo Kim, Yonsei University",EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images,"Light field cameras capture both the spatial and the angular properties of light rays in space. Due to its property, one can compute the depth from light fields in uncontrolled lighting environments, which is a big advantage over active sensing devices. Depth computed from light fields can be used for many applications including 3D modelling and refocusing. However, light field images from hand-held cameras have very narrow baselines with noise, making the depth estimation difficult. any approaches have been proposed to overcome these limitations for the light field depth estimation, but there is a clear trade-off between the accuracy and the speed in these methods. In this paper, we introduce a fast and accurate light field depth estimation method based on a fully-convolutional neural network. Our network is designed by considering the light field geometry and we also overcome the lack of training data by proposing light field specific data augmentation methods. We achieved the top rank in the HCI 4D Light Field Benchmark on most metrics, and we also demonstrate the effectiveness of the proposed method on real-world light-field images.",http://arxiv.org/pdf/1804.02379v1
494,2391,Poster,A Hybrid L1-L0 Layer Decomposition Model for Tone Mapping,"Zhetong Liang, PolyU; Jun Xu, Hong Kong Polytechnic U; David Zhang, Hong Kong Polytechnic University; Zisheng Cao, ; Lei Zhang, The Hong Kong Polytechnic University",,,
495,2738,Poster,Deeply Learned Filter Response Functions for Hyperspectral Reconstruction,"Shijie Nie, NII, Japan; Lin Gu, National Institute of Informatics; Yinqiang Zheng, National Institute of Informatics, Japan; Antony Lam, Saitama University; Nobutaka Ono, Tokyo Metropolitan University; Imari Sato, National Institute of Informatics, Japan",,,
496,1815,Poster,CRRN: Multi-Scale Guided Concurrent Reflection Removal Network,"Renjie Wan, Nanyang Technological Universi; Boxin Shi, Peking University; Ling-Yu Duan, ; Ah-Hwee Tan, ; Alex Kot,",,,
497,1058,Poster,Single Image Reflection Separation with Perceptual Losses,"Xuaner Zhang, UC Berkeley; Qifeng Chen, Intel Labs",,,
498,3871,Poster,A Robust Method for Strong Rolling Shutter Effects Correction Using Lines with Automatic Feature Selection,"Yizhen Lao, Institut Pascal; Omar Ait-Aider, Institut Pascal",,,
499,3375,Poster,Time-resolved Light Transport Decomposition for Thermal Photometric Stereo,"Nobuhiro Ikeya, NAIST; Kenichiro Tanaka, NAIST; Tsuyoshi Takatani, NAIST; Hiroyuki Kubo, ; Takuya Funatomi, NAIST; Yasuhiro Mukaigawa, NAIST",,,
500,1439,Poster,Efficient Diverse Ensemble for Discriminative Co-Tracking,"Kourosh Meshgi, Kyoto University; Shigeyuki Oba, Kyoto University; Shin Ishii, Kyoto University",Efficient Diverse Ensemble for Discriminative Co-Tracking,"Ensemble discriminative tracking utilizes a committee of classifiers, to label data samples, which are in turn, used for retraining the tracker to localize the target using the collective knowledge of the committee. Committee members could vary in their features, memory update schemes, or training data, however, it is inevitable to have committee members that excessively agree because of large overlaps in their version space. To remove this redundancy and have an effective ensemble learning, it is critical for the committee to include consistent hypotheses that differ from one-another, covering the version space with minimum overlaps. In this study, we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training. The artificial data is sampled from the empirical distribution of the samples taken from both target and background, whereas the process is governed by query-by-committee to shrink the overlap between classifiers. The experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks.",http://arxiv.org/pdf/1711.06564v1
501,1949,Poster,Rolling Shutter and Radial Distortion are Features for High Frame Rate Multi-camera Tracking,"Akash Bapat, UNC Chapel Hill; Jan-Michael Frahm, UNC Chapel Hill; True Price, UNC Chapel Hill",,,
502,3980,Poster,A Twofold Siamese Network for Real-Time Object Tracking,"Anfeng He, USTC; Chong Luo, Microsoft Research Asia; Xinmei Tian, USTC; Wenjun Zeng,",A Twofold Siamese Network for Real-Time Object Tracking,"Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similarity-learning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC \cite{SiamFC} allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.",http://arxiv.org/pdf/1802.08817v1
503,255,Poster,Multi-Cue Correlation Filters for Robust Visual Tracking,"Ning Wang, USTC; Wengang Zhou, USTC; Qi Tian, ; Richang Hong, ; Meng Wang, HeFei University of Technology; Houqiang Li,",,,
504,678,Poster,Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking,"Qiang Wang, CASIA; Zhu Teng, Beijing Jiaotong University; Junliang Xing, Institute of Automation, Chinese Academy of Sciences; Jin Gao, Institute of Automation, Chinese Academy of Sciences; Weiming Hu,",,,
505,1304,Poster,SINT++: Robust Visual Tracking via Adversarial Hard Positive Generation,"Xiao Wang, Anhui university; Chenglong Li, Anhui University; Bin Luo, ; Jin Tang,",,,
506,2129,Poster,High-speed Tracking with Multi-kernel Correlation Filters,"Ming Tang, NLPR, IA, CAS; Bin Yu, NLPR, IA, CAS; Fan Zhang, BUPT; Jinqiao Wang,",Tracking in Aerial Hyperspectral Videos using Deep Kernelized Correlation Filters,"Hyperspectral imaging holds enormous potential to improve the state-of-the-art in aerial vehicle tracking with low spatial and temporal resolutions. Recently, adaptive multi-modal hyperspectral sensors, controlled by Dynamic Data Driven Applications Systems (DDDAS) methodology, have attracted growing interest due to their ability to record extended data quickly from the aerial platforms. In this study, we apply popular concepts from traditional object tracking - (1) Kernelized Correlation Filters (KCF) and (2) Deep Convolutional Neural Network (CNN) features - to the hyperspectral aerial tracking domain. Specifically, we propose the Deep Hyperspectral Kernelized Correlation Filter based tracker (DeepHKCF) to efficiently track aerial vehicles using an adaptive multi-modal hyperspectral sensor. We address low temporal resolution by designing a single KCF-in-multiple Regions-of-Interest (ROIs) approach to cover a reasonable large area. To increase the speed of deep convolutional features extraction from multiple ROIs, we design an effective ROI mapping strategy. The proposed tracker also provides flexibility to couple it to the more advanced correlation filter trackers. The DeepHKCF tracker performs exceptionally with deep features set up in a synthetic hyperspectral video generated by the Digital Imaging and Remote Sensing Image Generation (DIRSIG) software. Additionally, we generate a large, synthetic, single-channel dataset using DIRSIG to perform vehicle classification in the Wide Area Motion Imagery (WAMI) platform . This way, the high-fidelity of the DIRSIG software is proved and a large scale aerial vehicle classification dataset is released to support studies on vehicle detection and tracking in the WAMI platform.",http://arxiv.org/pdf/1711.07235v2
507,3285,Poster,Occlusion Aware Unsupervised Learning of Optical Flow,"Yang Wang, Baidu USA; Yi Yang, ; Zhenheng Yang, ; Liang Zhao, Baidu USA; Wei Xu,",Occlusion Aware Unsupervised Learning of Optical Flow,"It has been recently shown that a convolutional neural network can learn optical flow estimation with unsupervised learning. However, the performance of the unsupervised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsupervised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.",http://arxiv.org/pdf/1711.05890v2
508,353,Poster,Revisiting Video Saliency: A Large-scale Benchmark and a New Model,"Wenguan Wang, Beijing Institute of Technology; Jianbing Shen, Beijing Institute of Technolog; Fang Guo, Beijing Institute of Technology; Ming-Ming Cheng, Nankai University; Ali Borji, UCF",Revisiting Video Saliency: A Large-scale Benchmark and a New Model,"In this work, we contribute to video saliency research in two ways. First, we introduce a new benchmark for predicting human eye movements during dynamic scene free-viewing, which is long-time urged in this field. Our dataset, named DHF1K (Dynamic Human Fixation), consists of 1K high-quality, elaborately selected video sequences spanning a large range of scenes, motions, object types and background complexity. Existing video saliency datasets lack variety and generality of common dynamic scenes and fall short in covering challenging situations in unconstrained environments. In contrast, DHF1K makes a significant leap in terms of scalability, diversity and difficulty, and is expected to boost video saliency modeling. Second, we propose a novel video saliency model that augments the CNN-LSTM network architecture with an attention mechanism to enable fast, end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. We thoroughly examine the performance of our model, with respect to state-of-the-art saliency models, on three large-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that our model outperforms other competitors.",http://arxiv.org/pdf/1801.07424v2
509,1353,Poster,Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking,"Feng Li, Harbin Institute of Technology; Cheng Tian, Harbin Institute of Technology; Wangmeng Zuo, Harbin Institute of Technology; Lei Zhang, The Hong Kong Polytechnic University; Ming-Hsuan Yang, UC Merced",Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking,"Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). Motivated by online Passive-Agressive (PA) algorithm, we introduce the temporal regularization to SRDCF with single sample, thus resulting in our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Experiments are conducted on three benchmark datasets: OTB-2015, Temple-Color, and VOT-2016. Compared with SRDCF, STRCF with hand-crafted features provides a 5 times speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF combined with CNN features also performs favorably against state-of-the-art CNN-based trackers and achieves an AUC score of 68.3% on OTB-2015.",http://arxiv.org/pdf/1803.08679v1
510,2636,Poster,Multimodal Visual Concept Learning with Weakly Supervised Techniques,"Giorgos Bouritsas, NTUA; Petros Koutras, NTUA; Athanasia Zlatintsi, NTUA; Petros Maragos, NTUA",Multimodal Visual Concept Learning with Weakly Supervised Techniques,"Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in order to automatically recognize video concepts. Towards this goal, in this paper we use textual cues as means of supervision, introducing two weakly supervised techniques that extend the Multiple Instance Learning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and the Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes the spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets, while the latter models different interpretations of each description's semantics with Probabilistic Labels, both formulated through a convex optimization algorithm. In addition, we provide a novel technique to extract weak labels in the presence of complex semantics, that consists of semantic similarity computations. We evaluate our methods on two distinct problems, namely face and action recognition, in the challenging and realistic setting of movies accompanied by their screenplays, contained in the COGNIMUSE database. We show that, on both tasks, our method considerably outperforms a state-of-the-art weakly supervised approach, as well as other baselines.",http://arxiv.org/pdf/1712.00796v3
511,3028,Poster,Efficient Large-scale Approximate Nearest Neighbor Search on OpenCL FPGA,"Jialiang Zhang, University of Wisconsin-Madiso; Soroosh Khoram, UW-Madison; Jing Li, University of Wisconsin-Madison",,,
512,3112,Poster,Learning a Complete Image Indexing Pipeline,"Himalaya Jain, Inria, Technicolor; Joaquin Zepeda, ; Patrick Perez, Technicolor Research; Rémi Gribonval, Inria",Learning a Complete Image Indexing Pipeline,"To work at scale, a complete image indexing system comprises two components: An inverted file index to restrict the actual search to only a subset that should contain most of the items relevant to the query; An approximate distance computation mechanism to rapidly scan these lists. While supervised deep learning has recently enabled improvements to the latter, the former continues to be based on unsupervised clustering in the literature. In this work, we propose a first system that learns both components within a unifying neural framework of structured binary encoding.",http://arxiv.org/pdf/1712.04480v1
513,3279,Poster,Transparency by Design: Closing the Gap Between Performance and Interpretabilty in Visual Reasoning,"David Mascharka, MIT Lincoln Laboratory; Philip Tran, Planck Aerosystems; Ryan Soklaski, MIT Lincoln Laboratory; Arjun Majumdar, MIT Lincoln Laboratory",,,
514,3295,Poster,Fooling Vision and Language Models Despite Localization and Attention Mechanism,"Xiaojun Xu, Shanghai Jiao Tong University; Xinyun Chen, UC Berkeley; Chang Liu, UC Berkeley; Anna Rohrbach, UC Berkeley; Trevor Darrell, UC Berkeley, USA; Dawn Song, UC Berkeley",Fooling Vision and Language Models Despite Localization and Attention Mechanism,"Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., > 90%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses.",http://arxiv.org/pdf/1709.08693v2
515,3576,Poster,Categorizing Concepts with Basic Level for Vision-to-Language,"Hanzhang Wang, Tongji University; Hanli Wang, Tongji University; Kaisheng Xu, Tongji University",,,
516,112,Poster,Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering,"Aishwarya Agrawal, Georgia Institute of Technology; Dhruv Batra, Georgia Tech; Devi Parikh, Georgia Tech; Aniruddha Kembhavi, Allen Institute for Artificial Intelligence",The Matrix Calculus You Need For Deep Learning,"This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.",http://arxiv.org/pdf/1802.01528v2
517,420,Poster,Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation,"Jiwoon Ahn, DGIST; Suha Kwak, POSTECH",Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation,"The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision.",http://arxiv.org/pdf/1803.10464v2
518,733,Poster,From Lifestyle VLOGs to Everyday Interactions,"David Fouhey, UC Berkeley; WEICHENG KUO, Berkeley; Alexei Efros, UC Berkeley; Jitendra Malik,",From Lifestyle Vlogs to Everyday Interactions,"A major stumbling block to progress in understanding basic human interactions, such as getting out of bed or opening a refrigerator, is lack of good training data. Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it. We use Internet Lifestyle Vlogs as the source of surprisingly large and diverse interaction data. We show that by collecting the data first, we are able to achieve greater scale and far greater diversity in terms of actions and actors. Additionally, our data exposes biases built into common explicitly gathered data. We make sense of our data by analyzing the central component of interaction -- hands. We benchmark two tasks: identifying semantic object contact at the video level and non-semantic contact state at the frame level. We additionally demonstrate future prediction of hands.",http://arxiv.org/pdf/1712.02310v1
519,1164,Poster,Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation,"Naoto Inoue, The University of Tokyo; Ryosuke  Furuta, The University of Tokyo; Toshihiko Yamasaki, The University of Tokyo; Kiyoharu Aizawa,",Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation,"Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.",http://arxiv.org/pdf/1803.11365v1
520,1282,Poster,RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints,"Asako Kanezaki, National Institute of Advanced; Yasuyuki Matsushita, Osaka University; Yoshifumi Nishida, National Institute of Advanced Industrial Science and Technology (AIST)",RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints,"We propose a Convolutional Neural Network (CNN)-based model ""RotationNet,"" which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset. The code is available on https://github.com/kanezaki/rotationnet",http://arxiv.org/pdf/1603.06208v4
521,1390,Poster,An End-to-End TextSpotter with Explicit Alignment and Attention,"Tong He, The University of Adelaide; Zhi Tian, SIAT, CAS; Weilin Huang, The University of Oxford; Chunhua Shen, University of Adelaide; Yu Qiao, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Changming Sun, CSIRO Data61",An end-to-end TextSpotter with Explicit Alignment and Attention,"Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Training of two tasks in a unified framework is non-trivial due to significant dif- ferences in optimisation difficulties. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in one shot. Our main contributions are three-fold: 1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in ar- bitrary orientation, which is the key to boost the per- formance; 2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; 3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by shar- ing convolutional features, which is critical to identify challenging text instances. Our model achieves impressive results in end-to-end recognition on the ICDAR2015 dataset, significantly advancing most recent results, with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detec- tor by achieving a new state-of-the-art detection performance on two datasets.",http://arxiv.org/pdf/1803.03474v3
522,1562,Poster,Name-removed-for-review: A Multi-camera HD Dataset for Dense Unscripted Pedestrian Detection,"Tatjana Chavdarova, Idiap and EPFL; Pierre Baqué, EPFL; Andrii Maksai, ; STÉPHANE BOUQUET, EPFL; Cijo Jose, Idiap and EPFL; Louis Lettry, ETH Zürich; Francois Fleuret, Idiap Research Institute; Pascal Fua, ; Luc Van Gool, KTH",,,
523,1607,Poster,Direct Shape Regression Networks for End-to-End Face Alignment,"Xin Miao, UT Arlington; Xiantong Zhen, Beihang University; Vassilis Athitsos, University of Texas at Arlington; Xianglong Liu, Beihang University; Cheng Deng, Xidian University; Heng Huang, University of Pittsburgh",,,
524,1817,Poster,Natural and Effective Obfuscation by Head Inpainting,"Qianru Sun, MPI for Informatics; Liqian Ma, KU Leuven; Seong Joon Oh, MPI-INF; Mario Fritz, MPI, Saarbrucken, Germany; Luc Van Gool, KU Leuven; Bernt Schiele, MPI Informatics Germany",Natural and Effective Obfuscation by Head Inpainting,"As more and more personal photos are shared online, being able to obfuscate identities in such photos is becoming a necessity for privacy protection. People have largely resorted to blacking out or blurring head regions, but they result in poor user experience while being surprisingly ineffective against state of the art person recognizers. In this work, we propose a novel head inpainting obfuscation technique. Generating a realistic head inpainting in social media photos is challenging because subjects appear in diverse activities and head orientations. We thus split the task into two sub-tasks: (1) facial landmark generation from image context (e.g. body pose) for seamless hypothesis of sensible head pose, and (2) facial landmark conditioned head inpainting. We verify that our inpainting method generates realistic person images, while achieving superior obfuscation performance against automatic person recognizers.",http://arxiv.org/pdf/1711.09001v5
525,2111,Poster,3D Semantic Trajectory Reconstruction from 3D Pixel Continuum,"Jae Yoon, ; Ziwei Li, UMN; Hyun Park,",3D Semantic Trajectory Reconstruction from 3D Pixel Continuum,"This paper presents a method to reconstruct dense semantic trajectory stream of human interactions in 3D from synchronized multiple videos. The interactions inherently introduce self-occlusion and illumination/appearance/shape changes, resulting in highly fragmented trajectory reconstruction with noisy and coarse semantic labels. Our conjecture is that among many views, there exists a set of views that can confidently recognize the visual semantic label of a 3D trajectory. We introduce a new representation called 3D semantic map---a probability distribution over the semantic labels per trajectory. We construct the 3D semantic map by reasoning about visibility and 2D recognition confidence based on view-pooling, i.e., finding the view that best represents the semantics of the trajectory. Using the 3D semantic map, we precisely infer all trajectory labels jointly by considering the affinity between long range trajectories via estimating their local rigid transformations. This inference quantitatively outperforms the baseline approaches in terms of predictive validity, representation robustness, and affinity effectiveness. We demonstrate that our algorithm can robustly compute the semantic labels of a large scale trajectory set involving real-world human interactions with object, scenes, and people.",http://arxiv.org/pdf/1712.01359v1
526,2519,Poster,Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition,"Shizhong Han, 1986; zibo Meng, ; Zhiyuan Li, University of South Carolina; JAMES O'REILLY, University of South Carolina; Jie Cai, University of South Carolina; Xiaofeng Wang, University of South Carolina; Yan Tong, University of South Carolina",Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition,"Recognizing facial action units (AUs) during spontaneous facial displays is a challenging problem. Most recently, Convolutional Neural Networks (CNNs) have shown promise for facial AU recognition, where predefined and fixed convolution filter sizes are employed. In order to achieve the best performance, the optimal filter size is often empirically found by conducting extensive experimental validation. Such a training process suffers from expensive training cost, especially as the network becomes deeper.   This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the filter sizes and weights of all convolutional layers are learned simultaneously from the training data along with learning convolution filters. Specifically, the filter size is defined as a continuous variable, which is optimized by minimizing the training loss. Experimental results on two AU-coded spontaneous databases have shown that the proposed OFS-CNN is capable of estimating optimal filter size for varying image resolution and outperforms traditional CNNs with the best filter size obtained by exhaustive search. The OFS-CNN also beats the CNN using multiple filter sizes and more importantly, is much more efficient during testing with the proposed forward-backward propagation algorithm.",http://arxiv.org/pdf/1707.08630v2
527,2548,Poster,V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map,"Gyeongsik Moon, Seoul National University; Ju Yong Chang, Kwangwoon University; Kyoung Mu Lee,",,,
528,2833,Poster,Ring loss: Convex Feature Normalization for Face Recognition,"Yutong Zheng, Carnegie Mellon University; Dipan Pal, Carnegie Mellon University; Marios Savvides,",Ring loss: Convex Feature Normalization for Face Recognition,"We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.",http://arxiv.org/pdf/1803.00130v1
529,2864,Poster,Adversarially Occluded Samples for Person Re-identification,"Houjing Huang, CASIA; Dangwei Li, ; Zhang Zhang, ; Xiaotang Chen, ; Kaiqi Huang,",,,
530,2896,Poster,Classifier Learning with Prior Probabilities for Facial Action Unit Recognition,"Yong Zhang, CASIA; Weiming Dong, ; Bao-Gang Hu, CASIA; Qiang Ji, RPI",,,
531,3299,Poster,4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications,"Shiyang Cheng, Imperial College London; Irene Kotsia, Middlesex University London; Maja Pantic, Imperial College London, UK; Stefanos Zafeiriou, Imperial College London",4DFAB: A Large Scale 4D Facial Expression Database for Biometric Applications,"The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.",http://arxiv.org/pdf/1712.01443v1
532,3468,Poster,Seeing Small Faces from Robust Anchor's Perspective,"Chenchen Zhu, Carnegie Mellon University; Ran Tao, Carnegie Mellon University; Khoa Luu, ; Marios Savvides,",Seeing Small Faces from Robust Anchor's Perspective,"This paper introduces a novel anchor design to support anchor-based face detection for superior scale-invariant performance, especially on tiny faces. To achieve this, we explicitly address the problem that anchor-based detectors drop performance drastically on faces with tiny sizes, e.g. less than 16x16 pixels. In this paper, we investigate why this is the case. We discover that current anchor design cannot guarantee high overlaps between tiny faces and anchor boxes, which increases the difficulty of training. The new Expected Max Overlapping (EMO) score is proposed which can theoretically explain the low overlapping issue and inspire several effective strategies of new anchor design leading to higher face overlaps, including anchor stride reduction with new network architectures, extra shifted anchors, and stochastic face shifting. Comprehensive experiments show that our proposed method significantly outperforms the baseline anchor-based detector, while consistently achieving state-of-the-art results on challenging face detection datasets with competitive runtime speed.",http://arxiv.org/pdf/1802.09058v1
533,131,Poster,2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning,"Diogo Luvizon, ETIS Lab; David Picard, ETIS /LIP6; Hedi Tabia, ETIS / ENSEA",2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning,"Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.",http://arxiv.org/pdf/1802.09232v2
534,671,Poster,Dense 3D Regression for Hand Pose Estimation,"Chengde Wan, ; Thomas Probst, ; Luc Van Gool, KTH; Angela Yao, University of Bonn",Dense 3D Regression for Hand Pose Estimation,"We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-the-art methods based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D directional vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multi-task network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes while enforcing consensus between the the estimated 3D pose and the pixel-wise 2D and 3D estimations by design. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-the-art approaches by a large margin. On the ICVL hand dataset, our method achieves similar accuracy compared to the currently proposed nearly saturated result and outperforms various other proposed methods. Code is available $\href{""https://github.com/melonwan/denseReg""}{\text{online}}$.",http://arxiv.org/pdf/1711.08996v1
535,954,Poster,Camera Style Adaptation for Person Re-identification,"Zhun Zhong, Xiamen University; Liang Zheng, University of Texas at San Ant; Zhedong Zheng, UTS; Shaozi Li, ; Yi Yang, University of Technology, Sydney",Camera Style Adaptation for Person Re-identification,"Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art.",http://arxiv.org/pdf/1711.10295v1
536,1022,Poster,A Benchmark for Articulated Human Pose Estimation and Tracking,"Mykhaylo Andriluka, MPI Informatics; Umar Iqbal, ; Eldar Insafutdinov, MPI Informatics; Anton Milan, University of Adelaide; Leonid Pishchulin, MPI Informatik; Juergen Gall, University of Bonn, Germany; Bernt Schiele, MPI Informatics Germany",PoseTrack: A Benchmark for Human Pose Estimation and Tracking,"Human poses and motions are important cues for analysis of videos with people and there is strong evidence that representations based on body pose are highly effective for a variety of tasks such as activity recognition, content retrieval and social signal processing. In this work, we aim to further advance the state of the art by establishing ""PoseTrack"" , a new large-scale benchmark for video-based human pose estimation and articulated tracking, and bringing together the community of researchers working on visual human analysis. The benchmark encompasses three competition tracks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To facilitate the benchmark and challenge we collect, annotate and release a new %large-scale benchmark dataset that features videos with multiple people labeled with person tracks and articulated pose. A centralized evaluation server is provided to allow participants to evaluate on a held-out test set. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods. The benchmark is freely accessible at https://posetrack.net.",http://arxiv.org/pdf/1710.10000v1
537,1290,Poster,Exploit the Unknown Gradually:~ One-Shot Video-Based Person Re-Identification by Stepwise Learning,"Yu Wu, University of technology sydne; Yutian Lin, ; Xuanyi Dong, UTS; Yan Yan, UTS; Wanli Ouyang, The University of Sydney; Yi Yang,",,,
538,2152,Poster,Pose-Robust Face Recognition via Deep Residual Equivariant Mapping,"Kaidi Cao, Tsinghua University; Yu Rong, CUHK; Cheng Li, SenseTime; Chen-Change Loy, the Chinese University of Hong Kong",Pose-Robust Face Recognition via Deep Residual Equivariant Mapping,"Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead.",http://arxiv.org/pdf/1803.00839v1
539,2735,Poster,DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation,"Jiang Liu, Carnegie Mellon University; Chenqiang Gao, Chongqing University of Posts and Telecommunications; Deyu Meng, Xi'an Jiaotong University; Alexander Hauptmann,",DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation,"In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.",http://arxiv.org/pdf/1712.06679v2
540,2798,Poster,LSTM Pose Machines,"Yue Luo, SenseTime; Jimmy Ren, SenseTime Group Limited; Zhouxia Wang, SenseTime; Wenxiu Sun, SenseTime Group Limited; Jinshan Pan, UC Merced; Jianbo Liu, SenseTime; Jiahao Pang, SenseTime Group Limited; Liang Lin,",LSTM Pose Machines,"We observed that recent state-of-the-art results on single image human pose estimation were achieved by multi-stage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of these models on videos is not only computationally intensive, it also suffers from performance degeneration and flicking. Such suboptimal results are mainly attributed to the inability of imposing sequential geometric consistency, handling severe image quality degradation (e.g. motion blur and occlusion) as well as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the multi-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and results in significantly faster speed in invoking the network for videos. It also enables the adoption of Long Short-Term Memory (LSTM) units between video frames. We found such memory augmented RNN is very effective in imposing geometric consistency among frames. It also well handles input quality degradation in videos while successfully stabilizes the sequential outputs. The experiments showed that our approach significantly outperformed current state-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why such mechanism would benefit the prediction for video-based pose estimations.",http://arxiv.org/pdf/1712.06316v4
541,2852,Poster,Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition,"Feng Liu, Sichuan University; Dan Zeng, Sichuan University; Qijun Zhao, Sichuan University; Xiaoming Liu, Michigan State University",Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition,"This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.",http://arxiv.org/pdf/1803.11366v1
542,3362,Poster,Convolutional Sequence to Sequence Model for Human Dynamics,"Chen Li, ; Zhen Zhang, National University of Singapore; Wee Sun Lee, ; Gim Hee Lee, National Univeristy of Singapore",Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition,"Variations of human body skeletons may be considered as dynamic graphs, which are generic data representation for numerous real-world applications. In this paper, we propose a spatio-temporal graph convolution (STGC) approach for assembling the successes of local convolutional filtering and sequence learning ability of autoregressive moving average. To encode dynamic graphs, the constructed multi-scale local graph convolution filters, consisting of matrices of local receptive fields and signal mappings, are recursively performed on structured graph data of temporal and spatial domain. The proposed model is generic and principled as it can be generalized into other dynamic models. We theoretically prove the stability of STGC and provide an upper-bound of the signal transformation to be learnt. Further, the proposed recursive model can be stacked into a multi-layer architecture. To evaluate our model, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D. The experimental results demonstrate the effectiveness of our proposed model and the improvement over the state-of-the-art.",http://arxiv.org/pdf/1802.09834v1
543,3470,Poster,Gesture Recognition: Focus on the Hands,"Pradyumna Narayana, Colorado State University; Ross Beveridge, Colorado State University; Bruce Draper, Colorado State University",Vision-based Engagement Detection in Virtual Reality,"User engagement modeling for manipulating actions in vision-based interfaces is one of the most important case studies of user mental state detection. In a Virtual Reality environment that employs camera sensors to recognize human activities, we have to know when user intends to perform an action and when not. Without a proper algorithm for recognizing engagement status, any kind of activities could be interpreted as manipulating actions, called ""Midas Touch"" problem. Baseline approach for solving this problem is activating gesture recognition system using some focus gestures such as waiving or raising hand. However, a desirable natural user interface should be able to understand user's mental status automatically. In this paper, a novel multi-modal model for engagement detection, DAIA, is presented. using DAIA, the spectrum of mental status for performing an action is quantized in a finite number of engagement states. For this purpose, a Finite State Transducer (FST) is designed. This engagement framework shows how to integrate multi-modal information from user biometric data streams such as 2D and 3D imaging. FST is employed to make the state transition smoothly using combination of several boolean expressions. Our FST true detection rate is 92.3% in total for four different states. Results also show FST can segment user hand gestures more robustly.",http://arxiv.org/pdf/1609.01344v1
544,862,Poster,Crowd Counting via Adversarial Cross-Scale Consistency Pursuit,"Zan Shen, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong U; Bingbing Ni, ; Yi Xu, Shanghai Jiao Tong University; Minsi Wang, Shanghai Jiao Tong University; jianguo Hu, Minivision; Xiaokang Yang,",,,
545,1343,Poster,3D Human Pose Estimation in the Wild by Adversarial Learning,"Wei Yang, The Chinese University of Hong Kong ; Wanli Ouyang, The University of Sydney; Xiaolong Wang, Carnegie Mellon University; Xiaogang Wang, Chinese University of Hong Kong",3D Human Pose Estimation in the Wild by Adversarial Learning,"Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.",http://arxiv.org/pdf/1803.09722v1
546,1797,Poster,CosFace: Large Margin Cosine Loss for Deep Face Recognition,"Hao Wang, ; Yitong Wang, Tencent AI Lab; Zheng Zhou, ; xing Ji, ; Dihong Gong, ; Zhifeng Li, ; Jingchao Zhou, ; Wei Liu,",CosFace: Large Margin Cosine Loss for Deep Face Recognition,"Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by $L_2$ normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.",http://arxiv.org/pdf/1801.09414v2
547,2136,Poster,Encoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction,"Yanyu Xu, Shanghaitech University; Zhixin Piao, ; Shenghua Gao, ShanghaiTech University",,,
548,2139,Poster,Mean-Variance Loss for Deep Age Estimation from a Face,"Hongyu Pan, Institute of Computing Technol; Hu Han, ; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen,",,,
549,3236,Poster,Probabilistic Joint Face-Skull Modelling for Facial Reconstruction,"Dennis Madsen, University of Basel; Marcel Lüthi, ; Andreas Schneider, ; Thomas Vetter, U. Basel",,,
550,3795,Poster,Learning Latent Super-Events to Detect Multiple Activities in Videos,"AJ Piergiovanni, Indiana University; Michael Ryoo, Indiana University",Learning Latent Super-Events to Detect Multiple Activities in Videos,"In this paper, we introduce the concept of learning latent super-events from activity videos, and present how it benefits activity detection in continuous videos. We define a super-event as a set of multiple events occurring together in videos with a particular temporal organization; it is the opposite concept of sub-events. Real-world videos contain multiple activities and are rarely segmented (e.g., surveillance videos), and learning latent super-events allows the model to capture how the events are temporally related in videos. We design temporal structure filters that enable the model to focus on particular sub-intervals of the videos, and use them together with a soft attention mechanism to learn representations of latent super-events. Super-event representations are combined with per-frame or per-segment CNNs to provide frame-level annotations. Our approach is designed to be fully differentiable, enabling end-to-end learning of latent super-event representations jointly with the activity detector using them. Our experiments with multiple public video datasets confirm that the proposed concept of latent super-event learning significantly benefits activity detection, advancing the state-of-the-arts.",http://arxiv.org/pdf/1712.01938v2
551,1664,Poster,Temporal Hallucinating for Action Recognition with Few Still Images,"Lei Zhou, ; Yali Wang, SIAT, CAS; Yu Qiao, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",,,
552,1736,Poster,Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition,"Yansong Tang, Tsinghua University; Yi Tian, ; Peiyang Li, ; Jiwen Lu, Tsinghua University; Jie Zhou,",,,
553,2529,Poster,Gaze Prediction in Dynamic $360^\circ$ Immersive Videos,"Yanyu Xu, Shanghaitech University; Yanbing Dong, ; Junru Wu, ; Zhengzhong Sun, ; Zhiru Shi, ; Jingyi Yu, ; Shenghua Gao, ShanghaiTech University",,,
554,1549,Poster,When will you do what? - Anticipating Temporal Occurrences of Activities,"Alexander Richard, University of Bonn; Juergen Gall, University of Bonn, Germany; Yazan Abu Farha, University of Bonn",,,
555,1679,Poster,Fusing Crowd Density Maps and Visual Object Trackers for People Tracking in Crowd Scenes,"Weihong Ren, City University of Hong Kong; Di Kang, ; Yandong Tang, Shenyang Institute of Automation, Chinese Academy of Sciences; Antoni Chan, City University of Hong Kong, Hong Kong",,,
556,460,Poster,Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification,"Jianlou Si, BUPT; Honggang Zhang, ; Chun-Guang Li, Beijing Univ. of Posts&Telecom; Jason Kuen, NTU, Singapore; Xiangfei Kong, Nanyang Technological University; Alex Kot, ; Gang Wang,",Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification,"Typical person re-identification (ReID) methods usually describe each pedestrian with a single feature vector and match them in a task-specific metric space. However, the methods based on a single feature vector are not sufficient enough to overcome visual ambiguity, which frequently occurs in real scenario. In this paper, we propose a novel end-to-end trainable framework, called Dual ATtention Matching network (DuATM), to learn context-aware feature sequences and perform attentive sequence comparison simultaneously. The core component of our DuATM framework is a dual attention mechanism, in which both intra-sequence and inter-sequence attention strategies are used for feature refinement and feature-pair alignment, respectively. Thus, detailed visual cues contained in the intermediate feature sequences can be automatically exploited and properly compared. We train the proposed DuATM network as a siamese network via a triplet loss assisted with a de-correlation loss and a cross-entropy loss. We conduct extensive experiments on both image and video based ReID benchmark datasets. Experimental results demonstrate the significant advantages of our approach compared to the state-of-the-art methods.",http://arxiv.org/pdf/1803.09937v1
557,517,Poster,Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints,"Jiahuan Zhou, Northwestern University; Bing Su, Chinese Academy of Sciences; Ying Wu, Northwestern University, USA",,,
558,1378,Poster,Crowd Counting with Deep Negative Correlation Learning,"Zenglin Shi, University of Bern; Le Zhang, Advanced Digital Sciences Cent; XiaoFeng Cao, university of technology sydney; Yun Liu, Nankai University; yangdong Ye, Zhengzhou University, China; Guoyan Zheng, University of Bern",,,
559,2231,Poster,Human Appearance Transfer,"Mihai Zanfir, IMAR and Lund University ; Alin-Ionut Popa, IMAR; Andrei Zanfir, IMAR and Lund University; Cristian Sminchisescu,","Dear Sir or Madam, May I introduce the YAFC Corpus: Corpus, Benchmarks and Metrics for Formality Style Transfer","Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics.",http://arxiv.org/pdf/1803.06535v1
560,2932,Poster,Domain Generalization with Adversarial Feature Learning,"Haoliang Li, Nanyang Technological Universi; Sinno Jilain Pan, Nanyang Technological University, Singapore; Shiqi Wang, City University of Hong Kong; Alex Kot,",Adversarial Deep Learning for Robust Detection of Binary Encoded Malware,"Malware is constantly adapting in order to avoid detection. Model based malware detectors, such as SVM and neural networks, are vulnerable to so-called adversarial examples which are modest changes to detectable malware that allows the resulting malware to evade detection. Continuous-valued methods that are robust to adversarial examples of images have been developed using saddle-point optimization formulations. We are inspired by them to develop similar methods for the discrete, e.g. binary, domain which characterizes the features of malware. A specific extra challenge of malware is that the adversarial examples must be generated in a way that preserves their malicious functionality. We introduce methods capable of generating functionally preserved adversarial malware examples in the binary domain. Using the saddle-point formulation, we incorporate the adversarial examples into the training of models that are robust to them. We evaluate the effectiveness of the methods and others in the literature on a set of Portable Execution~(PE) files. Comparison prompts our introduction of an online measure computed during training to assess general expectation of robustness.",http://arxiv.org/pdf/1801.02950v3
561,2959,Poster,Pyramid Stereo Matching Network,"Jia-Ren Chang, National Chiao Tung University; Yong-Sheng Chen, National Chiao Tung University",Pyramid Stereo Matching Network,"Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in illposed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",http://arxiv.org/pdf/1803.08669v1
562,2970,Poster,Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars,"Antonio Loquercio, University of Zurich; Ana Maqueda, Universidad Politecnica de Madrid; Guillermo Gallego, University of Zurich; Narciso Garcia, Universidad Politecnica de Madrid; Davide Scaramuzza, University of Zurich",Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars,"Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (~1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.",http://arxiv.org/pdf/1804.01310v1
563,3010,Poster,Learning Answer Embeddings for Visual Question Answering,"Hexiang Hu, ; Wei-Lun Chao, USC; Fei Sha, University of Southern California",Visual Question Reasoning on General Dependency Tree,"The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.",http://arxiv.org/pdf/1804.00105v1
564,3063,Poster,Good View Hunting: Learning Photo Composition from 1 Million View Pairs,"Zijun Wei, Stony Brook University; Jianming Zhang, Adobe Research; Minh Hoai, Stony Brook University; Xiaohui Shen, Adobe Research; Zhe Lin, Adobe Systems, Inc.; Radomír Mech, ; Dimitris Samaras,",,,
565,3435,Poster,CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise,"Kuang-Huei Lee, Microsoft; Xiaodong He, ; Lei Zhang, Microsoft; Linjun Yang, Facebook",CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise,"In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.",http://arxiv.org/pdf/1711.07131v2
566,3436,Poster,Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,"Shuai Li, University of Wollongong; Wanqing Li, ; Chris Cook, University of Wollongong; Ce Zhu, University of Electronic Science and Technology of China; Yanbo Gao, University of Electronic Science and Technology of China",Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN,"Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM.",http://arxiv.org/pdf/1803.04831v1
567,3617,Poster,Encoder-Decoder Alignment for Zero-Pair Image-to-Image Translation,"Yaxing Wang, Computer vision center; Joost van de Weijer, Computer Vision Center Barcelona; Luis Herranz, Computer Vision Center",Mix and match networks: encoder-decoder alignment for zero-pair image translation,"We address the problem of image translation between domains or modalities for which no direct paired data is available (i.e. zero-pair translation). We propose mix and match networks, based on multiple encoders and decoders aligned in such a way that other encoder-decoder pairs can be composed at test time to perform unseen image translation tasks between domains or modalities for which explicit paired samples were not seen during training. We study the impact of autoencoders, side information and losses in improving the alignment and transferability of trained pairwise translation models to unseen translations. We show our approach is scalable and can perform colorization and style transfer between unseen combinations of domains. We evaluate our system in a challenging cross-modal setting where semantic segmentation is estimated from depth images, without explicit access to any depth-semantic segmentation training pairs. Our model outperforms baselines based on pix2pix and CycleGAN models.",http://arxiv.org/pdf/1804.02199v1
568,3702,Poster,Structured Uncertainty Prediction Networks,"Garoe Dorta, University of Bath; Sara Vicente, Anthropics Technology Ltd; Lourdes Agapito, University College London; Neill Campbell, University of bath; Ivor Simpson, Anthropics Technology Ltd",Structured Uncertainty Prediction Networks,"This paper is the first work to propose a network to predict a structured uncertainty distribution for a synthesized image. Previous approaches have been mostly limited to predicting diagonal covariance matrices. Our novel model learns to predict a full Gaussian covariance matrix for each reconstruction, which permits efficient sampling and likelihood evaluation.   We demonstrate that our model can accurately reconstruct ground truth correlated residual distributions for synthetic datasets and generate plausible high frequency samples for real face images. We also illustrate the use of these predicted covariances for structure preserving image denoising.",http://arxiv.org/pdf/1802.07079v2
569,3905,Poster,Between-class Learning for Image Classification,"Yuji Tokozume, The University of Tokyo; Yoshitaka Ushiku, ; Tatsuya Harada, University of Tokyo",Between-class Learning for Image Classification,"In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning). We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose constraints on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively.",http://arxiv.org/pdf/1711.10284v2
570,3995,Poster,Adversarial Feature Augmentation for Unsupervised Domain Adaptation,"Riccardo Volpi, IIT (Italy); Pietro Morerio, Istituto Italiano di Tecnologi; Silvio Savarese, ; Vittorio Murino, Istituto Italiano di Tecnologia",Adversarial Feature Augmentation for Unsupervised Domain Adaptation,"Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.",http://arxiv.org/pdf/1711.08561v1
571,456,Poster,Generative Image Inpainting with Contextual Attention,"Jiahui Yu, UIUC; Zhe Lin, Adobe Systems, Inc.; Jimei Yang, ; Xiaohui Shen, Adobe Research; Xin Lu, ; Thomas Huang,",Generative Image Inpainting with Contextual Attention,"Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting.",http://arxiv.org/pdf/1801.07892v2
572,561,Poster,CSGNet: Neural Shape Parser for Constructive Solid Geometry,"Gopal Sharma, University of Massachusetts; Subhransu Maji, ; Rishabh Goyal, Indian Institute of Technology, Kanpu; Difan Liu, UMass Amherst; Evangelos Kalogerakis, UMass",CSGNet: Neural Shape Parser for Constructive Solid Geometry,"We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.",http://arxiv.org/pdf/1712.08290v2
573,2225,Poster,Conditional Image-to-Image Translation,"Jianxin Lin, USTC; Yingce Xia, ; Tao Qin, ; Zhibo Chen, ; Tie-Yan Liu,",Word Level Font-to-Font Image Translation using Convolutional Recurrent Generative Adversarial Networks,"Conversion of one font to another font is very useful in real life applications. In this paper, we propose a Convolutional Recurrent Generative model to solve the word level font transfer problem. Our network is able to convert the font style of any printed text images from its current font to the required font. The network is trained end-to-end for the complete word images. Thus it eliminates the necessary pre-processing steps, like character segmentations. We extend our model to conditional setting that helps to learn one-to-many mapping function. We employ a novel convolutional recurrent model architecture in the Generator that efficiently deals with the word images of arbitrary width. It also helps to maintain the consistency of the final images after concatenating the generated image patches of target font. Besides, the Generator and the Discriminator network, we employ a Classification network to classify the generated word images of converted font style to their subsequent font categories. Most of the earlier works related to image translation are performed on square images. Our proposed architecture is the first work which can handle images of varying widths. Word images generally have varying width depending on the number of characters present. Hence, we test our model on a synthetically generated font dataset. We compare our method with some of the state-of-the-art methods for image translation. The superior performance of our network on the same dataset proves the ability of our model to learn the font distributions.",http://arxiv.org/pdf/1801.07156v1
574,2695,Poster,Tight Nonconvex Relaxation of MAP Inference,"D. Khuê Lê-Huu, Inria & CentraleSupélec, Université Paris-Saclay; Nikos Paragios, Ecole Centrale de Paris",Continuous Relaxation of MAP Inference: A Nonconvex Perspective,"In this paper, we study a nonconvex continuous relaxation of MAP inference in discrete Markov random fields (MRFs). We show that for arbitrary MRFs, this relaxation is tight, and a discrete stationary point of it can be easily reached by a simple block coordinate descent algorithm. In addition, we study the resolution of this relaxation using popular gradient methods, and further propose a more effective solution using a multilinear decomposition framework based on the alternating direction method of multipliers (ADMM). Experiments on many real-world problems demonstrate that the proposed ADMM significantly outperforms other nonconvex relaxation based methods, and compares favorably with state of the art MRF optimization algorithms in different settings.",http://arxiv.org/pdf/1802.07796v2
575,2709,Poster,Feature Generating Networks for Zero-Shot Learning,"Yongqin Xian, Max Planck Institute; Tobias Lorenz, Max Planck Institute for Informatics; Bernt Schiele, MPI Informatics Germany; Zeynep Akata, University of Amsterdam",Radical analysis network for zero-shot learning in printed Chinese character recognition,"Chinese characters have a huge set of character categories, more than 20,000 and the number is still increasing as more and more novel characters continue being created. However, the enormous characters can be decomposed into a compact set of about 500 fundamental and structural radicals. This paper introduces a novel radical analysis network (RAN) to recognize printed Chinese characters by identifying radicals and analyzing two-dimensional spatial structures among them. The proposed RAN first extracts visual features from input by employing convolutional neural networks as an encoder. Then a decoder based on recurrent neural networks is employed, aiming at generating captions of Chinese characters by detecting radicals and two-dimensional structures through a spatial attention mechanism. The manner of treating a Chinese character as a composition of radicals rather than a single character class largely reduces the size of vocabulary and enables RAN to possess the ability of recognizing unseen Chinese character classes, namely zero-shot learning.",http://arxiv.org/pdf/1711.01889v2
576,3107,Poster,Joint Optimization Framework for Learning with Noisy Labels,"Daiki Tanaka, The University of Tokyo; Daiki Ikami, The University of Tokyo; Toshihiko Yamasaki, The University of Tokyo; Kiyoharu Aizawa,",Joint Optimization Framework for Learning with Noisy Labels,"Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods.",http://arxiv.org/pdf/1803.11364v1
577,3325,Poster,Convolutional Image Captioning,"Jyoti Aneja, UIUC; Aditya Deshpande, University of Illinois at UC; Alex Schwing,",Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning,"Modern neural image captioning systems typically adopt the encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for caption generation. Inspired by the robustness analysis of CNN-based image classifiers to adversarial perturbations, we propose \textbf{Show-and-Fool}, a novel algorithm for crafting adversarial examples in neural image captioning. Unlike image classification tasks with a finite set of class labels, finding visually-similar adversarial examples in an image captioning system is much more challenging since the space of possible captions in a captioning system is almost infinite. In this paper, we design three approaches for crafting adversarial examples in image captioning: (i) targeted caption method; (ii) targeted keyword method; and (iii) untargeted method. We formulate the process of finding adversarial perturbations as optimization problems and design novel loss functions for efficient search. Experimental results on the Show-and-Tell model and MSCOCO data set show that Show-and-Fool can successfully craft visually-similar adversarial examples with randomly targeted captions, and the adversarial examples can be made highly transferable to the Show-Attend-and-Tell model. Consequently, the presence of adversarial examples leads to new robustness implications of neural image captioning. To the best of our knowledge, this is the first work on crafting effective adversarial examples for image captioning tasks.",http://arxiv.org/pdf/1712.02051v1
578,3580,Poster,AON: Towards Arbitrarily-Oriented Text Recognition,"Zhanzhan Cheng, Hikvision Research Institute; Yangliu Xu, Tongji University; Fan Bai, Fudan University; Yi Niu, Hikvision Research Institute; Shiliang Pu, ; Shuigeng Zhou, Fudan University",AON: Towards Arbitrarily-Oriented Text Recognition,"Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets.",http://arxiv.org/pdf/1711.04226v2
579,3903,Poster,Wrapped Gaussian Process Regression on Riemannian Manifolds,"Anton Mallasto, University of Copenhagen; Aasa Feragen, University of Copenhagen",,,
580,2368,Poster,Geometry-Guided CNN for Self-supervised Video Representation learning,"Chuang Gan, Tsinghua University; Boqing Gong, University of Central Florida; Kun Liu, Beijing University of Posts and Telecommunications; hao Su, ; Leonidas J. Guibas,",,,
581,2604,Poster,DiverseNet: When One Right Answer Is Not Enough,"Michael Firman, UCL; Neill Campbell, University of bath; Lourdes Agapito, University College London; Gabriel Brostow, University College London UK",,,
582,2744,Poster,Face Detector Adaptation without Negative Transfer or Catastrophic Forgetting,"Muhammad Abdullah Jamal, University of Central Florida; Haoxiang Li, Adobe Research; Boqing Gong, University of Central Florida",,,
583,3073,Poster,Analyzing Filters Toward Efficient ConvNet,"Takumi Kobayashi,",,,
584,3220,Poster,Regularizing Deep Networks by Modeling and Predicting Label Structure,"Mohammadreza Mostajabi, TTI-Chicago; Michael Maire, ; Greg Shakhnarovich,",Regularizing Deep Networks by Modeling and Predicting Label Structure,"We construct custom regularization functions for use in supervised training of deep neural networks. Our technique is applicable when the ground-truth labels themselves exhibit internal structure; we derive a regularizer by learning an autoencoder over the set of annotations. Training thereby becomes a two-phase procedure. The first phase models labels with an autoencoder. The second phase trains the actual network of interest by attaching an auxiliary branch that must predict output via a hidden layer of the autoencoder. After training, we discard this auxiliary branch.   We experiment in the context of semantic segmentation, demonstrating this regularization strategy leads to consistent accuracy boosts over baselines, both when training from scratch, or in combination with ImageNet pretraining. Gains are also consistent over different choices of convolutional network architecture. As our regularizer is discarded after training, our method has zero cost at test time; the performance improvements are essentially free. We are simply able to learn better network weights by building an abstract model of the label space, and then training the network to understand this abstraction alongside the original task.",http://arxiv.org/pdf/1804.02009v1
585,573,Poster,In-Place Activated BatchNorm for Memory-Optimized Training of DNNs,"Samuel Rota Bulo', Mapillary Research; Lorenzo Porzi, Mapillary Research; Peter Kontschieder,",In-Place Activated BatchNorm for Memory-Optimized Training of DNNs,"In this work we present In-Place Activated Batch Normalization (InPlace-ABN) - a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report results for COCO-Stuff, Cityscapes and Mapillary Vistas, obtaining new state-of-the-art results on the latter without additional training data but in a single-scale and -model scenario. Code can be found at https://github.com/mapillary/inplace_abn .",http://arxiv.org/pdf/1712.02616v2
586,2040,Poster,DVQA: Understanding Data Visualization via Question Answering,"Kushal Kafle, ; Brian Price, ; Scott Cohen, ; Christopher Kanan, RIT",DVQA: Understanding Data Visualizations via Question Answering,"Bar charts are an effective way to convey numeric information, but today's algorithms cannot parse them. Existing methods fail when faced with even minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a question answering framework. Unlike visual question answering (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we propose two strong baselines that perform considerably better. Our work will enable algorithms to automatically extract numeric and semantic information from vast quantities of bar charts found in scientific publications, Internet articles, business reports, and many other areas.",http://arxiv.org/pdf/1801.08163v2
587,1877,Poster,DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Network,"Shuang Ma, SUNY Buffalo; Jianlong Fu, ; Chang Chen, ; Tao Mei, Microsoft Research Asia",DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks (with Supplementary Materials),"Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Network (GAN) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object configuration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instancelevel correspondences could be consequently discovered through attending on the learned instance pairs. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-ofthe- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem.",http://arxiv.org/pdf/1802.06454v1
588,1244,Poster,Unsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints,"Reza Mahjourian, University of Texas at Austin; Martin Wicke, Google Brain; Anelia Angelova, Google Brain",,,
589,1699,Poster,FOTS: Fast Oriented Text Spotting with a Unified Network,"Xuebo Liu, SenseTime Group Ltd.; Ding Liang, Sensetime; Shi Yan, SenseTime; Dagui Chen, SenseTime; Yu Qiao, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Junjie Yan,",FOTS: Fast Oriented Text Spotting with a Unified Network,"Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specially, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method learns more generic features to make our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps.",http://arxiv.org/pdf/1801.01671v2
590,2499,Poster,Mobile Video Object Detection with Temporally-Aware Feature Maps,"Menglong Zhu, ; Mason Liu, Georgia Tech",Mobile Video Object Detection with Temporally-Aware Feature Maps,"This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.",http://arxiv.org/pdf/1711.06368v2
591,2697,Poster,Weakly Supervised Phrase Localization with Multi-Scale Anchored Transformer Network,"Fang Zhao, National University of Singapore; Jianshu Li, National University of Singapo; Jian Zhao, NUS; Jiashi Feng,",,,
592,2730,Poster,Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking,"Filip Radenovic, CTU Prague; Ahmet Iscen, Inria; Giorgos Tolias, Czech Technical University in Prague; Yannis Avrithis, Inria; Ondrej Chum, Czech Technical University in Prague",Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking,"In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected.   An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.",http://arxiv.org/pdf/1803.11285v1
593,4249,Poster,Cross-Dataset Adaptation for Visual Question Answering,"Wei-Lun Chao, USC; Hexiang Hu, ; Fei Sha, University of Southern California",,,
594,2048,Poster,Globally Optimal Inlier Set Maximization for Atlanta Frame Estimation,"Kyungdon Joo, ; Tae-Hyun Oh, MIT; In So Kweon, KAIST; Jean-Charles Bazin, KAIST",,,
595,2064,Poster,End-to-end Convolutional Semantic Embeddings,"Quanzeng You, Microsoft; Zhengyou Zhang, Microsoft Research; Jiebo Luo, University of Rochester",URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection,"Malicious URLs host unsolicited content and are used to perpetrate cybercrimes. It is imperative to detect them in a timely manner. Traditionally, this is done through the usage of blacklists, which cannot be exhaustive, and cannot detect newly generated malicious URLs. To address this, recent years have witnessed several efforts to perform Malicious URL Detection using Machine Learning. The most popular and scalable approaches use lexical properties of the URL string by extracting Bag-of-words like features, followed by applying machine learning models such as SVMs. There are also other features designed by experts to improve the prediction performance of the model. These approaches suffer from several limitations: (i) Inability to effectively capture semantic meaning and sequential patterns in URL strings; (ii) Requiring substantial manual feature engineering; and (iii) Inability to handle unseen features and generalize to test data. To address these challenges, we propose URLNet, an end-to-end deep learning framework to learn a nonlinear URL embedding for Malicious URL Detection directly from the URL. Specifically, we apply Convolutional Neural Networks to both characters and words of the URL String to learn the URL embedding in a jointly optimized framework. This approach allows the model to capture several types of semantic information, which was not possible by the existing models. We also propose advanced word-embeddings to solve the problem of too many rare words observed in this task. We conduct extensive experiments on a large-scale dataset and show a significant performance gain over existing methods. We also conduct ablation studies to evaluate the performance of various components of URLNet.",http://arxiv.org/pdf/1802.03162v2
596,2788,Poster,Referring Image Segmentation via Recurrent Refinement Networks,"Ruiyu Li, CUHK; Kaican Li, CUHK; Yi-Chun Kuo, CUHK; Michelle Shu, ; Xiaojuan Qi, CUHK; Xiaoyong Shen, CUHK; Jiaya Jia, Chinese University of Hong Kong",,,
597,3495,Poster,Two can play this Game: Visual Dialog with Discriminative Visual Question Generation and Visual Question Answering,"Unnat Jain, UIUC; Lana Lazebnik, ; Alex Schwing,",Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering,"Human conversation is a complex mechanism with subtle nuances. It is hence an ambitious goal to develop artificial intelligence agents that can participate fluently in a conversation. While we are still far from achieving this goal, recent progress in visual question answering, image captioning, and visual question generation shows that dialog systems may be realizable in the not too distant future. To this end, a novel dataset was introduced recently and encouraging results were demonstrated, particularly for question answering. In this paper, we demonstrate a simple symmetric discriminative baseline, that can be applied to both predicting an answer as well as predicting a question. We show that this method performs on par with the state of the art, even memory net based methods. In addition, for the first time on the visual dialog dataset, we assess the performance of a system asking questions, and demonstrate how visual dialog can be generated from discriminative question generation and question answering.",http://arxiv.org/pdf/1803.11186v1
598,289,Poster,Generative Adversarial Learning Towards Fast Weakly Supervised Detection,"Yunhang Shen, Xiamen University; Rongrong Ji, ; Shengchuan Zhang, ; Wangmeng Zuo, Harbin Institute of Technology; Yan Wang, Microsoft",,,
599,3244,Poster,Deeper Look at Power Normalizations.,"Piotr Koniusz, Data61/CSIRO; Hongguang Zhang, Data61; Fatih Porikli, NICTA, Australia",,,
600,1746,Poster,Dimensionalitys Blessing: Detecting the distributions underlying images,"Wen-Yan Lin, ADSC; Yasuyuki Matsushita, Osaka University; Siying Liu, I2r.a-star.edu.sg; Jianhuang Lai, Sun Yat-sen University",,,
601,1989,Poster,Eliminating Background-bias for Robust Person Re-identification,"Maoqing Tian, Sensetime Limited; Shuai Yi, The Chinese University of Hong Kong; Hongsheng Li, ; Shihua Li, ; Xuesen Zhang, SenseTime; Jianping Shi, SenseTime; Junjie Yan, ; Xiaogang Wang, Chinese University of Hong Kong",,,
602,1501,Poster,Learning Discriminative Evaluation Metrics for Image Captioning,"Yin Cui, CornellTech; Guandao Yang, Cornell University; Andreas Veit, Cornel Tech ; Xun Huang, ; Serge Belongie,",,,
603,246,Poster,Single-Shot Object Detection with Enriched Semantics,"Zhishuai Zhang, Johns Hopkins University; Siyuan Qiao, Johns Hopkins University; Cihang Xie, JHU; Wei Shen, Shanghai University; Bo Wang, HikVision USA Inc.; Alan Yuille, JHU",Single-Shot Object Detection with Enriched Semantics,"We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of object detection features within a typical deep detector, by a semantic segmentation branch and a global activation module. The segmentation branch is supervised by weak segmentation ground-truth, i.e., no extra annotation is required. In conjunction with that, we employ a global activation module which learns relationship between channels and object classes in a self-supervised manner. Comprehensive experimental results on both PASCAL VOC and MS COCO detection datasets demonstrate the effectiveness of the proposed method. In particular, with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of 32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on a Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on VOC2007 with an inference speed of 13.0 milliseconds per image.",http://arxiv.org/pdf/1712.00433v2
604,1162,Poster,Low-Shot Recognition with Imprinted Weights,"Hang Qi, UCLA; Matthew Brown, ; David Lowe,",Low-Shot Learning with Imprinted Weights,"Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process weight imprinting as it directly sets weights for a new category based on an appropriately scaled copy of the embedding layer activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine-tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings.",http://arxiv.org/pdf/1712.07136v2
605,4272,Poster,Neural Motifs: Scene Graph Parsing with Global Context,"Rowan Zellers, University of Washington; Mark Yatskar, University of Washington; Samuel Thomson, Carnegie Mellon University; Yejin Choi, University of Washington",Neural Motifs: Scene Graph Parsing with Global Context,"We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs.",http://arxiv.org/pdf/1711.06640v2
606,2701,Poster,Variational Autoencoders for Deforming 3D Mesh Models,"Qingyang Tan, UCAS; Lin Gao, Chinese Academy of Sciences; Yu-Kun Lai, Cardiff University; Shihong Xia, Institute of Computing Technology, CAS, Beijing, China",Variational Autoencoders for Deforming 3D Mesh Models,"3D geometric contents are becoming increasingly popular. In this paper, we study the problem of analyzing deforming 3D meshes using deep neural networks. Deforming 3D meshes are flexible to represent 3D animation sequences as well as collections of objects of the same category, allowing diverse shapes with large-scale non-linear deformations. We propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. The framework is easy to train, and requires very few training examples. We also propose an extended model which allows flexibly adjusting the significance of different latent variables by altering the prior distribution. Extensive experiments demonstrate that our general framework is able to learn a reasonable representation for a collection of deformable shapes, and produce competitive results for a variety of applications, including shape generation, shape interpolation, shape space embedding and shape exploration, outperforming state-of-the-art methods.",http://arxiv.org/pdf/1709.04307v3
607,4001,Poster,Fast Monte-Carlo Localization on Aerial Vehicles using Approximate Continuous Belief Representations,"Aditya Dhawale, Carnegie Mellon University; Kumar Shaurya Shankar, Carnegie Mellon University; Nathan Michael, Carnegie Mellon University",Fast Monte-Carlo Localization on Aerial Vehicles using Approximate Continuous Belief Representations,"Size, weight, and power constrained platforms impose constraints on computational resources that introduce unique challenges in implementing localization algorithms. We present a framework to perform fast localization on such platforms enabled by the compressive capabilities of Gaussian Mixture Model representations of point cloud data. Given raw structural data from a depth sensor and pitch and roll estimates from an on-board attitude reference system, a multi-hypothesis particle filter localizes the vehicle by exploiting the likelihood of the data originating from the mixture model. We demonstrate analysis of this likelihood in the vicinity of the ground truth pose and detail its utilization in a particle filter-based vehicle localization strategy, and later present results of real-time implementations on a desktop system and an off-the-shelf embedded platform that outperform localization results from running a state-of-the-art algorithm on the same environment.",http://arxiv.org/pdf/1712.05507v3
608,936,Poster,DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map,"Peng Wang, Baidu; Ruigang Yang, University of Kentucky; Binbin Cao, Baidu; Wei Xu, ; Yuanqing Lin,",,,
609,1645,Poster,LiDAR-Video Driving Dataset: Learning Driving Policies Effectively,"Yiping Chen, Xiamen University; Jingkang Wang, Shanghai Jiao Tong University; Cewu Lu, Shanghai Jiao Tong University; Zhipeng Luo, Xiamen University; Jonathan Li, University of Waterloo; Han Xue, Shanghai Jiao Tong University; Cheng Wang, Xiamen University",,,
610,2998,Poster,Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks,"Alexander Sage, ETH Zurich; Eirikur Agustsson, ETH Zurich; Radu Timofte, ETH Zurich; Luc Van Gool, KTH",Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks,"Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training. We are able to generate a high diversity of plausible logos and we demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. Moreover, we validate the proposed clustered GAN training on CIFAR 10, achieving state-of-the-art Inception scores when using synthetic labels obtained via clustering the features of an ImageNet classifier. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models will be made publicly available at https://data.vision.ee.ethz.ch/cvl/lld/.",http://arxiv.org/pdf/1712.04407v1
611,2236,Poster,Egocentric Basketball Motion Planning from a Single First-Person Image,"Gedas Bertasius, University of Pennsylvania; Aaron Chan, U. of Southern California; Jianbo Shi, University of Pennsylvania, USA",Egocentric Basketball Motion Planning from a Single First-Person Image,"We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence.   Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs).",http://arxiv.org/pdf/1803.01413v1
612,116,Poster,Human-centric Indoor Scene Synthesis Using Stochastic Grammar,"Siyuan Qi, UCLA; Yixin Zhu, UCLA; Siyuan Huang, UCLA; Chenfanfu Jiang, ; Song-Chun Zhu,",,,
613,962,Poster,Rotation-sensitive Regression for Oriented Scene Text Detection,"Minghui Liao, Huazhong University of Science and Technology; Zhen Zhu, Huazhong University of Science and Technology; Baoguang Shi, Huazhong University of Science and Technology; Gui-Song Xia, Wuhan University; Xiang Bai, Huazhong University of Science and Technology",Rotation-Sensitive Regression for Oriented Scene Text Detection,"Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotation-sensitive features. The proposed method named Rotation-sensitive Regression Detector (RRD) achieves state-of-the-art performance on three oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection.",http://arxiv.org/pdf/1803.05265v1
614,252,Poster,Separating Self-Expression and Visual Content in Hashtag Supervision,"Andreas Veit, Cornel Tech ; Maximillian Nickel,  ; Serge Belongie, ; Laurens van der Maaten, Facebook",Separating Self-Expression and Visual Content in Hashtag Supervision,"The variety, abundance, and structured nature of hashtags make them an interesting data source for training vision models. For instance, hashtags have the potential to significantly reduce the problem of manual supervision and annotation when learning vision models for a large number of concepts. However, a key challenge when learning from hashtags is that they are inherently subjective because they are provided by users as a form of self-expression. As a consequence, hashtags may have synonyms (different hashtags referring to the same visual content) and may be ambiguous (the same hashtag referring to different visual content). These challenges limit the effectiveness of approaches that simply treat hashtags as image-label pairs. This paper presents an approach that extends upon modeling simple image-label pairs by modeling the joint distribution of images, hashtags, and users. We demonstrate the efficacy of such approaches in image tagging and retrieval experiments, and show how the joint model can be used to perform user-conditional retrieval and tagging.",http://arxiv.org/pdf/1711.09825v1
615,1056,Poster,Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning,"Jongchan Park, KAIST; Joon-Young Lee, ; Donggeun Yoo, Lunit; In So Kweon, KAIST",,,
616,999,Oral,Im2Flow: Motion Hallucination from Static Images for Action Recognition,"Ruohan Gao, University of Texas at Austin; Bo Xiong, UT-Austin; Kristen Grauman,",Im2Flow: Motion Hallucination from Static Images for Action Recognition,"Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.",http://arxiv.org/pdf/1712.04109v1
617,778,Oral,"Finding It"": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video""","De-An Huang, Stanford University; Shyamal Buch, Stanford University; Lucio Dery, Stanford University; Animesh Garg, Stanford University; Fei-Fei Li, Stanford University; Juan Carlos Niebles, Stanford University",Neon2: Finding Local Minima via First-Order Oracles,"We propose a reduction for non-convex optimization that can (1) turn an stationary-point finding algorithm into an local-minimum finding one, and (2) replace the Hessian-vector product computations with only gradient computations. It works both in the stochastic and the deterministic settings, without hurting the algorithm's performance.   As applications, our reduction turns Natasha2 into a first-order method without hurting its performance. It also converts SGD, GD, SCSG, and SVRG into algorithms finding approximate local minima, outperforming some best known results.",http://arxiv.org/pdf/1711.06673v2
618,1458,Oral,Actor and Action Video Segmentation from a Sentence,"Kirill Gavrilyuk, University of Amsterdam; Amir Ghodrati, University of Amsterdam; zhenyang Li, University of Amsterdam; Cees Snoek, University of Amsterdam",Actor and Action Video Segmentation from a Sentence,"This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art.",http://arxiv.org/pdf/1803.07485v1
619,3516,Oral,Egocentric Activity Recognition on a Budget,"Rafael Possas, University of Sydney; Sheila Maricela Pinto Caceres, University of Sydney; Fabio Ramos, University of Sydney",,,
620,1249,Spotlight,Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF,"Linchao Bao, Tencent AI Lab; Baoyuan Wu, Tencent AI Lab; Wei Liu,",CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF,"This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatio-temporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very high-order dependencies. To this end, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors.",http://arxiv.org/pdf/1803.09453v1
621,1284,Spotlight,Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints,"Alexander Richard, University of Bonn; Hilde Kuehne, University of Bonn; Juergen Gall, University of Bonn, Germany",,,
622,1317,Spotlight,Low-Latency Video Semantic Segmentation,"Yule Li, Ict; Jianping Shi, SenseTime; Dahua Lin, CUHK",Low-Latency Video Semantic Segmentation,"Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components: (1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms.",http://arxiv.org/pdf/1804.00389v1
623,668,Spotlight,Fine-grained Video Captioning for Sports Narrative,"Huanyu Yu, Shanghai Jiao Tong University; Shuo Cheng, SJTU; Bingbing Ni, ; Minsi Wang, Shanghai Jiao Tong University; Zhang Jian, Shanghai Jiao Tong University; Xiaokang Yang,",,,
624,2113,Spotlight,End-to-End Learning of Motion Representation for Video Understanding,"Lijie Fan, Tsinghua University; Wenbing Huang, Tencent AI Lab; Chuang Gan, Tsinghua University; Stefano  Ermon, Stanford University; Junzhou Huang, UT Arlingtron; Boqing Gong, University of Central Florida",End-to-End Learning of Motion Representation for Video Understanding,"Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.",http://arxiv.org/pdf/1804.00413v1
625,3006,Spotlight,Compressed Video Action Recognition,"Chao-Yuan Wu, UT Austin; Manzil Zaheer, Carnegie Mellon University; Hexiang Hu, ; R. Manmatha, A9; Alexander Smola, ; Philipp Krahenbuhl,",Compressed Video Action Recognition,"Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video.   This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset.",http://arxiv.org/pdf/1712.00636v2
626,3817,Spotlight,Good Appearance Features for Multi-Target Multi-Camera Tracking,"Ergys Ristani, Duke University; Carlo Tomasi, Duke University",Multi-camera Multi-Object Tracking,"In this paper, we propose a pipeline for multi-target visual tracking under multi-camera system. For multi-camera system tracking problem, efficient data association across cameras, and at the same time, across frames becomes more important than single-camera system tracking. However, most of the multi-camera tracking algorithms emphasis on single camera across frame data association. Thus in our work, we model our tracking problem as a global graph, and adopt Generalized Maximum Multi Clique optimization problem as our core algorithm to take both across frame and across camera data correlation into account all together. Furthermore, in order to compute good similarity scores as the input of our graph model, we extract both appearance and dynamic motion similarities. For appearance feature, Local Maximal Occurrence Representation(LOMO) feature extraction algorithm for ReID is conducted. When it comes to capturing the dynamic information, we build Hankel matrix for each tracklet of target and apply rank estimation with Iterative Hankel Total Least Squares(IHTLS) algorithm to it. We evaluate our tracker on the challenging Terrace Sequences from EPFL CVLAB as well as recently published Duke MTMC dataset.",http://arxiv.org/pdf/1709.07065v1
627,1063,Spotlight,AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions,"Chunhui Gu, Google; Chen Sun, Google; David Ross, Google Research; Carl Vondrick, Google; Caroline Pantofaru, Google; Yeqng Li, Google Inc.; Sudheendra Vijayanarasimhan, Google Research; George Toderici, Google; Susanna Ricco, Google; Rahul Sukthankar, Google Research; Cordelia Schmid, INRIA Grenoble, France; Jitendra Malik,",AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions,"This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 192 15-minute video clips, where actions are localized in space and time, resulting in 740k action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. We will release the dataset publicly.   AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 16.2% mAP, underscoring the need for developing new approaches for video understanding.",http://arxiv.org/pdf/1705.08421v3
628,2261,Spotlight,Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination,"Hazel Doughty, University of Bristol; Dima Damen, University of Bristol; Walterio Mayol-Cuevas,",,,
629,3292,Spotlight,MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories and head poses,"Irtiza Hasan, University of Verona; Francesco Setti, ; Theodore Tsesmelis, ; Alessio Del Bue, Istituto Italiano di Tecnologia (IIT); Fabio Galasso, ; Marco Cristani, U. Verona",,,
630,1163,Oral,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Peter Anderson, Australian National University; Xiaodong He, ; Chris Buehler, ; Damien Teney, Unversity of Adelaide; Mark Johnson, Macquarie University; Stephen Gould, Australian National University; Lei Zhang, Microsoft",Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",http://arxiv.org/pdf/1707.07998v3
631,3586,Oral,Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering,"Nguyen Duy Kien, Tohoku University; Takayuki Okatani, Tohoku University/RIKEN AIP",Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering,"A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.",http://arxiv.org/pdf/1804.00775v1
632,3275,Oral,FlipDial: A Generative Model for Two-Way Visual Dialogue,"Daniela Massiceti, University of Oxford; Siddharth Narayanaswamy, University of Oxford; Puneet Kumar Dokania, University of Oxford; Phil Torr, Oxford",FlipDial: A Generative Model for Two-Way Visual Dialogue,"We present FlipDial, a generative model for visual dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art model in the sequential answering task (one-way visual dialogue) on the VisDial dataset by 5 points in Mean Rank using the generated answers. We are the first to extend this paradigm to full two-way visual dialogue, where our model is capable of generating both questions and answers in sequence based on a visual input, for which we propose a set of novel evaluation measures and metrics.",http://arxiv.org/pdf/1802.03803v2
633,185,Oral,Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning,"Qi Wu, University of Adelaide; Peng Wang, ; Chunhua Shen, University of Adelaide; Ian Reid, ; Anton Van den Hengel, University of Adelaide",,,
634,811,Spotlight,Visual Question Generation as Dual Task of Visual Question Answering,"Yikang Li, ; Nan Duan, Microsoft; Bolei Zhou, Massachuate Institute of Technology; Xiao Chu, Baidu; Wanli Ouyang, The University of Sydney; Xiaogang Wang, Chinese University of Hong Kong",Visual Question Generation as Dual Task of Visual Question Answering,"Recently visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, which have been explored separately. In this work, we propose an end-to-end unified framework, the Invertible Question Answering Network (iQAN), to leverage the complementary relations between questions and answers in images by jointly training the model on VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms are proposed as constraints to explicitly leverage Q,A's dependencies to guide the training process. After training, iQAN can take either question or answer as input, then output the counterpart. Evaluated on the large-scale visual question answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy over the baselines. We also show the dual learning framework of iQAN can be generalized to other VQA architectures and consistently improve the results over both the VQA and VQG tasks.",http://arxiv.org/pdf/1709.07192v1
635,1139,Spotlight,Unsupervised Textual Grounding: Linking Words to Image Concepts,"Raymond Yeh, UIUC; Minh Do, University of Illinois at Urbana-Champaign; Alex Schwing,",Unsupervised Textual Grounding: Linking Words to Image Concepts,"Textual grounding, i.e., linking words to objects in images, is a challenging but important task for robotics and human-computer interaction. Existing techniques benefit from recent progress in deep learning and generally formulate the task as a supervised learning problem, selecting a bounding box from a set of possible options. To train these deep net based approaches, access to a large-scale datasets is required, however, constructing such a dataset is time-consuming and expensive. Therefore, we develop a completely unsupervised mechanism for textual grounding using hypothesis testing as a mechanism to link words to detected image concepts. We demonstrate our approach on the ReferIt Game dataset and the Flickr30k data, outperforming baselines by 7.98% and 6.96% respectively.",http://arxiv.org/pdf/1803.11185v1
636,1240,Spotlight,Answer with Grounding Snippets: Focal Visual-Text Attention for  Visual Question Answering,"Junwei Liang, Carnegie Mellon University; Lu Jiang, ; Liangliang Cao, ; Alexander Hauptmann,",,,
637,1561,Spotlight,SeGAN: Segmenting and Generating the Invisible,"KIANA EHSANI, 1993; Roozbeh Mottaghi, Allen Institute for Artificial Intelligence; Ali Farhadi,",SeGAN: Segmenting and Generating the Invisible,"Objects often occlude each other in scenes; Inferring their appearance beyond their visible parts plays an important role in scene understanding, depth estimation, object interaction and manipulation. In this paper, we study the challenging problem of completing the appearance of occluded objects. Doing so requires knowing which pixels to paint (segmenting the invisible parts of objects) and what color to paint them (generating the invisible parts). Our proposed novel solution, SeGAN, jointly optimizes for both segmentation and generation of the invisible parts of objects. Our experimental results show that: (a) SeGAN can learn to generate the appearance of the occluded parts of objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the invisible parts of objects; (c) trained on synthetic photo realistic images, SeGAN can reliably segment natural images; (d) by reasoning about occluder occludee relations, our method can infer depth layering.",http://arxiv.org/pdf/1703.10239v2
638,2603,Spotlight,Cascade R-CNN: Delving into High Quality Object Detection,"Zhaowei Cai, UC San Diego; Nuno Vasconcelos, UCSD, USA",,,
639,2801,Spotlight,Learning Semantic Concepts and Order for Image and Sentence Matching,"Yan Huang, ; Qi Wu, University of Adelaide; Liang Wang, unknown",Learning Semantic Concepts and Order for Image and Sentence Matching,"Image and sentence matching has made great progress recently, but it remains challenging due to the large visual-semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.",http://arxiv.org/pdf/1712.02036v1
640,471,Spotlight,Functional Map of the World,"Gordon Christie, JHU/APL; Neil Fendley, JHU/APL; James Wilson, DigitalGlobe; Ryan Mukherjee, JHU/APL",Rhythmic Representations: Learning Periodic Patterns for Scalable Place Recognition at a Sub-Linear Storage Cost,"Robotic and animal mapping systems share many challenges and characteristics: they must function in a wide variety of environmental conditions, enable the robot or animal to navigate effectively to find food or shelter, and be computationally tractable from both a speed and storage perspective. With regards to map storage, the mammalian brain appears to take a diametrically opposed approach to all current robotic mapping systems. Where robotic mapping systems attempt to solve the data association problem to minimise representational aliasing, neurons in the brain intentionally break data association by encoding large (potentially unlimited) numbers of places with a single neuron. In this paper, we propose a novel method based on supervised learning techniques that seeks out regularly repeating visual patterns in the environment with mutually complementary co-prime frequencies, and an encoding scheme that enables storage requirements to grow sub-linearly with the size of the environment being mapped. To improve robustness in challenging real-world environments while maintaining storage growth sub-linearity, we incorporate both multi-exemplar learning and data augmentation techniques. Using large benchmark robotic mapping datasets, we demonstrate the combined system achieving high-performance place recognition with sub-linear storage requirements, and characterize the performance-storage growth trade-off curve. The work serves as the first robotic mapping system with sub-linear storage scaling properties, as well as the first large-scale demonstration in real-world environments of one of the proposed memory benefits of these neurons.",http://arxiv.org/pdf/1712.07315v2
641,836,Spotlight,MegDet: A Large Mini-Batch Object Detector,"Chao Peng, Megvii; Tete Xiao, Peking University; Zeming Li, Tsinghua University, Megvii; Yuning Jiang, Megvii inc.; Xiangyu Zhang, Megvii Inc; Kai Jia, Mevii; Gang Yu, Face++; Jian Sun,",MegDet: A Large Mini-Batch Object Detector,"The improvements in recent CNN-based object detection works, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from new network, new framework, or novel loss design. But mini-batch size, a key factor in the training, has not been well studied. In this paper, we propose a Large MiniBatch Object Detector (MegDet) to enable the training with much larger mini-batch size than before (e.g. from 16 to 256), so that we can effectively utilize multiple GPUs (up to 128 in our experiments) to significantly shorten the training time. Technically, we suggest a learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our submission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task.",http://arxiv.org/pdf/1711.07240v3
642,2657,Spotlight,Learning Globally Optimized Object Detector via Policy Gradient,"Yongming Rao, ; Dahua Lin, CUHK; Jiwen Lu, Tsinghua University",,,
643,2823,Spotlight,Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network,"Zizhao Zhang, University of Florida; Yuanpu Xie, University of Florida; Lin Yang,",Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network,"This paper presents a novel method to deal with the challenging task of generating photographic images conditioned on semantic image descriptions. Our method introduces accompanying hierarchical-nested adversarial objectives inside the network hierarchies, which regularize mid-level representations and assist generator training to capture the complex image statistics. We present an extensile single-stream generator architecture to better adapt the jointed discriminators and push generated images up to high resolutions. We adopt a multi-purpose adversarial loss to encourage more effective image and text information usage in order to improve the semantic consistency and image fidelity simultaneously. Furthermore, we introduce a new visual-semantic similarity measure to evaluate the semantic consistency of generated images. With extensive experimental validation on three public datasets, our method significantly improves previous state of the arts on all datasets over different evaluation metrics.",http://arxiv.org/pdf/1802.09178v2
644,270,Oral,Illuminant Spectra-based Source Separation Using Flash Photography,"Zhuo Hui, Carnegie Mellon University; Kalyan Sunkavalli, Adobe Systems Inc.; Sunil Hadap, ; Aswin Sankaranarayanan, Carnegie Mellon University",Illuminant Spectra-based Source Separation Using Flash Photography,"Real-world lighting often consists of multiple illuminants with different spectra. Separating and manipulating these illuminants in post-process is a challenging problem that requires either significant manual input or calibrated scene geometry and lighting. In this work, we leverage a flash/no-flash image pair to analyze and edit scene illuminants based on their spectral differences. We derive a novel physics-based relationship between color variations in the observed flash/no-flash intensities and the spectra and surface shading corresponding to individual scene illuminants. Our technique uses this constraint to automatically separate an image into constituent images lit by each illuminant. This separation can be used to support applications like white balancing, lighting editing, and RGB photometric stereo, where we demonstrate results that outperform state-of-the-art techniques on a wide range of images.",http://arxiv.org/pdf/1704.05564v2
645,3054,Oral,Trapping Light for Time of Flight,"Ruilin Xu, Columbia University; Mohit Gupta, Wisconsin; Shree Nayar, Columbia University",Two-photon excitation of rubidium atoms inside porous glass,"We study the two-photon laser excitation to the 5D$_{5/2}$ energy level of $^{85}$Rb atoms contained in the interstices of a porous material made from sintered ground glass with typical pore dimensions in the 10 - 100 $\mu$m range. The excitation spectra show unusual flat-top lineshapes which are shown to be the consequence of wave-vector randomization of the laser light in the porous material. For large atomic densities, the spectra are affected by radiation trapping around the D2 transitions. The effect of the transient atomic response limited by time of flight between pores walls appears to have a minor influence in the excitation spectra. It is however revealed by the shortening of the temporal evolution of the emitted blue light following a sudden switch-off of the laser excitation.",http://arxiv.org/pdf/1706.04868v1
646,2765,Oral,The Perception-Distortion Tradeoff,"Yochai Blau, Technion; Tomer Michaeli, Technion",The Perception-Distortion Tradeoff,"Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. However, as we show experimentally, for some measures it is less severe (e.g. distance between VGG features). We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.",http://arxiv.org/pdf/1711.06077v2
647,906,Spotlight,Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images,"Hao Zhou, UMD; Jin Sun, University of Maryland; Yaser Yacoob, Univ of Maryland; David Jacobs, University of Maryland",Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images,"Lighting estimation from face images is an important task and has applications in many areas such as image editing, intrinsic image decomposition, and image forgery detection. We propose to train a deep Convolutional Neural Network (CNN) to regress lighting parameters from a single face image. Lacking massive ground truth lighting labels for face images in the wild, we use an existing method to estimate lighting parameters, which are treated as ground truth with unknown noises. To alleviate the effect of such noises, we utilize the idea of Generative Adversarial Networks (GAN) and propose a Label Denoising Adversarial Network (LDAN) to make use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images. Experiments show that our network outperforms existing methods in producing consistent lighting parameters of different faces under similar lighting conditions. Moreover, our method is 100,000 times faster in execution time than prior optimization-based lighting estimation approaches.",http://arxiv.org/pdf/1709.01993v1
648,1697,Spotlight,Optimal Structured Light a la Carte,"Parsa Mirdehghan, University of Toronto; Wenzheng Chen, UofT; Kyros Kutulakos,",,,
649,281,Spotlight,Tracking Multiple Objects Outside the Line of Sight using Speckle Imaging,"Brandon Smith, University of Wisconsin-Madiso; Matthew O'Toole, Stanford University; Mohit Gupta, Wisconsin",,,
650,3977,Spotlight,Inferring Light Fields from Shadows,"Manel Baradad, MIT; Vickie Ye, MIT; Adam Yedida, MIT; Fredo Durand, ; William Freeman, MIT/Google; Gregory Wornell, ; Antonio Torralba, MIT",Accretion kinematics through the warped transition disk in HD142527 from resolved CO(6-5) observations,"The finding of residual gas in the large central cavity of the HD142527 disk motivates questions on the origin of its non-Keplerian kinematics, and possible connections with planet formation. We aim to understand the physical structure that underlies the intra-cavity gaseous flows, guided by new molecular-line data in CO(6-5) with unprecedented angular resolutions. Given the warped structure inferred from the identification of scattered-light shadows cast on the outer disk, the kinematics are consistent, to first order, with axisymmetric accretion onto the inner disk occurring at all azimuth. A steady-state accretion profile, fixed at the stellar accretion rate, explains the depth of the cavity as traced in CO isotopologues. The abrupt warp and evidence for near free-fall radial flows in HD 142527 resemble theoretical models for disk tearing, which could be driven by the reported low mass companion, whose orbit may be contained in the plane of the inner disk. The companion's high inclination with respect to the massive outer disk could drive Kozai oscillations over long time-scales; high-eccentricity periods may perhaps account for the large cavity. While shadowing by the tilted disk could imprint an azimuthal modulation in the molecular-line maps, further observations are required to ascertain the significance of azimuthal structure in the density field inside the cavity of HD142527.",http://arxiv.org/pdf/1505.07732v1
651,2805,Spotlight,Modifying Non-Local Variations Across Multiple Views,"Tal Tlusty, Technion; Tomer Michaeli, Technion; Tali Dekel, Google; Lihi Zelnik-Manor,",,,
652,3970,Spotlight,Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework,"Jie Chen, Nanyang Technological University; Cheen-Hau Tan, ; Junhui Hou, City University of Hong Kong; Lap-Pui Chau, Nanyang Technological University; He Li,",Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework,"Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of mis-alignment blur. Extensive evaluations show that up to 5 dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera.",http://arxiv.org/pdf/1803.10433v1
653,754,Spotlight,"SfSNet : Learning Shape, Reflectance and Illuminance of Faces `in the wild'","Soumyadip Sengupta, University of Maryland; Angjoo Kanazawa, University of Maryland; Carlos Castillo, ; David Jacobs, University of Maryland","SfSNet : Learning Shape, Reflectance and Illuminance of Faces in the Wild","We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained image of a human face into shape, reflectance and illuminance. Our network is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real world images. This allows the network to capture low frequency variations from synthetic images and high frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation.",http://arxiv.org/pdf/1712.01261v1
654,1567,Spotlight,Deep Photo Enhancer: Unsupervised Learning of Image Enhancement from Photographs with GANs,"Yu-Sheng Chen, National Taiwan University; Yu-Ching Wang, National Taiwan University; Man-Hsin Kao, National Taiwan University; Yung-Yu Chuang, National Taiwan University",,,
655,2344,Spotlight,LIME: Live Intrinsic Material Estimation,"Abhimitra Meka, Max Planck Institute for Infor; Maxim Maximov, Graduate School of Computer Science, Saarland University; Michael Zollhöfer, MPI Informatics; Avishek Chatterjee, Max Planck Institute for Informatics; Hans-Peter Seidel, Max Planck Institute for Informatics; Christian Richardt, University of Bath; Christian Theobalt, MPI Informatics",,,
656,878,Spotlight,Learning to Detect Features in Texture Images,"Linguang Zhang, Princeton University; Szymon Rusinkiewicz, Princeton University",Learning the Synthesizability of Dynamic Texture Samples,"A dynamic texture (DT) refers to a sequence of images that exhibit temporal regularities and has many applications in computer vision and graphics. Given an exemplar of dynamic texture, it is a dynamic but challenging task to generate new samples with high quality that are perceptually similar to the input exemplar, which is known to be {\em example-based dynamic texture synthesis (EDTS)}. Numerous approaches have been devoted to this problem, in the past decades, but none them are able to tackle all kinds of dynamic textures equally well. In this paper, we investigate the synthesizability of dynamic texture samples: {\em given a dynamic texture sample, how synthesizable it is by using EDTS, and which EDTS method is the most suitable to synthesize it?} To this end, we propose to learn regression models to connect dynamic texture samples with synthesizability scores, with the help of a compiled dynamic texture dataset annotated in terms of synthesizability. More precisely, we first define the synthesizability of DT samples and characterize them by a set of spatiotemporal features. Based on these features and an annotated dynamic texture dataset, we then train regression models to predict the synthesizability scores of texture samples and learn classifiers to select the most suitable EDTS methods. We further complete the selection, partition and synthesizability prediction of dynamic texture samples in a hierarchical scheme. We finally apply the learned synthesizability to detecting synthesizable regions in videos. The experiments demonstrate that our method can effectively learn and predict the synthesizability of DT samples.",http://arxiv.org/pdf/1802.00941v1
657,1969,Spotlight,Learning to Extract a Video Sequence from a Single Motion-Blurred Image,"Meiguang Jin, University of Bern, Switzerlan; Givi Meishvili, University of Bern, Switzerland; Paolo Favaro, Bern University, Switzerland",Bringing Alive Blurred Moments!,"We present a solution for the novel goal of extracting a video from a single blurred image to sequentially reconstruct the views of a scene as beheld by the camera during the time of exposure. We approach this task by first learning a motion representation for videos in an unsupervised manner through training of a novel video autoencoder network that performs a surrogate task of video reconstruction. Instead of enforc- ing temporal constraints on features learned at the image level, we em- ploy a recurrent design with convolutional filters to directly learn tempo- rally correlated representations. Once trained, this network is employed for the guided training of a motion encoder for blurred images. This net- work extracts embedded motion information from a single blurred image so as to generate a sharp video in conjunction with the trained recur- rent video decoder. Experiments on real scenes and standard data-sets demonstrate our framework's ability to generate a plausible sequence of sharp frames.",http://arxiv.org/pdf/1804.02913v1
658,3329,Spotlight,Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion,"Rushil Anirudh, Lawrence Livermore National La; Hyojin Kim, Lawrence Livermore National Laboratory; Jayaraman J. Thiagarajan, LLNL; K. Aditya Mohan, Lawrence Livermore National Laboratory; Kyle Champley, Lawrence Livermore National Laboratory; Timo Bremer, Lawrence Livermore National Laboratory",Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion,"Computed Tomography (CT) reconstruction is a fundamental component to a wide variety of applications ranging from security, to healthcare. The classical techniques require measuring projections, called sinograms, from a full 180$^\circ$ view of the object. This is impractical in a limited angle scenario, when the viewing angle is less than 180$^\circ$, which can occur due to different factors including restrictions on scanning time, limited flexibility of scanner rotation, etc. The sinograms obtained as a result, cause existing techniques to produce highly artifact-laden reconstructions. In this paper, we propose to address this problem through implicit sinogram completion, on a challenging real world dataset containing scans of common checked-in luggage. We propose a system, consisting of 1D and 2D convolutional neural networks, that operates on a limited angle sinogram to directly produce the best estimate of a reconstruction. Next, we use the x-ray transform on this reconstruction to obtain a ""completed"" sinogram, as if it came from a full 180$^\circ$ measurement. We feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction. We show with extensive experimentation that this combined strategy outperforms many competitive baselines. We also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by our network. We show that this measure is a strong indicator of quality as measured by the PSNR, while not requiring ground truth at test time. Finally, using a segmentation experiment, we show that our reconstruction preserves the 3D structure of objects effectively.",http://arxiv.org/pdf/1711.10388v2
659,3482,Spotlight,A Common Framework for Interactive Texture Transfer,"Yifang Men, Peking University; Zhouhui Lian, ; Jianguo Xiao, Peking University",,,
660,2946,Poster,AMNet: Memorability Estimation with Attention,"Jiri Fajtl, Kingston University; Vasileios Argyriou, Kingston University; Dorothy Monekosso, Leeds Beckett; Paolo Remagnino, Kingston University",AMNet: Memorability Estimation with Attention,"In this paper we present the design and evaluation of an end-to-end trainable, deep neural network with a visual attention mechanism for memorability estimation in still images. We analyze the suitability of transfer learning of deep models from image classification to the memorability task. Further on we study the impact of the attention mechanism on the memorability estimation and evaluate our network on the SUN Memorability and the LaMem datasets. Our network outperforms the existing state of the art models on both datasets in terms of the Spearman's rank correlation as well as the mean squared error, closely matching human consistency.",http://arxiv.org/pdf/1804.03115v1
661,3123,Poster,Blind Predicting Similar Quality Map for Image Quality Assessment,"Da Pan, Communication University of CN; Ping Shi, ; Ming Hou, ; Zefeng Ying, ; Sizhe Fu, ; Yuan Zhang,",Learn to Evaluate Image Perceptual Quality Blindly from Statistics of Self-similarity,"Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) is particularly challenging due to the absence of knowledge about the reference image and distortion type. Features based on natural scene statistics (NSS) have been successfully used in BIQA, while the quality relevance of the feature plays an essential role to the quality prediction performance. Motivated by the fact that the early processing stage in human visual system aims to remove the signal redundancies for efficient visual coding, we propose a simple but very effective BIQA method by computing the statistics of self-similarity (SOS) in an image. Specifically, we calculate the inter-scale similarity and intra-scale similarity of the distorted image, extract the SOS features from these similarities, and learn a regression model to map the SOS features to the subjective quality score. Extensive experiments demonstrate very competitive quality prediction performance and generalization ability of the proposed SOS based BIQA method.",http://arxiv.org/pdf/1510.02884v1
662,421,Poster,Deep End-to-End Time-of-Flight Imaging,"Shuochen Su, University of British Columbia; Felix Heide, Stanford University; Gordon Wetzstein, ; Wolfgang Heidrich,",ATGV-Net: Accurate Depth Super-Resolution,"In this work we present a novel approach for single depth map super-resolution. Modern consumer depth sensors, especially Time-of-Flight sensors, produce dense depth measurements, but are affected by noise and have a low lateral resolution. We propose a method that combines the benefits of recent advances in machine learning based single image super-resolution, i.e. deep convolutional networks, with a variational method to recover accurate high-resolution depth maps. In particular, we integrate a variational method that models the piecewise affine structures apparent in depth data via an anisotropic total generalized variation regularization term on top of a deep network. We call our method ATGV-Net and train it end-to-end by unrolling the optimization procedure of the variational method. To train deep networks, a large corpus of training data with accurate ground-truth is required. We demonstrate that it is feasible to train our method solely on synthetic data that we generate in large quantities for this task. Our evaluations show that we achieve state-of-the-art results on three different benchmarks, as well as on a challenging Time-of-Flight dataset, all without utilizing an additional intensity image as guidance.",http://arxiv.org/pdf/1607.07988v1
663,3751,Poster,Aperture Supervision for Monocular Depth Estimation,"Pratul Srinivasan, Berkeley; Rahul Garg, ; Neal Wadhwa, ; Ren Ng, Berkeley; Jonathan Barron, Google",Aperture Supervision for Monocular Depth Estimation,"We present a novel method to train machine learning algorithms to estimate scene depths from a single image, by using the information provided by a camera's aperture as supervision. Prior works use a depth sensor's outputs or images of the same scene from alternate viewpoints as supervision, while our method instead uses images from the same viewpoint taken with a varying camera aperture. To enable learning algorithms to use aperture effects as supervision, we introduce two differentiable aperture rendering functions that use the input image and predicted depths to simulate the depth-of-field effects caused by real camera apertures. We train a monocular depth estimation network end-to-end to predict the scene depths that best explain these finite aperture images as defocus-blurred renderings of the input all-in-focus image.",http://arxiv.org/pdf/1711.07933v2
664,648,Poster,Seeing Temporal Modulation of Lights from Standard Cameras,"Naoki Sakakibara, Nagoya Institute of Technology; Fumihiko Sakaue, Nagoya Institute of Technology; JUN SATO, Nagoya Institute of Technology",,,
665,2162,Poster,Statistical Tomography of Microscopic Life,"Aviad Levis, Technion Institute of Technology; Ronen Talmon, Technion - Israel Institute of Technology; Yoav Schechner, Technion Haifa, Israel",,,
666,2165,Poster,Divide and Conquer for Full-Resolution Light Field Deblurring,"Mahesh Mohan M R, IIT Madras; A.N. Rajagopalan, IIT Madras",,,
667,3636,Poster,Multispectral Image Intrinsic Decomposition via Low Rank Constraint,"Qian Huang, Nanjing University; Zhu Weixin, Nanjing university; Yang Zhao, Nanjing University; Linsen Chen, Nanjing University; yao wang, new york university; Tao Yue, Nanjing Univ.; Xun Cao, EE Department, Nanjing Univ",Multispectral Image Intrinsic Decomposition via Low Rank Constraint,"Multispectral images contain many clues of surface characteristics of the objects, thus can be widely used in many computer vision tasks, e.g., recolorization and segmentation. However, due to the complex illumination and the geometry structure of natural scenes, the spectra curves of a same surface can look very different. In this paper, a Low Rank Multispectral Image Intrinsic Decomposition model (LRIID) is presented to decompose the shading and reflectance from a single multispectral image. We extend the Retinex model, which is proposed for RGB image intrinsic decomposition, for multispectral domain. Based on this, a low rank constraint is proposed to reduce the ill-posedness of the problem and make the algorithm solvable. A dataset of 12 images is given with the ground truth of shadings and reflectance, so that the objective evaluations can be conducted. The experiments demonstrate the effectiveness of proposed method.",http://arxiv.org/pdf/1802.08793v1
668,35,Poster,Improving Color Reproduction Accuracy in the Camera Imaging Pipeline,"Hakki Karaimer, York University; Michael Brown, York University",,,
669,2648,Poster,A Closer Look at Spatiotemporal Convolutions for Action Recognition,"Du Tran, Dartmouth College; heng Wang, ; Lorenzo Torresani, Darthmout College, USA; Jamie Ray, Facebook; Manohar Paluri,",A Closer Look at Spatiotemporal Convolutions for Action Recognition,"In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block ""R(2+1)D"" which gives rise to CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101 and HMDB51.",http://arxiv.org/pdf/1711.11248v2
670,3118,Poster,Inferring Co-Attention in Social Scene Videos,"Lifeng Fan, VCLA@UCLA; Yixin Chen, VCLA@UCLA; Ping Wei, Xi'an Jiaotong University; Song-Chun Zhu,",,,
671,3241,Poster,Making Convolutional Networks Recurrent for Visual Sequence Learning,"Xiaodong Yang, NVIDIA; Pavlo Molchanov, NVIDIA Research; Jan Kautz, NVIDIA",Recurrent Filter Learning for Visual Tracking,"Recently using convolutional neural networks (CNNs) has gained popularity in visual tracking, due to its robust feature representation of images. Recent methods perform online tracking by fine-tuning a pre-trained CNN model to the specific target object using stochastic gradient descent (SGD) back-propagation, which is usually time-consuming. In this paper, we propose a recurrent filter generation methods for visual tracking. We directly feed the target's image patch to a recurrent neural network (RNN) to estimate an object-specific filter for tracking. As the video sequence is a spatiotemporal data, we extend the matrix multiplications of the fully-connected layers of the RNN to a convolution operation on feature maps, which preserves the target's spatial structure and also is memory-efficient. The tracked object in the subsequent frames will be fed into the RNN to adapt the generated filters to appearance variations of the target. Note that once the off-line training process of our network is finished, there is no need to fine-tune the network for specific objects, which makes our approach more efficient than methods that use iterative fine-tuning to online learn the target. Extensive experiments conducted on widely used benchmarks, OTB and VOT, demonstrate encouraging results compared to other recent methods.",http://arxiv.org/pdf/1708.03874v1
672,308,Poster,Real-world Anomaly Detection in Surveillance Videos,"Waqas  Sultani, ; Chen Chen, University of Central Florida; Mubarak Shah, UCF",Real-world Anomaly Detection in Surveillance Videos,"Surveillance videos are able to capture a variety of realistic anomalies. In this paper, we propose to learn anomalies by exploiting both normal and anomalous videos. To avoid annotating the anomalous segments or clips in training videos, which is very time consuming, we propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training videos, i.e. the training labels (anomalous or normal) are at video-level instead of clip-level. In our approach, we consider normal and anomalous videos as bags and video segments as instances in multiple instance learning (MIL), and automatically learn a deep anomaly ranking model that predicts high anomaly scores for anomalous video segments. Furthermore, we introduce sparsity and temporal smoothness constraints in the ranking loss function to better localize anomaly during training. We also introduce a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies such as fighting, road accident, burglary, robbery, etc. as well as normal activities. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities. Our experimental results show that our MIL method for anomaly detection achieves significant improvement on anomaly detection performance as compared to the state-of-the-art approaches. We provide the results of several recent deep learning baselines on anomalous activity recognition. The low recognition performance of these baselines reveals that our dataset is very challenging and opens more opportunities for future work. The dataset is available at: http://crcv.ucf.edu/projects/real-world/",http://arxiv.org/pdf/1801.04264v2
673,593,Poster,Viewpoint-aware Attentive Multi-view Inference for Vehicle Re-identification,"Yi Zhou, University of East Anglia; Ling Shao, University of East Anglia",,,
674,609,Poster,Efficient Video Object Segmentation via Network Modulation,"Linjie Yang, Snap Research; YANRAN WANG, NORTHWESTERN; Xuehan Xiong, Snapchat; Jianchao Yang, Snap; Aggelos Katsaggelos, Northwestern University",Efficient Video Object Segmentation via Network Modulation,"Video object segmentation targets at segmenting a specific object throughout a video sequence, given only an annotated first frame. Recent deep learning based approaches find it effective by fine-tuning a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent. Despite the high accuracy these methods achieve, the fine-tuning process is inefficient and fail to meet the requirements of real world applications. We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object. Specifically, a second meta neural network named modulator is learned to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object. The experiments show that our approach is 70times faster than fine-tuning approaches while achieving similar accuracy.",http://arxiv.org/pdf/1802.01218v1
675,1522,Poster,Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment,"Li Ding, MIT; Chenliang Xu, University of Rochester",Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment,"In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods.",http://arxiv.org/pdf/1803.10699v1
676,3818,Poster,Depth-Aware Stereo Video Retargeting,"Bing Li, University of Southern Califor; Chia-Wen Lin, ; Tiejun Huang, ; Boxin Shi, Peking University; Wen Gao, ; C.-C. Jay Kuo, University of Southern California",,,
677,121,Poster,Instance Embedding Transfer to Unsupervised Video Object Segmentation,"Siyang Li, USC; Bryan Seybold, Google Research; Alexey Vorobyov, Google Inc.; Alireza Fathi, Stanford University; Qin Huang, University of Southern California; C.-C. Jay Kuo, University of Southern California",Instance Embedding Transfer to Unsupervised Video Object Segmentation,"We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset.",http://arxiv.org/pdf/1801.00908v2
678,429,Poster,Future Frame Prediction for Anomaly Detection  A New Baseline,"Wen Liu, ShanghaiTech University; Weixin Luo, Shanghaitech University; Dongze Lian, ShanghaiTech University; Shenghua Gao, ShanghaiTech University",,,
679,482,Poster,Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?,"Kensho Hara, AIST; Hirokatsu Kataoka, AIST; Yutaka Satoh, AIST",,,
680,717,Poster,Dynamic Video Segmentation Network,"Yu-Shuan Xu, National Tsing Hua University; Chun-Yi Lee, National Tsing Hua University; TSUJUI FU, NTHUCS; HsuanKung Yang, National Tsing Hua University",Dynamic Video Segmentation Network,"In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation. DVSNet consists of two convolutional neural networks: a segmentation network and a flow network. The former generates highly accurate semantic segmentations, but is deeper and slower. The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations. We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score. Frame regions with a higher expected confidence score traverse the flow network. Frame regions with a lower expected confidence score have to pass through the segmentation network. We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network. The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset. DVSNet is also able to reduce up to 95\% of the computational workloads.",http://arxiv.org/pdf/1804.00931v1
681,1067,Poster,Recognize Actions by Disentangling Components of Dynamics,"Yue Zhao, CUHK; Yuanjun Xiong, Amazon ; Dahua Lin, CUHK",,,
682,1924,Poster,Motion-Appearance Co-Memory Networks for Video Question Answering,"Jiyang Gao, ; Runzhou Ge, Univ. of Southern California; Kan Chen, Univ. of Southern California; Ram Nevatia,",Motion-Appearance Co-Memory Networks for Video Question Answering,"Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based these observations, we propose a motion-appearance comemory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.",http://arxiv.org/pdf/1803.10906v1
683,2302,Poster,Learning to Understand Image Blur,"Shanghang Zhang, ; Xiaohui Shen, Adobe Research; Zhe Lin, Adobe Systems, Inc.; Radomír Mech, ; João Costeira, ; Jose Moura, Carnegie Mellon University",Learning to Detect Multiple Photographic Defects,"In this paper, we introduce the problem of simultaneously detecting multiple photographic defects. We aim at detecting the existence, severity, and potential locations of common photographic defects related to color, noise, blur and composition. The automatic detection of such defects could be used to provide users with suggestions for how to improve photos without the need to laboriously try various correction methods. Defect detection could also help users select photos of higher quality while filtering out those with severe defects in photo curation and summarization.   To investigate this problem, we collected a large-scale dataset of user annotations on seven common photographic defects, which allows us to evaluate algorithms by measuring their consistency with human judgments. Our new dataset enables us to formulate the problem as a multi-task learning problem and train a multi-column deep convolutional neural network (CNN) to simultaneously predict the severity of all the defects. Unlike some existing single-defect estimation methods that rely on low-level statistics and may fail in many cases on natural photographs, our model is able to understand image contents and quality at a higher level. As a result, in our experiments, we show that our model has predictions with much higher consistency with human judgments than low-level methods as well as several baseline CNN models. Our model also performs better than an average human from our user study.",http://arxiv.org/pdf/1612.01635v5
684,2992,Poster,Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation,"Piotr Bilinski, University of Oxford; Victor Prisacariu, Oxford",,,
685,3857,Poster,Generative Adversarial Image Synthesis with Decision Tree Latent Controller,"Takuhiro Kaneko, NTT Corporation; Kaoru Hiramatsu, NTT Corporation; Kunio Kashino, NTT",,,
686,97,Poster,Learning a Discriminative Prior for Blind Image Deblurring,"Lerenhan Li, HUST; Jinshan Pan, UC Merced; Wei-Sheng Lai, University of California, Merced; Changxin Gao, HUST; Nong Sang, ; Ming-Hsuan Yang, UC Merced",Learning a Discriminative Prior for Blind Image Deblurring,"We present an effective blind image deblurring method based on a data-driven discriminative prior.Our work is motivated by the fact that a good image prior should favor clear images over blurred images.In this work, we formulate the image prior as a binary classifier which can be achieved by a deep convolutional neural network (CNN).The learned prior is able to distinguish whether an input image is clear or not.Embedded into the maximum a posterior (MAP) framework, it helps blind deblurring in various scenarios, including natural, face, text, and low-illumination images.However, it is difficult to optimize the deblurring method with the learned image prior as it involves a non-linear CNN.Therefore, we develop an efficient numerical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model.Furthermore, the proposed model can be easily extended to non-uniform deblurring.Both qualitative and quantitative experimental results show that our method performs favorably against state-of-the-art algorithms as well as domain-specific image deblurring approaches.",http://arxiv.org/pdf/1803.03363v2
687,303,Poster,Frame-Recurrent Video Super-Resolution,"Mehdi S. M. Sajjadi, Max Planck Institute for Intel; Raviteja Vemulapalli, Google; Matthew Brown,",Frame-Recurrent Video Super-Resolution,"Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results.   In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.",http://arxiv.org/pdf/1801.04590v4
688,510,Poster,Discovering Point Lights with Intensity Distance Fields,"Edward Zhang, University of Washington; MIchael Cohen, ; Brian Curless, Washington",The Ultra-luminous M81 X-9 source: 20 years variability and spectral states,"The source X-9 was discovered with the {\it Einstein Observatory} in the field of M81, and is located in the dwarf galaxy Holmberg IX. X-9 has a 0.2-4.0 keV luminosity of $\sim 5.5\times 10^{39}$ ergs~s$^{-1}$, if it is at the same distance as Holmberg IX (3.4 Mpc). This luminosity is above the Eddington luminosity of a 1~$M_{\odot}$ compact accreting object. Past hypotheses on the nature of this Super-Eddington source included a SNR or supershell, an accreting compact object and a background QSO. To shed light on the nature of this source, we have obtained and analyzed archival data, including the {\it Einstein} data, 23 ROSAT observations, Beppo-SAX and ASCA pointings. Our analysis reveals that most of the emission of X-9 arises from a point-like highly-variable source, and that lower luminosity extended emission may be associated with it. The spectrum of this source changes between low and high intensity states, in a way reminiscent of the spectra of galactic Black Hole candidates. Our result strongly suggest that X-9 is not a background QSO, but a bonafide `Super-Eddington' source in Ho IX, a dwarf companion of M81.",http://arxiv.org/pdf/astro-ph/0103250v1
689,786,Poster,Video Rain Removal By Multiscale Convolutional Sparse Coding,"Li Minghan, Xi'an Jiaotong University; Qi Xie, ; Qian Zhao, ; Wei Wei, Xi'an Jiaotong University; Shuhang Gu, ; Jing Tao, ; Deyu Meng, Xi'an Jiaotong University",,,
690,1392,Poster,Stereoscopic Neural Style Transfer,"Dongdong Chen, ; Lu Yuan, Microsoft Research Asia; Jing Liao, ; Nenghai Yu, ; Gang Hua, Microsoft Research",Neural Stereoscopic Image Style Transfer,"Neural style transfer is an emerging technique which is able to endow daily-life images with attractive artistic styles. Previous work has succeeded in applying convolutional neural network (CNN) to style transfer for monocular images or videos. However, style transfer for stereoscopic images is still a missing piece. Different from processing a monocular image, the two views of a stylized stereoscopic pair are required to be consistent to provide the observer a comfortable visual experience. In this paper, we propose a dual path network for view-consistent style transfer on stereoscopic images. While each view of the stereoscopic pair is processed in an individual path, a novel feature aggregation strategy is proposed to effectively share information between the two paths. Besides a traditional perceptual loss used for controlling style transfer quality in each view, a multi-layer view loss is proposed to enforce the network to coordinate the learning of both paths to generate view-consistent stylized results. Extensive experiments show that, compared with previous methods, the proposed model can generate stylized stereoscopic images which achieve the best view consistency.",http://arxiv.org/pdf/1802.09985v2
691,2602,Poster,Multi-Frame Quality Enhancement for Compressed Video,"Ren Yang, Beihang University; Mai Xu, Beihang University; Zulin Wang, Beihang University; Tianyi Li, Beihang University",Multi-Frame Quality Enhancement for Compressed Video,"The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, ignoring the similarity between consecutive frames. In this paper, we investigate that heavy quality fluctuation exists across compressed video frames, and thus low quality frames can be enhanced using the neighboring high quality frames, seen as Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as a first attempt in this direction. In our approach, we firstly develop a Support Vector Machine (SVM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are as the input. The MF-CNN compensates motion between the non-PQF and PQFs through the Motion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement subnet (QE-subnet) reduces compression artifacts of the non-PQF with the help of its nearest PQFs. Finally, the experiments validate the effectiveness and generality of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code of our MFQE approach is available at https://github.com/ryangBUAA/MFQE.git",http://arxiv.org/pdf/1803.04680v4
692,2799,Poster,CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition,"Anil Baslamisli, University of Amsterdam; Hoang-An Le, University of Amsterdam; Theo Gevers, University of Amsterdam",CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition,"Most of the traditional work on intrinsic image decomposition rely on deriving priors about scene characteristics. On the other hand, recent research use deep learning models as in-and-out black box and do not consider the well-established, traditional image formation process as the basis of their intrinsic learning process. As a consequence, although current deep learning approaches show superior performance when considering quantitative benchmark results, traditional approaches are still dominant in achieving high qualitative results. In this paper, the aim is to exploit the best of the two worlds. A method is proposed that (1) is empowered by deep learning capabilities, (2) considers a physics-based reflection model to steer the learning process, and (3) exploits the traditional approach to obtain intrinsic images by exploiting reflectance and shading gradient information. The proposed model is fast to compute and allows for the integration of all intrinsic components. To train the new model, an object centered large-scale datasets with intrinsic ground-truth images are created. The evaluation results demonstrate that the new model outperforms existing methods. Visual inspection shows that the image formation loss function augments color reproduction and the use of gradient information produces sharper edges. Datasets, models and higher resolution images are available at https://ivi.fnwi.uva.nl/cv/retinet.",http://arxiv.org/pdf/1712.01056v2
693,2914,Poster,Image Restoration by Estimating Frequency Distribution of Local Patches,"Jaeyoung Yoo, Seoul National University; Sang ho Lee, Seoul National University; Nojun Kwak, Seoul National University",,,
694,3535,Poster,Latent RANSAC,"Simon Korman, Weizmann Institute; Roee Litman, Tel-Aviv University",Latent RANSAC,"We present a method that can evaluate a RANSAC hypothesis in constant time, i.e. independent of the size of the data. A key observation here is that correct hypotheses are tightly clustered together in the latent parameter domain. In a manner similar to the generalized Hough transform we seek to find this cluster, only that we need as few as two votes for a successful detection. Rapidly locating such pairs of similar hypotheses is made possible by adapting the recent ""Random Grids"" range-search technique. We only perform the usual (costly) hypothesis verification stage upon the discovery of a close pair of hypotheses. We show that this event rarely happens for incorrect hypotheses, enabling a significant speedup of the RANSAC pipeline. The suggested approach is applied and tested on three robust estimation problems: camera localization, 3D rigid alignment and 2D-homography estimation. We perform rigorous testing on both synthetic and real datasets, demonstrating an improvement in efficiency without a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D alignment results on the challenging ""Redwood"" loop-closure challenge.",http://arxiv.org/pdf/1802.07045v1
695,3805,Poster,Two-Stream Convolutional Networks for Dynamic Texture Synthesis,"Matthew Tesfaldet, York University; Marcus Brubaker, York University; Konstantinos Derpanis, Ryerson University",Two-Stream Convolutional Networks for Dynamic Texture Synthesis,"We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a noise input sequence is optimized to simultaneously match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture imagery. Finally, we quantitatively evaluate our approach with a thorough user study.",http://arxiv.org/pdf/1706.06982v2
696,64,Poster,Towards Open-Set Identity Preserving Face Synthesis,"Jianmin Bao, USTC; Dong Chen, Microsoft Research Asia; Fang Wen, ; Houqiang Li, ; Gang Hua, Microsoft Research",Towards Open-Set Identity Preserving Face Synthesis,"We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.",http://arxiv.org/pdf/1803.11182v1
697,68,Poster,Learning from the Deep:  A Revised Underwater Image Formation Model,"Derya Akkaynak, University of Haifa; Tali Treibitz, University of Haifa",,,
698,362,Poster,Graph-Cut RANSAC,"Daniel Barath, MTA SZTAKI; Jiri Matas,",Five-point Fundamental Matrix Estimation for Uncalibrated Cameras,"We aim at estimating the fundamental matrix in two views from five correspondences of rotation invariant features obtained by e.g.\ the SIFT detector. The proposed minimal solver first estimates a homography from three correspondences assuming that they are co-planar and exploiting their rotational components. Then the fundamental matrix is obtained from the homography and two additional point pairs in general position. The proposed approach, combined with robust estimators like Graph-Cut RANSAC, is superior to other state-of-the-art algorithms both in terms of accuracy and number of iterations required. This is validated on synthesized data and $561$ real image pairs. Moreover, the tests show that requiring three points on a plane is not too restrictive in urban environment and locally optimized robust estimators lead to accurate estimates even if the points are not entirely co-planar. As a potential application, we show that using the proposed method makes two-view multi-motion estimation more accurate.",http://arxiv.org/pdf/1803.00260v1
699,1925,Poster,Temporal Deformable Residual Networks for Action Segmentation in Videos,"Peng Lei, Oregon State University; Sinisa Todorovic,",,,
700,2013,Poster,Weakly Supervised Action Localization by Sparse Temporal Pooling Network,"Phuc Nguyen, University of California, Irvine; Ting Liu, Google, Inc.; Gautam Prasad, Google, Inc.; Bohyung Han, Seoul National University",Weakly Supervised Action Localization by Sparse Temporal Pooling Network,"We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision.",http://arxiv.org/pdf/1712.05080v2
701,3170,Poster,PoseFlow: A Deep Motion Representation for Understanding Human Behaviors in Videos,"Dingwen Zhang, ; Guangyu Guo, ; Dong Huang, Carnegie Mellon University; Fernando de la Torre, ; Junwei Han, Northwestern Polytechnical U.",,,
702,3494,Poster,FFNet: Video Fast-Forwarding via Reinforcement Learning,"Shuyue Lan, Northwestern University; Rameswar Panda, UC Riverside; Qi Zhu, UC Riverside; Amit Roy-Chowdhury, UC Riverside",,,
703,3981,Poster,Multi-shot Pedestrian Re-identification via Sequential Decision Making,"Jianfu Zhang, Shanghai Jiaotong University; Naiyan Wang, tusimple; Liqing Zhang, Shanghai Jiaotong University",Multi-shot Pedestrian Re-identification via Sequential Decision Making,"Multi-shot pedestrian re-identification problem is at the core of surveillance video analysis. It matches two tracks of pedestrians from different cameras. In contrary to existing works that aggregate single frames features by time series model such as recurrent neural network, in this paper, we propose an interpretable reinforcement learning based approach to this problem. Particularly, we train an agent to verify a pair of images at each time. The agent could choose to output the result (same or different) or request another pair of images to see (unsure). By this way, our model implicitly learns the difficulty of image pairs, and postpone the decision when the model does not accumulate enough evidence. Moreover, by adjusting the reward for unsure action, we can easily trade off between speed and accuracy. In three open benchmarks, our method are competitive with the state-of-the-art methods while only using 3% to 6% images. These promising results demonstrate that our method is favorable in both efficiency and performance.",http://arxiv.org/pdf/1712.07257v1
704,330,Poster,Attend and Interact: Higher-Order Object Interactions for Video Understanding,"CHIH-YAO MA, GEORGIA TECH; Asim Kadav, NEC Labs; Iain Melvin, ; Zsolt Kira, ; Ghassan AlRegib, ; Hans Peter Graf,",Attend and Interact: Higher-Order Object Interactions for Video Understanding,"Human actions often involve complex interactions across several inter-related objects in the scene. However, existing approaches to fine-grained video understanding or visual relationship detection often rely on single object representation or pairwise object relationships. Furthermore, learning interactions across multiple objects in hundreds of frames for video is computationally infeasible and performance may suffer since a large combinatorial space has to be modeled. In this paper, we propose to efficiently learn higher-order interactions between arbitrary subgroups of objects for fine-grained video understanding. We demonstrate that modeling object interactions significantly improves accuracy for both action recognition and video captioning, while saving more than 3-times the computation over traditional pairwise relationships. The proposed method is validated on two large-scale datasets: Kinetics and ActivityNet Captions. Our SINet and SINet-Caption achieve state-of-the-art performances on both datasets even though the videos are sampled at a maximum of 1 FPS. To the best of our knowledge, this is the first work modeling object interactions on open domain large-scale video datasets, and we additionally model higher-order object interactions which improves the performance with low computational costs.",http://arxiv.org/pdf/1711.06330v2
705,1258,Poster,Where and Why Are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks,"Ping Wei, Xi'an Jiaotong University; Yang Liu, UCLA; Tianmin Shu, University of California, Los Angeles; Nanning Zheng, Xi'an Jiaotong University; Song-Chun Zhu,",,,
706,1761,Poster,Fully Convolutional Adaptation Networks for Semantic Segmentation,"Yiheng Zhang, University of Science and Technology of China; Zhaofan Qiu, University of Science and Technology of China; Ting Yao, Microsoft Research Asia; Dong Liu, Univ Sci Tech China; Tao Mei, Microsoft Research Asia",Low-Latency Video Semantic Segmentation,"Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components: (1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms.",http://arxiv.org/pdf/1804.00389v1
707,2131,Poster,Semantic Video Segmentation by Gated Recurrent Flow Propagation,"David Nilsson, Lund University; Cristian Sminchisescu,",Semantic Video Segmentation by Gated Recurrent Flow Propagation,"Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our extensive experiments in the challenging CityScapes and Camvid datasets, and based on multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.",http://arxiv.org/pdf/1612.08871v2
708,3102,Poster,Interpretable Video Captioning via Trajectory Structured Localization,"Xian Wu, Sysu; Guanbin Li, ; Liang Lin,",,,
709,540,Poster,Learning to Hash by Discrepancy Minimization,"Zhixiang Chen, Tsinghua University; Xin Yuan, Tsinghua University; Jiwen Lu, Tsinghua University; Jie Zhou,",,,
710,642,Poster,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,"Xiangyu Zhang, Megvii Inc; Xinyu Zhou, Megvii Technology Inc.; Mengxiao Lin, Megvii Technology Ltd.(Face++); Jian Sun,",ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,"We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.",http://arxiv.org/pdf/1707.01083v2
711,740,Poster,Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs,"Xiaolong Wang, Carnegie Mellon University; Yufei Ye, Carnegie Mellon University; Abhinav Gupta,",Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs,"We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 ~ 3% on some metrics to whopping 20% on a few).",http://arxiv.org/pdf/1803.08035v2
712,792,Poster,Referring Relationships,"Ranjay Krishna, Stanford University; Ines Chami, Stanford University; Michael Bernstein, Stanford University; Fei-Fei Li, Stanford University",Referring Relationships,"Images are not simply sets of objects: each image represents a web of interconnected relationships. These relationships between entities carry semantic meaning and help a viewer differentiate between instances of an entity. For example, in an image of a soccer match, there may be multiple persons present, but each participates in different relationships: one is kicking the ball, and the other is guarding the goal. In this paper, we formulate the task of utilizing these ""referring relationships"" to disambiguate between entities of the same category. We introduce an iterative model that localizes the two entities in the referring relationship, conditioned on one another. We formulate the cyclic condition between the entities in a relationship by modelling predicates that connect the entities as shifts in attention from one entity to another. We demonstrate that our model can not only outperform existing approaches on three datasets --- CLEVR, VRD and Visual Genome --- but also that it produces visually meaningful predicate shifts, as an instance of interpretable neural networks. Finally, we show that by modelling predicates as attention shifts, we can even localize entities in the absence of their category, allowing our model to find completely unseen categories.",http://arxiv.org/pdf/1803.10362v2
713,794,Poster,Improving Object Localization with Fitness NMS and Bounded IoU Loss,"Lachlan Tychsen-Smith, CSIRO (Data61); Lars Petersson,",Improving Object Localization with Fitness NMS and Bounded IoU Loss,"We demonstrate that many detection methods are designed to identify only a sufficently accurate bounding box, rather than the best available one. To address this issue we propose a simple and fast modification to the existing methods called Fitness NMS. This method is tested with the DeNet model and obtains a significantly improved MAP at greater localization accuracies without a loss in evaluation rate, and can be used in conjunction with Soft NMS for additional improvements. Next we derive a novel bounding box regression loss based on a set of IoU upper bounds that better matches the goal of IoU maximization while still providing good convergence properties. Following these novelties we investigate RoI clustering schemes for improving evaluation rates for the DeNet wide model variants and provide an analysis of localization performance at various input image dimensions. We obtain a MAP of 33.6%@79Hz and 41.8%@5Hz for MSCOCO and a Titan X (Maxwell). Source code available from: https://github.com/lachlants/denet",http://arxiv.org/pdf/1711.00164v3
714,847,Poster,End-to-End Deep Kronecker-Product Matching for Person Re-identification,"Yantao Shen, CUHK; Tong Xiao, The Chinese University of HK; Hongsheng Li, ; Shuai Yi, The Chinese University of Hong Kong; Xiaogang Wang, Chinese University of Hong Kong",,,
715,849,Poster,Semantic Visual Localization,"Johannes Schönberger, ETH Zurich; Marc Pollefeys, ETH; Andreas Geiger, MPI Tuebingen / ETH Zuerich; Torsten Sattler, ETH Zurich",Finding beans in burgers: Deep semantic-visual embedding with localization,"Several works have proposed to learn a two-path neural network that maps images and texts, respectively, to a same shared Euclidean space where geometry captures useful semantic relationships. Such a multi-modal embedding can be trained and used for various tasks, notably image captioning. In the present work, we introduce a new architecture of this type, with a visual path that leverages recent space-aware pooling mechanisms. Combined with a textual path which is jointly trained from scratch, our semantic-visual embedding offers a versatile model. Once trained under the supervision of captioned images, it yields new state-of-the-art performance on cross-modal retrieval. It also allows the localization of new concepts from the embedding space into any input image, delivering state-of-the-art result on the visual grounding of phrases.",http://arxiv.org/pdf/1804.01720v2
716,952,Poster,Objects as context for detecting their semantic parts,"Abel Gonzalez-Garcia, University of Edinburgh; Davide Modolo, Amazon; Vitto Ferrari,",Objects as context for detecting their semantic parts,"We present a semantic part detection approach that effectively leverages object information.We use the object appearance and its class as indicators of what parts to expect. We also model the expected relative location of parts inside the objects based on their appearance. We achieve this with a new network module, called OffsetNet, that efficiently predicts a variable number of part locations within a given object. Our model incorporates all these cues to detect parts in the context of their objects. This leads to considerably higher performance for the challenging task of part detection compared to using part appearance alone (+5 mAP on the PASCAL-Part dataset). We also compare to other part detection methods on both PASCAL-Part and CUB200-2011 datasets.",http://arxiv.org/pdf/1703.09529v3
717,1016,Poster,End-to-end weakly-supervised semantic alignment,"Ignacio ROCCO, Inria; Relja Arandjelovic, DeepMind; Josef Sivic,",End-to-end weakly-supervised semantic alignment,"We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence aligning two images depicting objects of the same category. This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter. We present the following three principal contributions. First, we develop a convolutional neural network architecture for semantic alignment that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs. The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time. Second, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter. Third, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment.",http://arxiv.org/pdf/1712.06861v1
718,1109,Poster,Dynamic Zoom-in Network for Fast Object Detection in Large Images,"Mingfei Gao, University of Maryland; Ruichi Yu, ; Ang Li, Google DeepMind; Vlad Morariu, University of Maryland; Larry Davis, University of Maryland, USA",Dynamic Zoom-in Network for Fast Object Detection in Large Images,"We introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images. Detection progresses in a coarse-to-fine manner, first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy. Built upon reinforcement learning, our approach consists of a model (R-net) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model (Q-net) that sequentially selects regions to zoom in. Experiments on the Caltech Pedestrians dataset show that our approach reduces the number of processed pixels by over 50% without a drop in detection accuracy. The merits of our approach become more significant on a high resolution test set collected from YFCC100M dataset, where our approach maintains high detection performance while reducing the number of processed pixels by about 70% and the detection time by over 50%.",http://arxiv.org/pdf/1711.05187v2
719,1408,Poster,Learning Markov Clustering Networks for Scene Text Detection,"ZICHUAN LIU, Nanyang Technological Universi; Guosheng Lin, Nanyang Technological Universi; Sheng Yang, Nanyang Technological University; Jiashi Feng, ; Weisi Lin, Nanyang Technological University; Wangling Goh, Nanyang Technological University",,,
720,1543,Poster,Deep Reinforcement Learning of Region Proposal Networks for Object Detection,"Aleksis Pirinen, Lund University; Cristian Sminchisescu,",Hierarchical Object Detection with Deep Reinforcement Learning,"We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions.",http://arxiv.org/pdf/1611.03718v2
721,1635,Poster,Beyond Holistic Object Recognition: Enriching Image Understanding with Part States,"Cewu Lu, Shanghai Jiao Tong University; hao Su, ; CK Tang, HKUST",Beyond Holistic Object Recognition: Enriching Image Understanding with Part States,"Important high-level vision tasks such as human-object interaction, image captioning and robotic manipulation require rich semantic descriptions of objects at part level. Based upon previous work on part localization, in this paper, we address the problem of inferring rich semantics imparted by an object part in still images. We propose to tokenize the semantic space as a discrete set of part states. Our modeling of part state is spatially localized, therefore, we formulate the part state inference problem as a pixel-wise annotation problem. An iterative part-state inference neural network is specifically designed for this task, which is efficient in time and accurate in performance. Extensive experiments demonstrate that the proposed method can effectively predict the semantic states of parts and simultaneously correct localization errors, thus benefiting a few visual understanding applications. The other contribution of this paper is our part state dataset which contains rich part-level semantic annotations.",http://arxiv.org/pdf/1612.07310v1
722,1946,Poster,Discriminability objective for training descriptive captions,"Ruotian Luo, Toyota Technological Institute; Scott Cohen, ; Brian Price, ; Greg Shakhnarovich,",Discriminability objective for training descriptive captions,"One property that remains lacking in image captions generated by contemporary methods is discriminability: being able to tell two images apart given the caption for one of them. We propose a way to improve this aspect of caption generation. By incorporating into the captioning training objective a loss component directly related to ability (by a machine) to disambiguate image/caption matches, we obtain systems that produce much more discriminative caption, according to human evaluation. Remarkably, our approach leads to improvement in other aspects of generated captions, reflected by a battery of standard scores such as BLEU, SPICE etc. Our approach is modular and can be applied to a variety of model/loss combinations commonly proposed for image captioning.",http://arxiv.org/pdf/1803.04376v1
723,2089,Poster,Visual Question Answering with Memory-Augmented Networks,"Chao Ma, ; Chunhua Shen, University of Adelaide; Anthony Dick, University of Adelaide; Qi Wu, University of Adelaide; Peng Wang, The University of Adelaide; Anton Van den Hengel, University of Adelaide; Ian Reid,",Visual Question Answering with Memory-Augmented Networks,"In this paper, we exploit a memory-augmented neural network to predict accurate answers to visual questions, even when those answers occur rarely in the training set. The memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which is important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. Experimental results on two large-scale benchmark datasets show the favorable performance of the proposed algorithm with a comparison to state of the art.",http://arxiv.org/pdf/1707.04968v2
724,2114,Poster,Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships,"Yong Liu, ICT; Ruiping Wang, Institute of Computing Technology, Chinese Academy of Sciences; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen,",,,
725,2198,Poster,Occluded Pedestrian Detection through Guided Attention in CNNs,"Shanshan Zhang, MPI; Jian Yang, Nanjing University of Science and Technology; Bernt Schiele, MPI Informatics Germany",,,
726,2290,Poster,Reward Learning by Instruction,"Hsiao-Yu Tung, Carnegie Mellon University; Adam Harley, Carnegie Mellon University; Katerina Fragkiadaki, Carnegie Mellon University",RLWS: A Reinforcement Learning based GPU Warp Scheduler,"The Streaming Multiprocessors (SMs) of a Graphics Processing Unit (GPU) execute instructions from a group of consecutive threads, called warps. At each cycle, an SM schedules a warp from a group of active warps and can context switch among the active warps to hide various stalls. Hence the performance of warp scheduler is critical to the performance of GPU. Several heuristic warp scheduling algorithms have been proposed which work well only for the situations they are designed for. GPU workloads are becoming very diverse in nature and hence one heuristic may not work for all cases. To work well over a diverse range of workloads, which might exhibit hitherto unseen characteristics, a warp scheduling algorithm must be able to adapt on-line.   We propose a Reinforcement Learning based Warp Scheduler (RLWS) which learns to schedule warps based on the current state of the core and the long-term benefits of scheduling actions, adapting not only to different types of workloads, but also to different execution phases in each workload. As the design space involving the state variables and the parameters (such as learning and exploration rates, reward and penalty values) used by RLWS is large, we use Genetic Algorithm to identify the useful subset of state variables and parameter values. We evaluated the proposed RLWS using the GPGPU-SIM simulator on a large number of workloads from the Rodinia, Parboil, CUDA-SDK and GPGPU-SIM benchmark suites and compared with other state-of-the-art warp scheduling methods. Our RL based implementation achieved either the best or very close to the best performance in 80\% of kernels with an average speedup of 1.06x over the Loose Round Robin strategy and 1.07x over the Two-Level strategy.",http://arxiv.org/pdf/1712.04303v1
727,2541,Poster,Weakly-Supervised Semantic Segmentation Network with Deep Seeded Region Growing,"Zilong Huang, HUST; Xinggang Wang, ; Jiasi Wang, HUST; Wenyu Liu, ; Jingdong Wang, Microsoft Research",,,
728,2979,Poster,PoTion: Pose MoTion Representation for Action Recognition,"Vasileios Choutas, Naver Labs Europe; Philippe Weinzaepfel, Xerox; Jerome Revaud, Naver Labs Europe; Cordelia Schmid, INRIA Grenoble, France",,,
729,3075,Poster,Bilateral Ordinal Relevance Multi-instance Regression for Facial Action Unit Intensity Estimation,"Yong Zhang, CASIA; Rui Zhao, Rensselaer Polytechnic Institu; Weiming Dong, ; Bao-Gang Hu, CASIA; Qiang Ji, RPI",,,
730,3259,Poster,Pulling Actions out of Context: Explicit Separation for Effective Combination,"Yang Wang, Stony Brook University; Minh Hoai, Stony Brook University",,,
731,3618,Poster,Dynamic Feature Learning for Partial Face Recognition,"Lingxiao He, Institute of AutomationChines; Haiqing Li, ; qi Zhang, ; Zhenan Sun, CRIPAC",,,
732,4066,Poster,Exploiting Transitivity for Learning Person Re-identification Models on a Budget,"Sourya Roy, UC Riverside ; Sujoy Paul, UC Riverside; Neal Young, UC Riverside ; Amit Roy-Chowdhury, UC Riverside",,,
733,4244,Poster,Deep Spatial Feature Reconstruction for Partial Person Re-identification,"Lingxiao He, Institute of AutomationChines; Jian Liang, CASIA; Haiqing Li, ; Zhenan Sun, CRIPAC",Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach,"Partial person re-identification (re-id) is a challenging problem, where only several partial observations (images) of people are available for matching. However, few studies have provided flexible solutions to identifying a person in an image containing arbitrary part of the body. In this paper, we propose a fast and accurate matching method to address this problem. The proposed method leverages Fully Convolutional Network (FCN) to generate fix-sized spatial feature maps such that pixel-level features are consistent. To match a pair of person images of different sizes, a novel method called Deep Spatial feature Reconstruction (DSR) is further developed to avoid explicit alignment. Specifically, DSR exploits the reconstructing error from popular dictionary learning models to calculate the similarity between different spatial feature maps. In that way, we expect that the proposed FCN can decrease the similarity of coupled images from different persons and increase that from the same person. Experimental results on two partial person datasets demonstrate the efficiency and effectiveness of the proposed method in comparison with several state-of-the-art partial person re-id approaches. Additionally, DSR achieves competitive results on a benchmark person dataset Market1501 with 83.58\% Rank-1 accuracy.",http://arxiv.org/pdf/1801.00881v2
734,92,Poster,Every Smile is Unique: Landmark-guided Diverse Smile Generation,"Wei Wang, University of Trento; Xavier Alameda-Pineda, University of Trento; Dan Xu, ; Elisa Ricci, U. Perugia; Nicu Sebe, University of Trento",Every Smile is Unique: Landmark-Guided Diverse Smile Generation,"Each smile is unique: one person surely smiles in different ways (e.g., closing/opening the eyes or mouth). Given one input image of a neutral face, can we generate multiple smile videos with distinctive characteristics? To tackle this one-to-many video generation problem, we propose a novel deep learning architecture named Conditional Multi-Mode Network (CMM-Net). To better encode the dynamics of facial expressions, CMM-Net explicitly exploits facial landmarks for generating smile sequences. Specifically, a variational auto-encoder is used to learn a facial landmark embedding. This single embedding is then exploited by a conditional recurrent network which generates a landmark embedding sequence conditioned on a specific expression (e.g., spontaneous smile). Next, the generated landmark embeddings are fed into a multi-mode recurrent landmark generator, producing a set of landmark sequences still associated to the given smile class but clearly distinct from each other. Finally, these landmark sequences are translated into face videos. Our experimental results demonstrate the effectiveness of our CMM-Net in generating realistic videos of multiple smile expressions.",http://arxiv.org/pdf/1802.01873v3
735,623,Poster,UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition,"Jiankang Deng, Imperial College London; Shiyang Cheng, Imperial College London; Niannan Xue, Imperial College London; Yuxiang Zhou, Imperial College; Stefanos Zafeiriou, Imperial College London",,,
736,643,Poster,Cascaded Pyramid Network for Multi-Person Pose Estimation,"Yilun Chen, Beihang University; Zhicheng Wang, Megvii(Face++); Yuxiang Peng, Tsinghua University; Zhiqiang Zhang, HUST; Gang Yu, Face++; Jian Sun,",Cascaded Pyramid Network for Multi-Person Pose Estimation,"The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these ""hard"" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the ""simple"" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the ""hard"" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge.Code (https://github.com/chenyilun95/tf-cpn.git) and the detection results are publicly available for further research.",http://arxiv.org/pdf/1711.07319v2
737,710,Poster,A Face to Face Neural Conversation Model,"Hang Chu, University of Toronto; Sanja Fidler,",The Rapidly Changing Landscape of Conversational Agents,"Conversational agents have become ubiquitous, ranging from goal-oriented systems for helping with reservations to chit-chat models found in modern virtual assistants. In this survey paper, we explore this fascinating field. We look at some of the pioneering work that defined the field and gradually move to the current state-of-the-art models. We look at statistical, neural, generative adversarial network based and reinforcement learning based approaches and how they evolved. Along the way we discuss various challenges that the field faces, lack of context in utterances, not having a good quantitative metric to compare models, lack of trust in agents because they do not have a consistent persona etc. We structure this paper in a way that answers these pertinent questions and discusses competing approaches to solve them.",http://arxiv.org/pdf/1803.08419v2
738,756,Poster,End-to-end Recovery of Human Shape and Pose,"Angjoo Kanazawa, University of Maryland; Michael Black, Max Planck Institute for Intelligent Systems; David Jacobs, University of Maryland; Jitendra Malik,",End-to-end Recovery of Human Shape and Pose,"We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allow our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, reprojection loss alone is highly under constrained. In this work we address this problem by introducing an adversary trained to tell whether a human body parameter is real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any coupled 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detection and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimizationbased methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.",http://arxiv.org/pdf/1712.06584v1
739,1287,Oral,Squeeze-and-Excitation Networks,"Jie Hu, Momenta; Li Shen, University of Oxford; Gang Sun, Momenta",Squeeze-and-Excitation Networks,"Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the ""Squeeze-and-Excitation"" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",http://arxiv.org/pdf/1709.01507v2
740,2523,Oral,"Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects","Md Amirul Islam, University of Manitoba; Mahmoud Kalash, University of Manitoba; Neil D. B. Bruce, University of Manitoba","Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects","Salient object detection is a problem that has been considered in detail and many solutions proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. The solution presented in this paper solves this more general problem that considers relative rank, and we propose data and metrics suitable to measuring success in a relative object saliency landscape. A novel deep learning solution is proposed based on a hierarchical representation of relative saliency and stage-wise refinement. We also show that the problem of salient object subitizing can be addressed with the same network, and our approach exceeds performance of any prior work across all metrics considered (both traditional and newly proposed).",http://arxiv.org/pdf/1803.05082v2
741,76,Oral,Context Encoding for Semantic Segmentation,"Hang Zhang, Rutgers University; Kristin Dana, ; Jianping Shi, SenseTime; Zhongyue Zhang, Amazon; Xiaogang Wang, Chinese University of Hong Kong; Ambrish Tyagi, Amazon; Amit Agrawal, Amazon",Context Encoding for Semantic Segmentation,"Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.",http://arxiv.org/pdf/1803.08904v1
742,3021,Spotlight,Creating Capsule Wardrobes from Fashion Images,"Wei-Lin Hsiao, UT-Austin; Kristen Grauman,",Creating Capsule Wardrobes from Fashion Images,"We propose to automatically create capsule wardrobes. Given an inventory of candidate garments and accessories, the algorithm must assemble a minimal set of items that provides maximal mix-and-match outfits. We pose the task as a subset selection problem. To permit efficient subset selection over the space of all outfit combinations, we develop submodular objective functions capturing the key ingredients of visual compatibility, versatility, and user-specific preference. Since adding garments to a capsule only expands its possible outfits, we devise an iterative approach to allow near-optimal submodular function maximization. Finally, we present an unsupervised approach to learn visual compatibility from ""in the wild"" full body outfit photos; the compatibility metric translates well to cleaner catalog photos and improves over existing methods. Our results on thousands of pieces from popular fashion websites show that automatic capsule creation has potential to mimic skilled fashionistas in assembling flexible wardrobes, while being significantly more scalable.",http://arxiv.org/pdf/1712.02662v1
743,3280,Spotlight,Webly Supervised Learning Meets Zero-shot Learning: A Hybrid Approach for Fine-grained Classification,"Li Niu, Rice University; Ashok Veeraraghavan, Rice University; Ashutosh  Sabharwal,",,,
744,405,Spotlight,"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models","Jiuxiang Gu, Nanyang Technological Universi; Jianfei Cai, ; Joty Shafiq Rayhan, ; Li Niu, Rice University; Gang Wang,","Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models","Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.",http://arxiv.org/pdf/1711.06420v1
745,1289,Spotlight,Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning,"Jingwen Wang, SCUT; Wenhao Jiang, Tencent AI Lab; Lin Ma, Tencent AI Lab; Wei Liu, ; Yong Xu, South China University of Technology",Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning,"Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video context. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100% (Meteor score increases from 4.82 to 9.65).",http://arxiv.org/pdf/1804.00100v2
746,2096,Spotlight,InLoc: Indoor Visual Localization with Dense Matching and View Synthesis,"Hajime Taira, Tokyo Institute of Technology; Masatoshi Okutomi, Tokyo Institute of Technology; Torsten Sattler, ETH Zurich; Mircea Cimpoi, Czech Institute of Informatics; Marc Pollefeys, ETH; Josef Sivic, ; Tomas Pajdla, ; Akihiko Torii, Tokyo Institute of Technology",InLoc: Indoor Visual Localization with Dense Matching and View Synthesis,"We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map. The contributions of this work are three-fold. First, we develop a new large-scale visual localization method targeted for indoor environments. The method proceeds along three steps: (i) efficient retrieval of candidate poses that ensures scalability to large-scale environments, (ii) pose estimation using dense matching rather than local features to deal with textureless indoor scenes, and (iii) pose verification by virtual view synthesis to cope with significant changes in viewpoint, scene layout, and occluders. Second, we collect a new dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario. Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data.",http://arxiv.org/pdf/1803.10368v2
747,2494,Spotlight,Towards High Performance Video Object Detection,"Xizhou Zhu, ; Jifeng Dai, Microsoft Research; Lu Yuan, Microsoft Research Asia; Yichen Wei, Microsoft Research Asia",Towards High Performance Video Object Detection,"There has been significant progresses for image object detection in recent years. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios.   Built upon the recent works, this work proposes a unified approach based on the principle of multi-frame end-to-end learning of features and cross-frame motion. Our approach extends prior works with three new techniques and steadily pushes forward the performance envelope (speed-accuracy tradeoff), towards high performance video object detection.",http://arxiv.org/pdf/1711.11577v1
748,205,Spotlight,Neural Baby Talk,"Jiasen Lu, Georgia Institute of Technology; Jianwei Yang, Georgia Tech; Dhruv Batra, Georgia Tech; Devi Parikh, Georgia Tech",Neural Baby Talk,"We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk",http://arxiv.org/pdf/1803.09845v1
749,242,Spotlight,Few-Shot Image Recognition by Predicting Parameters from Activations,"Siyuan Qiao, Johns Hopkins University; Chenxi Liu, JHU; Wei Shen, Shanghai University; Alan Yuille,",Few-Shot Image Recognition by Predicting Parameters from Activations,"In this paper, we are interested in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e.g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods.",http://arxiv.org/pdf/1706.03466v3
750,458,Spotlight,Iterative Visual Reasoning Beyond Convolutions,"Xinlei Chen, Facebook; Li-jia Li, Google Inc; Fei-Fei Li, Google Inc.; Abhinav Gupta,",Iterative Visual Reasoning Beyond Convolutions,"We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs with parallel updates; and a global graph-reasoning module. Our graph module has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to classes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, \eg achieving an $8.4\%$ absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning.",http://arxiv.org/pdf/1803.11189v1
751,732,Spotlight,Visual Question Reasoning on General Dependency Tree,"Qingxing Cao, Sun Yat-Sen University; Xiaodan Liang, Carnegie Mellon University; Bailin Li, SUN-YAT SEN UNIVERSITY; Liang Lin,",Visual Question Reasoning on General Dependency Tree,"The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.",http://arxiv.org/pdf/1804.00105v1
752,809,Spotlight,CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization,"Sixing Hu, NUS; Mengdan Feng, NUS; Rang Nguyen, National Uni. of Singapore; Gim Hee Lee, National University of SIngapore",,,
753,812,Spotlight,Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation,"Yunchao Wei, ; Huaxin Xiao, ; Honghui Shi, UIUC; Zequn Jie, ; Jiashi Feng, ; Thomas Huang,",,,
754,1356,Spotlight,Low-shot Learning from Imaginary Data,"Yu-Xiong Wang, Carnegie Mellon University; Ross Girshick, ; Martial Hebert, ; Bharath Hariharan, Cornell University",Low-Shot Learning from Imaginary Data,"Humans can quickly learn new visual concepts, perhaps because they can easily visualize or imagine what novel objects look like from different views. Incorporating this ability to hallucinate novel instances of new concepts might help machine vision systems perform better low-shot learning, i.e., learning concepts from few examples. We present a novel approach to low-shot learning that uses this idea. Our approach builds on recent progress in meta-learning (""learning to learn"") by combining a meta-learner with a ""hallucinator"" that produces additional training examples, and optimizing both models jointly. Our hallucinator can be incorporated into a variety of meta-learners and provides significant gains: up to a 6 point boost in classification accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classification benchmark.",http://arxiv.org/pdf/1801.05401v2
755,1321,Oral,DoubleFusion: Real-time Capture of Human Performance with Inner Body Shape from a Single Depth Sensor,"Tao Yu, Beihang University; Zerong Zheng, Tsinghua University; Kaiwen Guo, Google; Jianhui Zhao, Beihang University; Qionghai Dai, ; Hao Li, ; Gerard Pons-Moll, Max Planck for Informatics; Yebin Liu, Tsinghua University",,,
756,12,Oral,DensePose: Multi-Person Dense Human Pose Estimation In The Wild,"Alp Guler, INRIA; Natalia Neverova, Facebook AI Research; Iasonas Kokkinos, FAIR/UCL",,,
757,3718,Oral,Ordinal Depth Supervision for 3D Human Pose Estimation,"Georgios Pavlakos, ; Xiaowei Zhou, Zhejiang University; Kostas Daniilidis, University of Pennsylvania",,,
758,374,Spotlight,Consensus Maximization for Semantic Region Correspondences,"Pablo Speciale, ETH; Danda Paudel, ; Martin Oswald, ETH Zurich; Hayko Riemenschneider, Computer Vision Lab, ETH Zurich; Luc Van Gool, KTH; Marc Pollefeys, ETH",,,
759,3232,Spotlight,Robust Hough Transform Based 3D Reconstruction from Circular Light Fields,"Alessandro Vianello, Robert Bosch GmbH; Jens Ackermann, Robert Bosch GmbH; Maximilian Diebold, Heidelberg University; Bernd Jähne, University of Heidelberg",,,
760,2464,Spotlight,Alive Caricature from 2D to 3D,"Qianyi Wu, USTC; Juyong Zhang, University of Science and Technology of China; Yu-Kun Lai, Cardiff University; Jianmin Zheng, Nanyang Technological University; Jianfei Cai,",Alive Caricature from 2D to 3D,"Caricature is an art form that expresses subjects in abstract, simple and exaggerated view. While many caricatures are 2D images, this paper presents an algorithm for creating expressive 3D caricatures from 2D caricature images with a minimum of user interaction. The key idea of our approach is to introduce an intrinsic deformation representation that has a capacity of extrapolation enabling us to create a deformation space from standard face dataset, which maintains face constraints and meanwhile is sufficiently large for producing exaggerated face models. Built upon the proposed deformation representation, an optimization model is formulated to find the 3D caricature that captures the style of the 2D caricature image automatically. The experiments show that our approach has better capability in expressing caricatures than those fitting approaches directly using classical parametric face models such as 3DMM and FaceWareHouse. Moreover, our approach is based on standard face datasets and avoids constructing complicated 3D caricature training set, which provides great flexibility in real applications.",http://arxiv.org/pdf/1803.06802v3
761,203,Spotlight,Nonlinear 3D Face Morphable Model,"LUAN TRAN, Michigan State University; Xiaoming Liu, Michigan State University",What does 2D geometric information really tell us about 3D face shape?,"A face image contains geometric cues in the form of configurational information and contours that can be used to estimate 3D face shape. While it is clear that 3D reconstruction from 2D points is highly ambiguous if no further constraints are enforced, one might expect that the face-space constraint solves this problem. We show that this is not the case and that geometric information is an ambiguous cue. There are two sources for this ambiguity. The first is that, within the space of 3D face shapes, there are flexibility modes that remain when some parts of the face are fixed. The second occurs only under perspective projection and is a result of perspective transformation as camera distance varies. Two different faces, when viewed at different distances, can give rise to the same 2D geometry. To demonstrate these ambiguities, we develop new algorithms for fitting a 3D morphable model to 2D landmarks or contours under either orthographic or perspective projection and show how to compute flexibility modes for both cases. We show that both fitting problems can be posed as a separable nonlinear least squares problem and solved efficiently. We provide quantitative and qualitative evidence that the ambiguity exists in synthetic data and real images.",http://arxiv.org/pdf/1708.06703v1
762,2406,Spotlight,Through-Wall Human Pose Estimation Using Radio Signals,"Mingmin Zhao, MIT; Tianhong Li, MIT; Mohammad Abu Alsheikh, MIT; Yonglong Tian, Massachusetts Institute of Technology; Hang Zhao, MIT; Antonio Torralba, MIT; Dina Katabi, MIT",,,
763,765,Spotlight,What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets,"De-An Huang, Stanford University; Vignesh Ramanathan, Facebook; Dhruv Mahajan, ; Juan Carlos Niebles, Stanford University; Fei-Fei Li, Stanford University; Lorenzo Torresani, Darthmout College, USA; Manohar Paluri,",,,
764,1029,Spotlight,Fast Video Object Segmentation by Reference-Guided Mask Propagation,"Seoung Wug Oh, Yonsei Univeristy; Joon-Young Lee, ; Kalyan Sunkavalli, Adobe Systems Inc.; Seon Joo Kim, Yonsei University",,,
765,1203,Spotlight,NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning,"Alexander Richard, University of Bonn; Hilde Kuehne, University of Bonn; Ahsan Iqbal, University of Bonn; Juergen Gall, University of Bonn, Germany",,,
766,1926,Spotlight,Actor and Observer: Joint Modeling of First and Third-Person Videos,"Gunnar Sigurdsson, CMU; Cordelia Schmid, INRIA Grenoble, France; Ali Farhadi, ; Abhinav Gupta, ; Karteek Alahari,",,,
767,2266,Spotlight,HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization,"Bin Zhao, Northwestern Polytechnical Uni; Xuelong Li, ; Xiaoqiang Lu,",,,
768,423,Spotlight,Fast and Accurate Online Video Object Segmentation via Tracking Parts,"Jingchun Cheng, Tsinghua University; Yi-Hsuan Tsai, NEC Labs America; Wei-Chih Hung, University of California, Merced; Shengjin Wang, ; Ming-Hsuan Yang, UC Merced",,,
769,3330,Spotlight,Now You Shake Me: Towards Automatic 4D Cinema,"Yuhao Zhou, University of Toronto; Makarand Tapaswi, University of Toronto; Sanja Fidler,",,,
770,2161,Spotlight,Viewpoint-aware Video Summarization,"Atsushi Kanehira, University of Tokyo; Luc Van Gool, KTH; Yoshitaka Ushiku, ; Tatsuya Harada, University of Tokyo",A Survey on Content-Aware Video Analysis for Sports,"Sports data analysis is becoming increasingly large-scale, diversified, and shared, but difficulty persists in rapidly accessing the most crucial information. Previous surveys have focused on the methodologies of sports video analysis from the spatiotemporal viewpoint instead of a content-based viewpoint, and few of these studies have considered semantics. This study develops a deeper interpretation of content-aware sports video analysis by examining the insight offered by research into the structure of content under different scenarios. On the basis of this insight, we provide an overview of the themes particularly relevant to the research on content-aware systems for broadcast sports. Specifically, we focus on the video content analysis techniques applied in sportscasts over the past decade from the perspectives of fundamentals and general review, a content hierarchical model, and trends and challenges. Content-aware analysis methods are discussed with respect to object-, event-, and context-oriented groups. In each group, the gap between sensation and content excitement must be bridged using proper strategies. In this regard, a content-aware approach is required to determine user demands. Finally, the paper summarizes the future trends and challenges for sports video analysis. We believe that our findings can advance the field of research on content-aware video analysis for broadcast sports.",http://arxiv.org/pdf/1703.01170v1
771,2871,Oral,Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter,"Yuki Fujimura, Kyoto University; Masaaki Iiyama, Kyoto University; Atsushi Hashimoto, Kyoto University; Michihiko Minoh, Kyoto University",Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter,"Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.",http://arxiv.org/pdf/1804.02836v1
772,2194,Oral,Direction-aware Spatial Context Features for Shadow Detection,"Xiaowei Hu, CUHK; Lei Zhu, ; Chi-Wing Fu, ; Jing Qin, The Hong Kong Polytechnic University; Pheng-Ann Heng,",Direction-aware Spatial Context Features for Shadow Detection,"Shadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows. This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows. This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels. Moreover, a weighted cross entropy loss is designed to make the training more effective. We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network. Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate.",http://arxiv.org/pdf/1712.04142v1
773,2812,Oral,Discriminative Learning of Latent Features for Zero-Shot Recognition,"Yan Li, CASIA; Junge Zhang, ; jianguo Zhang, ; Kaiqi Huang,",Discriminative Learning of Latent Features for Zero-Shot Recognition,"Zero-shot learning (ZSL) aims to recognize unseen image categories by learning an embedding space between image and semantic representations. For years, among existing works, it has been the center task to learn the proper mapping matrices aligning the visual and semantic space, whilst the importance to learn discriminative representations for ZSL is ignored. In this work, we retrospect existing methods and demonstrate the necessity to learn discriminative representations for both visual and semantic instances of ZSL. We propose an end-to-end network that is capable of 1) automatically discovering discriminative regions by a zoom network; and 2) learning discriminative semantic representations in an augmented space introduced for both user-defined and latent attributes. Our proposed method is tested extensively on two challenging ZSL datasets, and the experiment results show that the proposed method significantly outperforms state-of-the-art methods.",http://arxiv.org/pdf/1803.06731v1
774,1435,Spotlight,Learning to Adapt Structured Output Space for Semantic Segmentation,"Yi-Hsuan Tsai, NEC Labs America; Wei-Chih Hung, University of California, Merced; Samuel Schulter, NEC Labs; Kihyuk Sohn, NEC Laboratories America; Ming-Hsuan Yang, UC Merced; Manmohan Chandraker, NEC Labs America",Learning to Adapt Structured Output Space for Semantic Segmentation,"Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.",http://arxiv.org/pdf/1802.10349v1
775,3022,Spotlight,Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics,"Alex Kendall, ; Yarin Gal, University of Cambridge; Roberto Cipolla,",Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics,"Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.",http://arxiv.org/pdf/1705.07115v1
776,3807,Spotlight,Jointly Localizing and Describing Events for Dense Video Captioning,"Yehao Li, Sun Yat-Sen University; Ting Yao, Microsoft Research Asia; Yingwei Pan, University of Science and Technology of China; Hongyang Chao, Sun Yat-sen University; Tao Mei, Microsoft Research Asia",Dense-Captioning Events in Videos,"Most natural videos contain numerous events. For example, in a video of a ""man playing a piano"", the video might also contain ""another man dancing"" or ""a crowd clapping"". We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with it's unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.",http://arxiv.org/pdf/1705.00754v1
777,1928,Spotlight,Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push,"Siavash Gorji, McGill University; James Clark, McGill University",,,
778,442,Spotlight,M3: Multimodal Memory Modelling for Video Captioning,"Junbo Wang, Institute of Automation, Chine; Wei Wang, ; Yan Huang, ; Liang Wang, unknown; Tieniu Tan, NLPR China",Multimodal Memory Modelling for Video Captioning,"Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, e.g., convolutional neural networks (CNNs) and recurrent neural networks (RNNs), video captioning has made great progress. However, learning an effective mapping from visual sequence space to language space is still a challenging problem. In this paper, we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide global visual attention on described targets. Specifically, the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. First, text representation in the Long Short-Term Memory (LSTM) based text decoder is written into the memory, and the memory contents will be read out to guide an attention to select related visual targets. Then, the selected visual information is written into the memory, which will be further read out to the text decoder. To evaluate the proposed model, we perform experiments on two publicly benchmark datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms the state-of-theart methods in terms of BLEU and METEOR.",http://arxiv.org/pdf/1611.05592v1
779,2361,Spotlight,Emotional Attention: A Study of Image Sentiment and Visual Attention,"Shaojing Fan, National University of Singapo; Zhiqi Shen, National University of Singapore; Ming Jiang, University of Minnesota; Bryan Koenig, Southern Utah University; Juan Xu, University of Minnesota; Mohan Kankanhalli, National University of Singapore; Qi Zhao,",When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment,"Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indoor-outdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation.",http://arxiv.org/pdf/1611.04636v1
780,3791,Spotlight,"A Low Power, High Throughput, Fully Event-Based Stereo System","Alexander Andreopoulos, IBM Research; Hirak Kashyap, UC Irvine and IBM; Tapan  Nayak, IBM; Arnon Amir, IBM; Myron Flickner, IBM",,,
781,406,Spotlight,VITON: An Image-based Virtual Try-on Network,"Xintong Han, University of Maryland; Zuxuan Wu, University of Maryland; Zhe Wu, University of Maryland; Ruichi Yu, ; Larry Davis, University of Maryland, USA",VITON: An Image-based Virtual Try-on Network,"We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected Zalando dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models.",http://arxiv.org/pdf/1711.08447v2
782,1688,Spotlight,Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation,"Pengyuan Lyu, Huazhong University of Science and Technology; Cong Yao, Huazhong University of Science and Technology; Wenhao Wu, Megvii; Shuicheng Yan, National University of Singapore; Xiang Bai, Huazhong University of Science and Technology",Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation,"Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn't need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84.3% on ICDAR2015 and 81.5% on MSRA-TD500.",http://arxiv.org/pdf/1802.08948v2
783,2342,Spotlight,Multi-Content GAN for Few-Shot Font Style Transfer,"Samaneh Azadi, UC Berkeley; Matthew Fisher, Adobe; Vladimir Kim, Adobe Research; Zhaowen Wang, Adobe; Eli Shechtman, Adobe Research; Trevor Darrell, UC Berkeley, USA",Multi-Content GAN for Few-Shot Font Style Transfer,"In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.",http://arxiv.org/pdf/1712.00516v1
784,298,Spotlight,Audio to Body Dynamics,"Eli Shlizerman, Facebook; Lucio Dery, Stanford; Hayden Schoen, Facebook; Ira Kemelmacher,",A Neurorobotic Experiment for Crossmodal Conflict Resolution in Complex Environments,"Crossmodal conflict resolution is a crucial component of robot sensorimotor coupling through interaction with the environment for swift and robust behaviour also in noisy conditions. In this paper, we propose a neurorobotic experiment in which an iCub robot exhibits human-like responses in a complex crossmodal environment. To better understand how humans deal with multisensory conflicts, we conducted a behavioural study exposing 33 subjects to congruent and incongruent dynamic audio-visual cues. In contrast to previous studies using simplified stimuli, we designed a scenario with four animated avatars and observed that the magnitude and extension of the visual bias are related to the semantics embedded in the scene, i.e., visual cues that are congruent with environmental statistics (moving lips and vocalization) induce a stronger bias. We propose a deep learning model that processes stereophonic sound, facial features, and body motion to trigger a discrete response resembling the collected behavioural data. After training, we exposed the iCub to the same experimental conditions as the human subjects, showing that the robot can replicate similar responses in real time. Our interdisciplinary work provides important insights into how crossmodal conflict resolution can be modelled in robots and introduces future research directions for the efficient combination of sensory drive with internally generated knowledge and expectations.",http://arxiv.org/pdf/1802.10408v1
785,2108,Spotlight,Weakly Supervised Coupled Networks for Visual Sentiment Analysis,"Jufeng Yang, Nankai University; Dongyu She, ; Yu-Kun Lai, Cardiff University; Paul Rosin, ; Ming-Hsuan Yang, UC Merced",,,
786,2895,Spotlight,Future Person Localization in First-Person Videos,"Takuma Yagi, The University of Tokyo; Karttikeya Mangalam, IIT Kanpur; Ryo Yonetani, The University of Tokyo; Yoichi Sato, Univ of Tokyo",Future Person Localization in First-Person Videos,"We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict that person's location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scales of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g., where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset.",http://arxiv.org/pdf/1711.11217v2
787,2578,Poster,Preserving Semantic Relations for Zero-Shot Learning,"Yashas Annadani, NITK; Soma Biswas, Indian Institute of Science",Preserving Semantic Relations for Zero-Shot Learning,"Zero-shot learning has gained popularity due to its potential to scale recognition models without requiring additional training data. This is usually achieved by associating categories with their semantic information like attributes. However, we believe that the potential offered by this paradigm is not yet fully exploited. In this work, we propose to utilize the structure of the space spanned by the attributes using a set of relations. We devise objective functions to preserve these relations in the embedding space, thereby inducing semanticity to the embedding space. Through extensive experimental evaluation on five benchmark datasets, we demonstrate that inducing semanticity to the embedding space is beneficial for zero-shot learning. The proposed approach outperforms the state-of-the-art on the standard zero-shot setting as well as the more realistic generalized zero-shot setting. We also demonstrate how the proposed approach can be useful for making approximate semantic inferences about an image belonging to a category for which attribute information is not available.",http://arxiv.org/pdf/1803.03049v1
788,2619,Poster,Show Me a Story: Towards Coherent Neural Story Illustration,"Hareesh Ravi, Rutgers University; Lezi Wang, Rutgers; Carlos Muniz, Rutgers University; Leonid Sigal, University of British Columbia; Mubbasir Kapadia, Rutgers University",,,
789,2662,Poster,Reconstruction Network for Video Captioning,"Bairui Wang, ; Lin Ma, Tencent AI Lab; Wei Zhang, ; Wei Liu,",Reconstruction Network for Video Captioning,"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) with a novel encoder-decoder-reconstructor architecture, which leverages both the forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder makes use of the forward flow to produce the sentence description based on the encoded video semantic features. Two types of reconstructors are customized to employ the backward flow and reproduce the video features based on the hidden state sequence generated by the decoder. The generation loss yielded by the encoder-decoder and the reconstruction loss introduced by the reconstructor are jointly drawn into training the proposed RecNet in an end-to-end fashion. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the encoder-decoder models and leads to significant gains in video caption accuracy.",http://arxiv.org/pdf/1803.11438v1
790,2778,Poster,Fast Spectral Ranking for Similarity Search,"Ahmet Iscen, Inria; Yannis Avrithis, Inria; Giorgos Tolias, Czech Technical University in Prague; Teddy Furon, ; Ondrej Chum, Czech Technical University in Prague",Fast Spectral Ranking for Similarity Search,"Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline. This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search, we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search.",http://arxiv.org/pdf/1703.06935v3
791,2779,Poster,Mining on Manifolds: Metric Learning without Labels,"Ahmet Iscen, Inria; Giorgos Tolias, Czech Technical University in Prague; Yannis Avrithis, Inria; Ondrej Chum, Czech Technical University in Prague",Mining on Manifolds: Metric Learning without Labels,"In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss. The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.",http://arxiv.org/pdf/1803.11095v1
792,3012,Poster,Pixar: Real-time 3D Object Detection from Point Clouds,"Bin Yang, Uber ATG, UofT; Wenjie Luo, Uber ATG.; UofT; Raquel Urtasun, University of Toronto",,,
793,3180,Poster,Leveraging Unlabeled Data for Crowd Counting by Learning to Rank,"Xialei Liu, Computer Vision Center of UAB; Joost van de Weijer, Computer Vision Center Barcelona; Andrew Bagdanov, Computer Vision Center, Barcelona",Leveraging Unlabeled Data for Crowd Counting by Learning to Rank,"We propose a novel crowd counting approach that leverages abundantly available unlabeled crowd imagery in a learning-to-rank framework. To induce a ranking of cropped images , we use the observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the super-image. This allows us to address the problem of limited size of existing datasets for crowd counting. We collect two crowd scene datasets from Google using keyword searches and query-by-example image retrieval, respectively. We demonstrate how to efficiently learn from these unlabeled datasets by incorporating learning-to-rank in a multi-task network which simultaneously ranks images and estimates crowd density maps. Experiments on two of the most challenging crowd counting datasets show that our approach obtains state-of-the-art results.",http://arxiv.org/pdf/1803.03095v1
794,3242,Poster,Zero-Shot Kernel Learning.,"Hongguang Zhang, Data61; Piotr Koniusz, Data61/CSIRO",A Comparative Study of Pairwise Learning Methods based on Kernel Ridge Regression,"Many machine learning problems can be formulated as predicting labels for a pair of objects. Problems of that kind are often referred to as pairwise learning, dyadic prediction or network inference problems. During the last decade kernel methods have played a dominant role in pairwise learning. They still obtain a state-of-the-art predictive performance, but a theoretical analysis of their behavior has been underexplored in the machine learning literature.   In this work we review and unify existing kernel-based algorithms that are commonly used in different pairwise learning settings, ranging from matrix filtering to zero-shot learning. To this end, we focus on closed-form efficient instantiations of Kronecker kernel ridge regression. We show that independent task kernel ridge regression, two-step kernel ridge regression and a linear matrix filter arise naturally as a special case of Kronecker kernel ridge regression, implying that all these methods implicitly minimize a squared loss. In addition, we analyze universality, consistency and spectral filtering properties. Our theoretical results provide valuable insights in assessing the advantages and limitations of existing pairwise learning methods.",http://arxiv.org/pdf/1803.01575v1
795,3451,Poster,Differential Attention for Visual Question Answering,"Badri Patro, IIT Kanpur; Vinay P. Namboodiri, Indian Institute of Technology Kanpur",Differential Attention for Visual Question Answering,"In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions.",http://arxiv.org/pdf/1804.00298v2
796,3822,Poster,Learning from Noisy Web Data with Category-level Supervision,"Li Niu, Rice University; Qingtao Tang, ; Ashok Veeraraghavan, Rice University; Ashutosh  Sabharwal,",Learning from Noisy Web Data with Category-level Supervision,"As tons of photos are being uploaded to public websites (e.g., Flickr, Bing, and Google) every day, learning from web data has become an increasingly popular research direction because of freely available web resources, which is also referred to as webly supervised learning. Nevertheless, the performance gap between webly supervised learning and traditional supervised learning is still very large, owning to the label noise of web data as well as the domain shift between web data and test data. To be exact, on one hand, the labels of images crawled from public websites are very noisy and often inaccurate. On the other hand, the data distributions between web data and test data are considerably different, which is known as domain shift. Some existing works tend to facilitate learning from web data with the aid of extra information, such as augmenting or purifying web data by virtue of instance-level supervision, which is usually in demand of heavy manual annotation. Instead, we propose to tackle the label noise and domain shift by leveraging more accessible category-level supervision. In particular, we build our method upon variational autoencoder (VAE), in which the classification network is attached on the hidden layer of VAE in a way that the classification network and VAE can jointly leverage the category-level hybrid semantic information. Moreover, we further extend our method to cope with the domain shift by utilizing unlabeled test instances in the training stage followed by low-rank refinement. The effectiveness of our proposed methods is clearly demonstrated by extensive experiments on three benchmark datasets.",http://arxiv.org/pdf/1803.03857v2
797,3855,Poster,Toward Driving Scene Understanding: A Dataset for Learning  Driver Behavior and Causal Reasoning,"Vasili Ramanishka, Boston University; Yi-Ting Chen, Honda Research Institute USA; Teruhisa Misu, Honda Research Institute; Kate Saenko,",,,
798,3878,Poster,Learning Attribute Representations with Localization for Flexible Fashion Search,"Kenan Ak, National University of Singapo; Joo Hwee Lim, I2R, Astar; Ashraf Kassim, ; JO YEW THAM,",,,
799,3979,Poster,Bidirecional Retrieval Made Simple,"Jônatas Wehrmann, PUCRS; Rodrigo Barros, PUCRS",,,
800,4014,Poster,Learning Multi-Instance Enriched Image Representation via Non-Greedy Simultaneous L1 -Norm Minimization and Maximization,"Hua Wang, Colorado School of Mines",,,
801,4255,Poster,Learning Visual Knowledge Memory Networks for Visual Question Answering,"Zhou Su, ; Jianguo Li, Intel Lab; Zhiqiang Shen, Fudan University; Yurong Chen,",Training Recurrent Answering Units with Joint Loss Minimization for VQA,"We propose a novel algorithm for visual question answering based on a recurrent deep neural network, where every module in the network corresponds to a complete answering unit with attention mechanism by itself. The network is optimized by minimizing loss aggregated from all the units, which share model parameters while receiving different information to compute attention probability. For training, our model attends to a region within image feature map, updates its memory based on the question and attended image feature, and answers the question based on its memory state. This procedure is performed to compute loss in each step. The motivation of this approach is our observation that multi-step inferences are often required to answer questions while each problem may have a unique desirable number of steps, which is difficult to identify in practice. Hence, we always make the first unit in the network solve problems, but allow it to learn the knowledge from the rest of units by backpropagation unless it degrades the model. To implement this idea, we early-stop training each unit as soon as it starts to overfit. Note that, since more complex models tend to overfit on easier questions quickly, the last answering unit in the unfolded recurrent neural network is typically killed first while the first one remains last. We make a single-step prediction for a new question using the shared model. This strategy works better than the other options within our framework since the selected model is trained effectively from all units without overfitting. The proposed algorithm outperforms other multi-step attention based approaches using a single step prediction in VQA dataset.",http://arxiv.org/pdf/1606.03647v2
802,186,Poster,Visual Grounding via Accumulated Attention,"chaorui Deng, ; Qi Wu, University of Adelaide; Fuyuan Hu, ; Fan Lyu, Suzhou University of Science and Technology; Mingkui Tan, South China University of Technology; Qingyao Wu, School of Software Engineering, South China University of Technology",,,
803,260,Poster,Beyond Trade-off: Accelerate FCN-based Face Detection with Higher Accuracy,"Guanglu Song, Beihang University; Yu Liu, CUHK; Ming Jiang, BUAA; Yujie Wang, Beihang university",,,
804,985,Poster,PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning,"Arun Mallya, UIUC; Lana Lazebnik,",PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning,"This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially ""pack"" multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNet-trained VGG-16 network and achieve accuracies close to those of separately trained networks for each task.",http://arxiv.org/pdf/1711.05769v1
805,1247,Poster,Repulsion Loss: Detecting Pedestrians in a Crowd,"Xinlong Wang, Tongji University; Tete Xiao, Peking University; Yuning Jiang, Megvii inc.; Shuai Shao, Megvii; Jian Sun, ; Chunhua Shen, University of Adelaide",Repulsion Loss: Detecting Pedestrians in a Crowd,"Detecting individual pedestrians in a crowd remains a challenging problem since the pedestrians often gather together and occlude each other in real-world scenarios. In this paper, we first explore how a state-of-the-art pedestrian detector is harmed by crowd occlusion via experimentation, providing insights into the crowd occlusion problem. Then, we propose a novel bounding box regression loss specifically designed for crowd scenes, termed repulsion loss. This loss is driven by two motivations: the attraction by target, and the repulsion by other surrounding objects. The repulsion term prevents the proposal from shifting to surrounding objects thus leading to more crowd-robust localization. Our detector trained by repulsion loss outperforms all the state-of-the-art methods with a significant improvement in occlusion cases.",http://arxiv.org/pdf/1711.07752v2
806,1910,Poster,Neural Sign Language Translation,"Necati Cihan Camgoz, CVSSP; Simon Hadfield, ; Richard Bowden, University of Surrey UK; Oscar Koller, ; Hermann Ney,",DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level Sign Language Translation,"There is an undeniable communication barrier between deaf people and people with normal hearing ability. Although innovations in sign language translation technology aim to tear down this communication barrier, the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions. Moreover, these existing systems can only perform single-sign ASL translation rather than sentence-level translation, making them much less useful in daily-life communication scenarios. In this work, we fill this critical gap by presenting DeepASL, a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive American Sign Language (ASL) translation at both word and sentence levels. DeepASL uses infrared light as its sensing mechanism to non-intrusively capture the ASL signs. It incorporates a novel hierarchical bidirectional deep recurrent neural network (HB-RNN) and a probabilistic framework based on Connectionist Temporal Classification (CTC) for word-level and sentence-level ASL translation respectively. To evaluate its performance, we have collected 7,306 samples from 11 participants, covering 56 commonly used ASL words and 100 ASL sentences. DeepASL achieves an average 94.5% word-level translation accuracy and an average 8.2% word error rate on translating unseen ASL sentences. Given its promising performance, we believe DeepASL represents a significant step towards breaking the communication barrier between deaf people and hearing majority, and thus has the significant potential to fundamentally change deaf people's lives.",http://arxiv.org/pdf/1802.07584v2
807,739,Poster,Non-local Neural Networks,"Xiaolong Wang, Carnegie Mellon University; Ross Girshick, ; Abhinav Gupta, ; Kaiming He,",A Non-Technical Survey on Deep Convolutional Neural Network Architectures,"Artificial neural networks have recently shown great results in many disciplines and a variety of applications, including natural language understanding, speech processing, games and image data generation. One particular application in which the strong performance of artificial neural networks was demonstrated is the recognition of objects in images, where deep convolutional neural networks are commonly applied. In this survey, we give a comprehensive introduction to this topic (object recognition with deep convolutional neural networks), with a strong focus on the evolution of network architectures. Therefore, we aim to compress the most important concepts in this field in a simple and non-technical manner to allow for future researchers to have a quick general understanding.   This work is structured as follows:   1. We will explain the basic ideas of (convolutional) neural networks and deep learning and examine their usage for three object recognition tasks: image classification, object localization and object detection.   2. We give a review on the evolution of deep convolutional neural networks by providing an extensive overview of the most important network architectures presented in chronological order of their appearances.",http://arxiv.org/pdf/1803.02129v1
808,1517,Poster,LAMV: Learning to align and match videos with kernelized temporal layers,"Lorenzo Baraldi, University of Modena; Matthijs Douze, ; Rita Cucchiara, ; Herve Jegou, Facebook AI Research",,,
809,633,Poster,Optimizing Video Object Detection via a Scale-Time Lattice,"Kai Chen, CUHK; Jiaqi Wang, CUHK; Shuo Yang, ; Xingcheng Zhang, CUHK; Yuanjun Xiong, Amazon ; Chen-Change Loy, the Chinese University of Hong Kong; Dahua Lin, CUHK",,,
810,2839,Poster,Learning Compressible 360°  Video Isomers,"Yu-Chuan Su, UT Austin; Kristen Grauman,",,,
811,2195,Poster,Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification,"Xiang Long, Tsinghua University; Chuang Gan, Tsinghua University; Gerard De Melo, Rutgers University; Jiajun Wu, MIT; Xiao Liu, ; Shilei Wen, Baidu Research",Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification,"Recently, substantial research effort has focused on how to apply CNNs or RNNs to better extract temporal patterns from videos, so as to improve the accuracy of video classification. In this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common video classification datasets. We investigate the potential of a purely attention based local feature integration. Accounting for the characteristics of such features in video classification, we propose a local feature integration framework based on attention clusters, and introduce a shifting operation to capture more diverse signals. We carefully analyze and compare the effect of different attention mechanisms, cluster sizes, and the use of the shifting operation, and also investigate the combination of attention clusters for multimodal integration. We demonstrate the effectiveness of our framework on three real-world video classification datasets. Our model achieves competitive results across all of these. In particular, on the large-scale Kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% in terms of the top-1 and 94.0% in terms of the top-5 accuracy on the validation set. The attention clusters are the backbone of our winner solution at ActivityNet Kinetics Challenge 2017. Code and models will be released soon.",http://arxiv.org/pdf/1711.09550v1
812,2582,Poster,What have we learned from deep representations for action recognition?,"Christoph Feichtenhofer, ; Axel Pinz, Graz University of Technology; Richard Wildes, York University; Andrew Zisserman, Oxford",,,
813,1575,Poster,Controllable Video Generation with Sparse Trajectories,"Zekun Hao, ; Xun Huang, ; Serge Belongie,",,,
814,3448,Poster,Representing and Learning High Dimensional Data with the Optimal Transport Map from a Probabilistic Viewpoint,"Serim Park, Oath; Matthew Thorpe,",,,
815,3711,Poster,CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization,"Frederick Tung, Simon Fraser University; Greg Mori,",,,
816,3713,Poster,Inference in Higher Order MRF-MAP Problems with Small and Large Cliques,"Ishant Shanu, Iiit delhi; Chetan Arora, Indraprastha Institute of Information Technology Delhi; S.N. Maheshwari, IIT Delhi",,,
817,3923,Poster,ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes,"Yuhua Chen, CVL@ETHZ; Wen Li, ETH; Luc Van Gool, KTH",ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes,"Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.",http://arxiv.org/pdf/1711.11556v2
818,4254,Poster,Eye In-Painting with Exemplar Generative Adversarial Networks,"Brian Dolhansky, Facebook; Cristian Canton Ferrer, Facebook",Eye In-Painting with Exemplar Generative Adversarial Networks,"This paper introduces a novel approach to in-painting where the identity of the object to remove or change is preserved and accounted for at inference time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize exemplar information to produce high-quality, personalized in painting results. We propose using exemplar information in the form of a reference image of the region to in-paint, or a perceptual code describing that object. Unlike previous conditional GAN formulations, this extra information can be inserted at multiple points within the adversarial network, thus increasing its descriptive power. We show that ExGANs can produce photo-realistic personalized in-painting results that are both perceptually and semantically plausible by applying them to the task of closed to-open eye in-painting in natural pictures. A new benchmark dataset is also introduced for the task of eye in-painting for future comparisons.",http://arxiv.org/pdf/1712.03999v1
819,4296,Poster,clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions,"Dongqing Zhang, ImaginationAI LLC",clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions,"Depthwise convolution and grouped convolution has been successfully applied to improve the efficiency of convolutional neural network (CNN). We suggest that these models can be considered as special cases of a generalized convolution operation, named channel local convolution(CLC), where an output channel is computed using a subset of the input channels. This definition entails computation dependency relations between input and output channels, which can be represented by a channel dependency graph(CDG). By modifying the CDG of grouped convolution, a new CLC kernel named interlaced grouped convolution (IGC) is created. Stacking IGC and GC kernels results in a convolution block (named CLC Block) for approximating regular convolution. By resorting to the CDG as an analysis tool, we derive the rule for setting the meta-parameters of IGC and GC and the framework for minimizing the computational cost. A new CNN model named clcNet is then constructed using CLC blocks, which shows significantly higher computational efficiency and fewer parameters compared to state-of-the-art networks, when being tested using the ImageNet-1K dataset. Source code is available at https://github.com/dqzhang17/clcnet.torch .",http://arxiv.org/pdf/1712.06145v3
820,232,Poster,Towards Effective Low-bitwidth Convolutional Neural Networks,"Bohan Zhuang, The University of Adelaide; Chunhua Shen, University of Adelaide; Mingkui Tan, South China University of Technology; Lingqiao Liu, University of Adelaide; Ian Reid,",Towards Effective Low-bitwidth Convolutional Neural Networks,"This paper tackles the problem of training a deep convolutional neural network with both low-precision weights and low-bitwidth activations. Optimizing a low-precision network is very challenging since the training process can easily get trapped in a poor local minima, which results in substantial accuracy loss. To mitigate this problem, we propose three simple-yet-effective approaches to improve the network training. First, we propose to use a two-stage optimization strategy to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and then quantized activations. This is in contrast to the traditional methods which optimize them simultaneously. Second, following a similar spirit of the first method, we propose another progressive optimization approach which progressively decreases the bit-width from high-precision to low-precision during the course of training. Third, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training. Extensive experiments on various datasets ( i.e., CIFAR-100 and ImageNet) show the effectiveness of the proposed methods. To highlight, using our methods to train a 4-bit precision network leads to no performance decrease in comparison with its full-precision counterpart with standard network architectures ( i.e., AlexNet and ResNet-50).",http://arxiv.org/pdf/1711.00205v2
821,419,Poster,Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks,"Jason Kuen, NTU, Singapore; Xiangfei Kong, Nanyang Technological University; Zhe Lin, Adobe Systems, Inc.; Gang Wang, ; Jianxiong Yin, NVIDIA; Simon See, NVIDIA; Yap-Peng Tan,",Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks,"It is desirable to train convolutional networks (CNNs) to run more efficiently during inference. In many cases however, the computational budget that the system has for inference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability. Thus, it is inadequate to train just inference-efficient CNNs, whose inference costs are not adjustable and cannot adapt to varied inference budgets. We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint). During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random downsampling ratio. The different stochastic downsampling configurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss. Sharing network parameters across different instances provides significant regularization boost. During inference, one may handpick a SDPoint instance that best fits the inference budget. The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classification.",http://arxiv.org/pdf/1801.09335v1
822,430,Poster,Face Aging with Identity-Preserved Conditional Generative Adversarial Networks,"Zongwei WANG, ; Xu Tang, ; Weixin Luo, Shanghaitech University; Shenghua Gao, ShanghaiTech University",Facial Aging and Rejuvenation by Conditional Multi-Adversarial Autoencoder with Ordinal Regression,"Facial aging and facial rejuvenation analyze a given face photograph to predict a future look or estimate a past look of the person. To achieve this, it is critical to preserve human identity and the corresponding aging progression and regression with high accuracy. However, existing methods cannot simultaneously handle these two objectives well. We propose a novel generative adversarial network based approach, named the Conditional Multi-Adversarial AutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation technique to control the aging accuracy and takes a high-level feature representation to preserve personalized identity. Specifically, the face is first mapped to a latent vector through a convolutional encoder. The latent vector is then projected onto the face manifold conditional on the age through a deconvolutional generator. The latent vector preserves personalized face features and the age controls facial aging and rejuvenation. A discriminator and an ordinal regression are imposed on the encoder and the generator in tandem, making the generated face images to be more photorealistic while simultaneously exhibiting desirable aging effects. Besides, a high-level feature representation is utilized to preserve personalized identity of the generated face. Experiments on two benchmark datasets demonstrate appealing performance of the proposed method over the state-of-the-art.",http://arxiv.org/pdf/1804.02740v1
823,779,Poster,Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns,"Jianming Lv, South China University of Technology; Weihang Chen, South China University of Technology; Qing Li, City University of Hong Kong; Can Yang, South China University of Technology",Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns,"Most of the proposed person re-identification algorithms conduct supervised training and testing on single labeled datasets with small size, so directly deploying these trained models to a large-scale real-world camera network may lead to poor performance due to underfitting. It is challenging to incrementally optimize the models by using the abundant unlabeled data collected from the target domain. To address this challenge, we propose an unsupervised incremental learning algorithm, TFusion, which is aided by the transfer learning of the pedestrians' spatio-temporal patterns in the target domain. Specifically, the algorithm firstly transfers the visual classifier trained from small labeled source dataset to the unlabeled target dataset so as to learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved classifier. Finally, we propose a learning-to-rank based mutual promotion procedure to incrementally optimize the classifiers based on the unlabeled data in the target domain. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm gains significant improvement compared with the state-of-art cross-dataset unsupervised person re-identification algorithms.",http://arxiv.org/pdf/1803.07293v1
824,1001,Poster,Feature Quantization for Defending Against Distortion of Images,"Zhun Sun, Tohoku University; Mete Ozay, ; Yan Zhang, RIKEN Center for AIP ; Xing Liu, Tohoku University; Takayuki Okatani, Tohoku University/RIKEN AIP",,,
825,1182,Poster,Tagging Like Humans: Diverse and Distinct Image Annotation,"Baoyuan Wu, Tencent AI Lab; Weidong Chen, Tencent; Wei Liu, ; Peng Sun, Tencent; Bernard Ghanem, ; Siwei Lyu, SUNY Albany",Tagging like Humans: Diverse and Distinct Image Annotation,"In this work we propose a new automatic image annotation model, dubbed {\bf diverse and distinct image annotation} (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. Extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts.",http://arxiv.org/pdf/1804.00113v1
826,1224,Poster,Re-weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation,"Qingchao Chen, Unviersity College London; Yang Liu, University of Cambridge; Zhaowen Wang, Adobe; Ian Wassell, ; Kevin Chetty,",,,
827,1696,Poster,Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis,"Seunghoon Hong, POSTECH; Dingdong Yang, University of Michigan; Jongwook Choi, University of Michigan; Honglak Lee, University of Michigan, USA",Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis,"We propose a novel hierarchical approach for text-to-image synthesis by inferring semantic layout. Instead of learning a direct mapping from text to image, our algorithm decomposes the generation process into multiple steps, in which it first constructs a semantic layout from the text by the layout generator and converts the layout to an image by the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching with the text description. Our model not only generates semantically more meaningful images, but also allows automatic annotation of generated images and user-controlled generation process by modifying the generated scene layout. We demonstrate the capability of the proposed model on challenging MS-COCO dataset and show that the model can substantially improve the image quality, interpretability of output and semantic alignment to input text over existing approaches.",http://arxiv.org/pdf/1801.05091v1
828,1738,Poster,Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present,"Xinpeng Chen, Wuhan University; Lin Ma, Tencent AI Lab; Wenhao Jiang, Tencent AI Lab; Jian Yao, ; Wei Liu,",Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present,"Recently, caption generation with an encoder-decoder framework has been extensively studied and applied in different domains, such as image captioning, code captioning, and so on. In this paper, we propose a novel architecture, namely Auto-Reconstructor Network (ARNet), which, coupling with the conventional encoder-decoder framework, works in an end-to-end fashion to generate captions. ARNet aims at reconstructing the previous hidden state with the present one, besides behaving as the input-dependent transition operator. Therefore, ARNet encourages the current hidden state to embed more information from the previous one, which can help regularize the transition dynamics of recurrent neural networks (RNNs). Extensive experimental results show that our proposed ARNet boosts the performance over the existing encoder-decoder models on both image captioning and source code captioning tasks. Additionally, ARNet remarkably reduces the discrepancy between training and inference processes for caption generation. Furthermore, the performance on permuted sequential MNIST demonstrates that ARNet can effectively regularize RNN, especially on modeling long-term dependencies. Our code is available at: https://github.com/chenxinpeng/ARNet",http://arxiv.org/pdf/1803.11439v2
829,1887,Poster,Unsupervised Domain Adaptation with Similarity-Based Classifier,"Pedro Pinheiro, EPFL",DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation,"In computer vision, one is often confronted with problems of domain shifts, which occur when one applies a classifier trained on a source dataset to target data sharing similar characteristics (e.g. same classes), but also different latent data structures (e.g. different acquisition conditions). In such a situation, the model will perform poorly on the new data, since the classifier is specialized to recognize visual cues specific to the source domain. In this work we explore a solution, named DeepJDOT, to tackle this problem: through a measure of discrepancy on joint deep representations/labels based on optimal transport, we not only learn new data representations aligned between the source and target domain, but also simultaneously preserve the discriminative information used by the classifier. We applied DeepJDOT to a series of visual recognition tasks, where it compares favorably against state-of-the-art deep domain adaptation methods.",http://arxiv.org/pdf/1803.10081v1
830,1960,Poster,Learning Deep Sketch Abstraction,"Umar Riaz Muhammad, Queen Mary Uni of London; Yongxin Yang, Queen Mary University of London; Yi-Zhe Song, ; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh",SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval,"We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset. Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval. Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset, we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition. Such superior performances effectively demonstrate the benefit of our sketch-specific design.",http://arxiv.org/pdf/1804.01401v1
831,2427,Poster,Matching Adversarial Networks,"Gellert Mattyus, UBER ATG; Raquel Urtasun, University of Toronto",Adversarial Network Embedding,"Learning low-dimensional representations of networks has proved effective in a variety of tasks such as node classification, link prediction and network visualization. Existing methods can effectively encode different structural properties into the representations, such as neighborhood connectivity patterns, global structural role similarities and other high-order proximities. However, except for objectives to capture network structural properties, most of them suffer from lack of additional constraints for enhancing the robustness of representations. In this paper, we aim to exploit the strengths of generative adversarial networks in capturing latent features, and investigate its contribution in learning stable and robust graph representations. Specifically, we propose an Adversarial Network Embedding (ANE) framework, which leverages the adversarial learning principle to regularize the representation learning. It consists of two components, i.e., a structure preserving component and an adversarial learning component. The former component aims to capture network structural properties, while the latter contributes to learning robust representations by matching the posterior distribution of the latent representations to given priors. As shown by the empirical results, our method is competitive with or superior to state-of-the-art approaches on benchmark network embedding tasks.",http://arxiv.org/pdf/1711.07838v1
832,2463,Poster,SoS-RSC: A Sum-of-Squares Polynomial  Approach to Robustifying Subspace Clustering Algorithms,"Octavia Camps, Northeastern University, USA; Mario Sznaier,",,,
833,2509,Poster,Resource Aware Person Re-identification across Multiple Resolutions,"Yan Wang, Cornell university; Lequn Wang, Cornell University; yurong you, shang hai jiao tong university; xu zou, tsinghua university; Vincent Chen, cornell university; Serena Li, CORNELL UNIVERSITY; Bharath Hariharan, Cornell University; Gao Huang, ; Kilian Weinberger, Cornell University",,,
834,2518,Poster,Learning and Using the Arrow of Time,"Donglai Wei, MIT; Andrew Zisserman, Oxford; William Freeman, MIT/Google; Joseph Lim, University of Southern California",Estimating causal effects of time-dependent exposures on a binary endpoint in a high-dimensional setting,"Recently, the intervention calculus when the DAG is absent (IDA) method was developed to estimate lower bounds of causal effects from observational high-dimensional data. Originally it was introduced to assess the effect of baseline biomarkers which do not vary over time. However, in many clinical settings, measurements of biomarkers are repeated at fixed time points during treatment exposure and, therefore, this method need to be extended. The purpose of this paper is then to extend the first step of the IDA, the Peter Clarks (PC)-algorithm, to a time-dependent exposure in the context of a binary outcome. We generalised the PC-algorithm for taking into account the chronological order of repeated measurements of the exposure and propose to apply the IDA with our new version, the chronologically ordered PC-algorithm (COPC-algorithm). A simulation study has been performed before applying the method for estimating causal effects of time-dependent immunological biomarkers on toxicity, death and progression in patients with metastatic melanoma. The simulation study showed that the completed partially directed acyclic graphs (CPDAGs) obtained using COPC-algorithm were structurally closer to the true CPDAG than CPDAGs obtained using PC-algorithm. Also, causal effects were more accurate when they were estimated based on CPDAGs obtained using COPC-algorithm. Moreover, CPDAGs obtained by COPC-algorithm allowed removing non-chronologic arrows with a variable measured at a time t pointing to a variable measured at a time t' where t'< t. Bidirected edges were less present in CPDAGs obtained with the COPC-algorithm, supporting the fact that there was less variability in causal effects estimated from these CPDAGs. The COPC-algorithm provided CPDAGs that keep the chronological structure present in the data, thus allowed to estimate lower bounds of the causal effect of time-dependent biomarkers.",http://arxiv.org/pdf/1803.10535v2
835,2588,Poster,Neural Style Transfer via Meta Networks,"Falong Shen, Peking University; Shuicheng Yan, ; Gang Zeng, Peking University",,,
836,2601,Poster,"People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting","Mark Marsden, Dublin City University; Kevin McGuinness, DCU ; Suzanne Little, DCU; Ciara Keogh, University College Dublin, Ireland; Noel O'Connor, DCU","People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting","In this paper we propose a technique to adapt a convolutional neural network (CNN) based object counter to additional visual domains and object types while still preserving the original counting function. Domain-specific normalisation and scaling operators are trained to allow the model to adjust to the statistical distributions of the various visual domains. The developed adaptation technique is used to produce a singular patch-based counting regressor capable of counting various object types including people, vehicles, cell nuclei and wildlife. As part of this study a challenging new cell counting dataset in the context of tissue culture and patient diagnosis is constructed. This new collection, referred to as the Dublin Cell Counting (DCC) dataset, is the first of its kind to be made available to the wider computer vision community. State-of-the-art object counting performance is achieved in both the Shanghaitech (parts A and B) and Penguins datasets while competitive performance is observed on the TRANCOS and Modified Bone Marrow (MBM) datasets, all using a shared counting model.",http://arxiv.org/pdf/1711.05586v1
837,2705,Poster,HydraNets: Specialized Dynamic Architectures for Efficient Inference,"Ravi Teja Mullapudi, Carnegie Mellon University; Noam Shazeer, Google; William Mark, Google; Kayvon Fatahalian, Stanford",,,
838,2763,Poster,SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval,"Peng Xu, Beijing University of Posts an; Yongye Huang, Beijing University of Posts and Telecommunications; Tongtong Yuan, Beijing University of Posts and Telecommunications; Kaiyue Pang, QMUL; Yi-Zhe Song, ; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh; Zhanyu Ma, Beijing University of Posts and Telecommunications ; Jun Guo, Beijing University of Posts and Telecommunications",SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval,"We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset. Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval. Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset, we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition. Such superior performances effectively demonstrate the benefit of our sketch-specific design.",http://arxiv.org/pdf/1804.01401v1
839,2781,Poster,From source to target and back: Symmetric Bi-Directional Adaptive GAN,"Paolo Russo, University of Rome La Sapienza; Fabio Carlucci, University of Rome La Sapienza; Tatiana Tommasi, Italian Institute of Tecnology; Barbara Caputo, University of Rome La Sapienza, Italy",From source to target and back: symmetric bi-directional adaptive GAN,"The effectiveness of generative adversarial approaches in producing images according to a specific style or visual domain has recently opened new directions to solve the unsupervised domain adaptation problem. It has been shown that source labeled images can be modified to mimic target samples making it possible to train directly a classifier in the target domain, despite the original lack of annotated data. Inverse mappings from the target to the source domain have also been evaluated but only passing through adapted feature spaces, thus without new image generation. In this paper we propose to better exploit the potential of generative adversarial networks for adaptation by introducing a novel symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. Moreover we define a new class consistency loss that aligns the generators in the two directions imposing to conserve the class identity of an image passing through both domain mappings. A detailed qualitative and quantitative analysis of the reconstructed images confirm the power of our approach. By integrating the two domain specific classifiers obtained with our bi-directional network we exceed previous state-of-the-art unsupervised adaptation results on four different benchmark datasets.",http://arxiv.org/pdf/1705.08824v2
840,3005,Poster,"OLÉ: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning","Jose Lezama, Universidad de la Republica, Uruguay; Qiang Qiu, ; Pablo Musé, Universidad de la Republica, Uruguay; Guillermo Sapiro, Duke",From source to target and back: symmetric bi-directional adaptive GAN,"The effectiveness of generative adversarial approaches in producing images according to a specific style or visual domain has recently opened new directions to solve the unsupervised domain adaptation problem. It has been shown that source labeled images can be modified to mimic target samples making it possible to train directly a classifier in the target domain, despite the original lack of annotated data. Inverse mappings from the target to the source domain have also been evaluated but only passing through adapted feature spaces, thus without new image generation. In this paper we propose to better exploit the potential of generative adversarial networks for adaptation by introducing a novel symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. Moreover we define a new class consistency loss that aligns the generators in the two directions imposing to conserve the class identity of an image passing through both domain mappings. A detailed qualitative and quantitative analysis of the reconstructed images confirm the power of our approach. By integrating the two domain specific classifiers obtained with our bi-directional network we exceed previous state-of-the-art unsupervised adaptation results on four different benchmark datasets.",http://arxiv.org/pdf/1705.08824v2
841,3009,Poster,Efficient parametrization of multi-domain deep neural networks,"Sylvestre-Alvise Rebuffi, University of Oxford; Hakan Bilen, University of Oxford; Andrea Vedaldi, U Oxford",SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters,"Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point set that can be embedded in R^n, by parametrizing a family of convolutional filters. We elaborately design the filter as a product of simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from the classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves the-state-of-the art accuracy 92.4% on standard benchmarks, and shows competitive performance on segmentation task.",http://arxiv.org/pdf/1803.11527v1
842,3208,Poster,Deep Density Clustering of Unconstrained Faces,"Wei-An Lin, UMD; Jun-Cheng Chen, ; Carlos Castillo, ; Rama Chellappa, University of Maryland, USA",,,
843,3327,Poster,Geometric Multi-Model Fitting with a Convex Relaxation Algorithm,"Paul Amayo, Oxford ; Pedro Pinies, University of Oxford; Lina  Paz, University of Oxford; Paul Newman, University of Oxford",Geometric Multi-Model Fitting with a Convex Relaxation Algorithm,"We propose a novel method to fit and segment multi-structural data via convex relaxation. Unlike greedy methods --which maximise the number of inliers-- this approach efficiently searches for a soft assignment of points to models by minimising the energy of the overall classification. Our approach is similar to state-of-the-art energy minimisation techniques which use a global energy. However, we deal with the scaling factor (as the number of models increases) of the original combinatorial problem by relaxing the solution. This relaxation brings two advantages: first, by operating in the continuous domain we can parallelize the calculations. Second, it allows for the use of different metrics which results in a more general formulation.   We demonstrate the versatility of our technique on two different problems of estimating structure from images: plane extraction from RGB-D data and homography estimation from pairs of images. In both cases, we report accurate results on publicly available datasets, in most of the cases outperforming the state-of-the-art.",http://arxiv.org/pdf/1706.01553v1
844,3552,Poster,Fast and Robust Estimation for Unit-Norm Constrained Linear Fitting Problems,"Daiki Ikami, The University of Tokyo; Toshihiko Yamasaki, The University of Tokyo; Kiyoharu Aizawa,",,,
845,3577,Poster,Importance Weighted Adversarial Nets for Partial Domain Adaptation,"Jing Zhang, University of Wollongong; Zewei Ding, University of Wollongong; Wanqing Li, ; Philip Ogunbona, University of Wollongong",Importance Weighted Adversarial Nets for Partial Domain Adaptation,"This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.",http://arxiv.org/pdf/1803.09210v2
846,1052,Poster,Efficient Subpixel Refinement with Symbolic Linear Predictors,"Vincent Lui, Monash University; Jonathon Geeves, Monash University; Winston Yii, Monash University; Tom Drummond, Monash",,,
847,1156,Poster,Scale-recurrent Network for Deep Image Deblurring,"Xin Tao, CUHK; Hongyun Gao, ; Yi Wang, The Chinese University of HK; Xiaoyong Shen, CUHK; Jue Wang, Megvii; Jiaya Jia, Chinese University of Hong Kong",Scale-recurrent Network for Deep Image Deblurring,"In single image deblurring, the ""coarse-to-fine"" scheme, i.e. gradually restoring the sharp image on different resolutions in a pyramid, is very successful in both traditional optimization-based methods and recent neural-network-based approaches. In this paper, we investigate this strategy and propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task. Compared with the many recent learning-based approaches in [25], it has a simpler network structure, a smaller number of parameters and is easier to train. We evaluate our method on large-scale deblurring datasets with complex motion. Results show that our method can produce better quality results than state-of-the-arts, both quantitatively and qualitatively.",http://arxiv.org/pdf/1802.01770v1
848,1944,Poster,DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks,"Orest Kupyn, Ukrainian Catholic University; Volodymyr Budzan, Ukrainian Catholic University; Mykola Mykhailych, UCU; Dmytro Mishkin, Czech Technical University; Jiri Matas,",DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks,"We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss . DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem -- object detection on (de-)blurred images. The method is 5 times faster than the closest competitor -- DeepDeblur. We also introduce a novel method for generating synthetic motion blurred images from sharp ones, allowing realistic dataset augmentation.   The model, code and the dataset are available at https://github.com/KupynOrest/DeblurGAN",http://arxiv.org/pdf/1711.07064v4
849,1991,Poster,A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping,"Debang Li, CASIA; Huikai Wu, CASIA; Junge Zhang, ; Kaiqi Huang,",A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping,"Image cropping aims at improving the aesthetic quality of images by adjusting their composition. Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism. The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size. Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming. Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem. Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping. Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The agent is evaluated on several popular unseen cropping datasets. Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods.",http://arxiv.org/pdf/1709.04595v3
850,2260,Poster,Single Image Dehazing via Conditional Generative Adversarial Network,"Runde Li, NJUST; Jinshan Pan, UC Merced; Zechao Li, Nanjing University of Science and Technology ; Jinhui Tang,",,,
851,3249,Poster,On the Duality Between Retinex and Image Dehazing,"Adrian Galdran, INESC TEC Porto; Aitor Alvarez-Gila, Tecnalia / CVC-Universitat Autonoma de Barcelona; Alessandro Bria, University of Cassino and L.M.; Javier Vazquez-Corral, Universitat Pompeu Fabra; Marcelo Bertalmio,",On the Duality Between Retinex and Image Dehazing,"Image dehazing deals with the removal of undesired loss of visibility in outdoor images due to the presence of fog. Retinex is a color vision model mimicking the ability of the Human Visual System to robustly discount varying illuminations when observing a scene under different spectral lighting conditions. Retinex has been widely explored in the computer vision literature for image enhancement and other related tasks. While these two problems are apparently unrelated, the goal of this work is to show that they can be connected by a simple linear relationship. Specifically, most Retinex-based algorithms have the characteristic feature of always increasing image brightness, which turns them into ideal candidates for effective image dehazing by directly applying Retinex to a hazy image whose intensities have been inverted. In this paper, we give theoretical proof that Retinex on inverted intensities is a solution to the image dehazing problem. Comprehensive qualitative and quantitative results indicate that several classical and modern implementations of Retinex can be transformed into competing image dehazing algorithms performing on pair with more complex fog removal methods, and can overcome some of the main challenges associated with this problem.",http://arxiv.org/pdf/1712.02754v2
852,2484,Poster,Arbitrary Style Transfer with Deep Feature Reshuffle,"Shuyang Gu, USTC; Congliang Chen, Peking University; Jing Liao, ; Lu Yuan, Microsoft Research Asia",,,
853,3311,Poster,Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration,"Xinyuan Zhang, Duke University; Xin Yuan, Nokia Bell Labs; Lawrence Carin,",Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration,"Low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications. We propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches. The low-rank tensors are fed into the alternative direction multiplier method (ADMM) to further improve image reconstruction. The motivating application is compressive sensing (CS), and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in CS applications. An iterative algorithm based on this low-rank tensor factorization strategy, called NLR-TFA, is presented in detail. Experimental results on noiseless and noisy CS measurements demonstrate the superiority of the proposed approach, especially at low CS sampling rates.",http://arxiv.org/pdf/1803.06795v1
854,137,Poster,Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration,"Lu Sheng, The Chinese University of HK; Jing Shao, The Sensetime Group Limited; Ziyi Lin, SenseTime Co. Ltd.; Xiaogang Wang, Chinese University of Hong Kong",,,
855,672,Poster,Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space,"Tatsuya Yokota, Nagoya Institute of Technology; Burak Erem, ; Seyhmus Guler, ; Simon Warfield, Harvard Medical School; Hidekata Hontani,",Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space,"Let us consider a case where all of the elements in some continuous slices are missing in tensor data.   In this case, the nuclear-norm and total variation regularization methods usually fail to recover the missing elements.   The key problem is capturing some delay/shift-invariant structure.   In this study, we consider a low-rank model in an embedded space of a tensor.   For this purpose, we extend a delay embedding for a time series to a ""multi-way delay-embedding transform"" for a tensor, which takes a given incomplete tensor as the input and outputs a higher-order incomplete Hankel tensor.   The higher-order tensor is then recovered by Tucker-based low-rank tensor factorization.   Finally, an estimated tensor can be obtained by using the inverse multi-way delay embedding transform of the recovered higher-order tensor.   Our experiments showed that the proposed method successfully recovered missing slices for some color images and functional magnetic resonance images.",http://arxiv.org/pdf/1804.01736v1
856,1531,Poster,Deep Semantic Face Deblurring,"Ziyi Shen, Beijing Institute of Technology; Wei-Sheng Lai, University of California, Merced; Tingfa Xu, Beijing Institute of Technology; Jan Kautz, NVIDIA; Ming-Hsuan Yang, UC Merced",Deep Semantic Face Deblurring,"In this paper, we present an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks (CNNs). As face images are highly structured and share several key semantic components (e.g., eyes and mouths), the semantic information of a face provides a strong prior for restoration. As such, we propose to incorporate global semantic priors as input and impose local structure losses to regularize the output within a multi-scale deep CNN. We train the network with perceptual and adversarial losses to generate photo-realistic results and develop an incremental training strategy to handle random blur kernels in the wild. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm restores sharp images with more facial details and performs favorably against state-of-the-art methods in terms of restoration quality, face recognition and execution speed.",http://arxiv.org/pdf/1803.03345v2
857,348,Poster,GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning,"Yueqi Duan, Tsinghua University; Ziwei Wang, Tsinghua University; Jiwen Lu, Tsinghua University; Xudong Lin, Tsinghua University; Jie Zhou,",,,
858,106,Poster,Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation,"Qihang Yu, Peking University; Lingxi Xie, UCLA; Yan Wang, JHU; Yuyin Zhou, JHU; Elliot  Fishman, ; Alan Yuille, JHU",Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation,"We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage.   This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice.",http://arxiv.org/pdf/1709.04518v4
859,418,Poster,Thoracic Disease Identification and Localization with Limited Supervision,"Zhe Li, Syracuse University; Chong Wang, Google Inc; Mei Han, Google Inc; Yuan Xue, Google; Wei Wei, Google Inc.; Li-jia Li, Google Inc; Fei-Fei Li, Google Inc.",Thoracic Disease Identification and Localization with Limited Supervision,"Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images. We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks.",http://arxiv.org/pdf/1711.06373v4
860,1173,Poster,Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation,"Xiaowei Xu, University of Notre Dame; Yiyu Shi, University of Notre Dame; Qing Lu, University of Notre Dame; Lin Yang, University of Notre Dame; Sharon Hu, University of Notre Dame; Danny Chen, University of Notre Dame",Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation,"With pervasive applications of medical imaging in health-care, biomedical image segmentation plays a central role in quantitative analysis, clinical diagno- sis, and medical intervention. Since manual anno- tation su ers limited reproducibility, arduous e orts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), par- ticularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmenta- tion, attaining much improved performance. At the same time, quantization of DNNs has become an ac- tive research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing litera- ture on quantization which primarily targets memory and computation complexity reduction, we apply quan- tization as a method to reduce over tting in FCNs for better accuracy. Speci cally, we focus on a state-of- the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annota- tion samples from the original training dataset, obtain- ing an e ective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantiza- tion for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method has a reduction of up to 6.4x on memory usage.",http://arxiv.org/pdf/1803.04907v1
861,3918,Poster,Visual Feature Attribution using Wasserstein GANs,"Christian Baumgartner, ETH Zurich; Lisa Koch, ETH Zurich; Kerem Tezcan, ETH Zurich; Jia Xi Ang, ETH Zurich; Ender Konukoglu, ETH Zurich",Visual Feature Attribution using Wasserstein GANs,"Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.",http://arxiv.org/pdf/1711.08998v2
862,369,Oral,"Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies","Hanbyul Joo, CMU; Tomas Simon, Oculus Research; Yaser Sheikh,","Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies","We present a unified deformation model for the markerless capture of multiple scales of human movement, including facial expressions, body motion, and hand gestures. An initial model is generated by locally stitching together models of the individual parts of the human body, which we refer to as the ""Frankenstein"" model. This model enables the full expression of part movements, including face and hands by a single seamless model. Using a large-scale capture of people wearing everyday clothes, we optimize the Frankenstein model to create ""Adam"". Adam is a calibrated model that shares the same skeleton hierarchy as the initial model but can express hair and clothing geometry, making it directly usable for fitting people as they normally appear in everyday life. Finally, we demonstrate the use of these models for total motion tracking, simultaneously capturing the large-scale body movements and the subtle face and hand motion of a social group of people.",http://arxiv.org/pdf/1801.01615v1
863,128,Oral,Augmented Skeleton Space Transfer for Depth-based Hand Pose Estimation,"Seungryul Baek, Imperial College London; Kwang In Kim, University of Bath; Tae-Kyun Kim, Imperial College London",,,
864,1978,Oral,Synthesizing Images of Humans in Unseen Poses,"Guha Balakrishnan, MIT; Adrian Dalca, ; Amy Zhao, MIT; Fredo Durand, ; John Guttag,",,,
865,567,Spotlight,SSNet: Scale Selection Network for Online 3D Action Prediction,"Jun Liu, Nanyang Technological University; Amir Shahroudy, NTU Singapore; Gang Wang, ; Ling-Yu Duan, ; Alex Kot,",,,
866,1209,Spotlight,Detecting and Recognizing Human-Object Interactions,"Georgia Gkioxari, Facebook; Ross Girshick, ; Kaiming He, ; Piotr Dollar, Facebook AI Research, Menlo Park, USA",Detecting and Recognizing Human-Object Interactions,"To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting <human, verb, object> triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -- their pose, clothing, action -- is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.",http://arxiv.org/pdf/1704.07333v3
867,3971,Spotlight,Unsupervised Learning and Segmentation of Complex Activities from Video,"Fadime Sener, University of Bonn; Angela Yao, University of Bonn",Unsupervised Learning and Segmentation of Complex Activities from Video,"This paper presents a new method for unsupervised segmentation of complex activities from video into multiple steps, or sub-activities, without any textual input. We propose an iterative discriminative-generative approach which alternates between discriminatively learning the appearance of sub-activities from the videos' visual features to sub-activity labels and generatively modelling the temporal structure of sub-activities using a Generalized Mallows Model. In addition, we introduce a model for background to account for frames unrelated to the actual activities. Our approach is validated on the challenging Breakfast Actions and Inria Instructional Videos datasets and outperforms both unsupervised and weakly-supervised state of the art.",http://arxiv.org/pdf/1803.09490v1
868,1349,Spotlight,Unsupervised Training for 3D Morphable Model Regression,"Kyle Genova, Princeton University; Forrester Cole, Google; Aaron Maschinot, Google; Daniel Vlasic, Google; Aaron Sarna, Google; William Freeman, Google",,,
869,2769,Spotlight,Video Based Reconstruction of 3D People Models,"Thiemo Alldieck, TU Braunschweig; Marcus Magnor, TU Braunschweig; Weipeng Xu, MPI Informatics; Christian Theobalt, MPI Informatics; Gerard Pons-Moll, Max Planck for Informatics",Video Based Reconstruction of 3D People Models,"This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.",http://arxiv.org/pdf/1803.04758v2
870,1097,Spotlight,Pose-Guided Photorealistic Face Rotation,"Yibo Hu, CRIPAC, CASIA; Xiang Wu, Institute of Automation, Chine; Bing Yu, ; Ran He, ; Zhenan Sun, CRIPAC",Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis,"Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoder-decoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition.",http://arxiv.org/pdf/1704.04086v2
871,2496,Spotlight,Mesoscopic Facial Geometry inference Using Deep Neural Networks,"Loc Huynh, USC ICT; Weikai Chen, USC ICT; Shunsuke Saito, ; Jun Xing, ICT; Koki Nagano, Pinscreen, Inc; Andrew Jones, USC ICT; Paul Debevec, USC ICT; Hao Li,",,,
872,687,Spotlight,Hand PointNet: 3D Hand Pose Estimation using Point Sets,"Liuhao Ge, NTU; Junwu Weng, Nanyang Technological Univ.; Yujun Cai, NTU; Junsong Yuan, Nanyang Technological University",,,
873,3001,Spotlight,Seeing Voices and Hearing Faces: Cross-modal biometric matching,"Arsha Nagrani, University of Oxford; Samuel Albanie, Oxford University; Andrew Zisserman, Oxford",Seeing Voices and Hearing Faces: Cross-modal biometric matching,"We introduce a seemingly impossible task: given only an audio clip of someone speaking, decide which of two face images is the speaker. In this paper we study this, and a number of related cross-modal tasks, aimed at answering the question: how much can we infer from the voice about the face and vice versa? We study this task ""in the wild"", employing the datasets that are now publicly available for face recognition from static images (VGGFace) and speaker identification from audio (VoxCeleb). These provide training and testing scenarios for both static and dynamic testing of cross-modal matching. We make the following contributions: (i) we introduce CNN architectures for both binary and multi-way cross-modal face and audio matching, (ii) we compare dynamic testing (where video information is available, but the audio is not from the same video) with static testing (where only a single still image is available), and (iii) we use human testing as a baseline to calibrate the difficulty of the task. We show that a CNN can indeed be trained to solve this task in both the static and dynamic scenarios, and is even well above chance on 10-way classification of the face given the voice. The CNN matches human performance on easy examples (e.g. different gender across faces) but exceeds human performance on more challenging examples (e.g. faces with the same gender, age and nationality).",http://arxiv.org/pdf/1804.00326v2
874,2355,Spotlight,Learning Monocular 3D Human Pose estimation on weakly-supervised Multi-view Images,"Helge Rhodin, epfl.ch; Jörg Spörri, Balgrist; Isinsu Katircioglu, EPFL Lausanne, Switzerland; Victor Constantin, EPFL; Frédéric Meyer, ; Erich Müller, ; Mathieu Salzmann, EPFL; Pascal Fua,",,,
875,1644,Spotlight,Separating Style and Content for Generalized Style Transfer,"Yexun Zhang, Shanghai Jiao Tong University; Ya Zhang, ; Wenbin Cai,",Separating Style and Content for Generalized Style Transfer,"Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method.",http://arxiv.org/pdf/1711.06454v5
876,3523,Spotlight,TextureGAN: Controlling Deep Image Synthesis with Texture Patches,"Wenqi Xian, ; Patsorn Sangkloy, Georgia Institute of Technology; Varun Agrawal, ; Amit Raj, Georgia Institute of Technolog; Jingwan Lu, Adobe Research; Chen Fang, Adobe Research; Fisher Yu, UC Berkeley; James Hays, Georgia Tech",TextureGAN: Controlling Deep Image Synthesis with Texture Patches,"In this paper, we investigate deep image synthesis guided by sketch, color, and texture. Previous image synthesis methods can be controlled by sketch and color strokes but we are the first to examine texture control. We allow a user to place a texture patch on a sketch at arbitrary locations and scales to control the desired output texture. Our generative network learns to synthesize objects consistent with these texture suggestions. To achieve this, we develop a local texture loss in addition to adversarial and content loss to train the generative network. We conduct experiments using sketches generated from real images and textures sampled from a separate texture database and results show that our proposed algorithm is able to generate plausible images that are faithful to user controls. Ablation studies show that our proposed pipeline can generate more realistic images than adapting existing methods directly.",http://arxiv.org/pdf/1706.02823v2
877,3536,Spotlight,Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images,"Tribhuvanesh Orekondy, MPI-INF; Mario Fritz, MPI, Saarbrucken, Germany; Bernt Schiele, MPI Informatics Germany",Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images,"Images convey a broad spectrum of personal information. If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual. By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved. Our findings argue for a ""redaction by segmentation"" paradigm. Hence, we propose the first sizable dataset of private images ""in the wild"" annotated with pixel and instance level labels across a broad range of privacy classes. We present the first model for automatic redaction of diverse private information.",http://arxiv.org/pdf/1712.01066v1
878,3966,Oral,MapNet: An Allocentric Spatial Memory for Mapping Environments,"Joao Henriques, ; Andrea Vedaldi, U Oxford",,,
879,3890,Oral,Accurate and Diverse Sampling of Sequences based on a ``Best of Many'' Sample Objective,"Apratim Bhattacharyya, MPI Informatics; Mario Fritz, MPI, Saarbrucken, Germany; Bernt Schiele, MPI Informatics Germany",,,
880,3399,Oral,VirtualHome: Simulating Household Activities via Programs,"Xavier Puig, MIT; Kevin Ra, ; Marko Boben, ; Jiaman Li, University of Toronto; Tingwu Wang, ; Sanja Fidler, ; Antonio Torralba, MIT",,,
881,2082,Spotlight,"The Easy, The Medium and The Hard: Adapting Across Varied Domain Shifts","Swami Sankaranarayanan, University of Maryland; Yogesh Balaji, University of Maryland; Carlos Castillo, ; Rama Chellappa, University of Maryland, USA",,,
882,2347,Spotlight,Multi-Agent Diverse Generative Adversarial Networks,"Viveka Kulharia, University of Oxford; Arnab Ghosh, University of Oxford; Vinay P. Namboodiri, Indian Institute of Technology Kanpur; Phil Torr, Oxford; Puneet Kumar Dokania, University of Oxford",Multi-Agent Diverse Generative Adversarial Networks,"We propose an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known mode collapse problem. Firstly, we propose a multi-agent GAN architecture incorporating multiple generators and one discriminator. Secondly, to enforce different generators to capture diverse high probability modes, we modify discriminator's objective function where along with finding the real and fake samples, the discriminator has to identify the generator that generated the fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. Our framework (MAD-GAN) is generalizable in the sense that it can be easily combined with other existing variants of GANs to produce diverse samples. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for the challenging tasks such as image-to-image translation (known to learn delta distribution) and face generation. In addition, we show that MAD-GAN is able to disentangle different modalities even when trained using highly challenging multi-view dataset (mixture of forests, icebergs, bedrooms etc). In the end, we also show its efficacy for the unsupervised feature representation task. In the appendix we introduce a similarity based competing objective which encourages the different generators to generate varied samples judged by a user defined similarity metric. We show extensive evaluations on a 1-D setting of mixture of gaussians for non parametric density estimation. The theoretical proofs back the efficacy of the framework and explains why various generators are pushed towards distinct clusters of modes.",http://arxiv.org/pdf/1704.02906v2
883,2918,Spotlight,A PID Controller Approach for Stochastic Optimization of Deep Networks,"An Wangpeng , Tsinghua University; Haoqian Wang, Tsinghua University, Shenzhen Graduate School; Qingyun Sun, Stanford Univsersity; Jun Xu, Hong Kong Polytechnic U; QIonghai Dai, Tsinghua University; Lei Zhang, The Hong Kong Polytechnic University",,,
884,3340,Spotlight,"Learning-Compression"" algorithms for neural net pruning""","Miguel Carreira-perpinan, UC Merced; Yerlan Idelbayev, UC Merced",Autoencoder based image compression: can the learning be quantization independent?,"This paper explores the problem of learning transforms for image compression via autoencoders. Usually, the rate-distortion performances of image compression are tuned by varying the quantization step size. In the case of autoen-coders, this in principle would require learning one transform per rate-distortion point at a given quantization step size. Here, we show that comparable performances can be obtained with a unique learned transform. The different rate-distortion points are then reached by varying the quantization step size at test time. This approach saves a lot of training time.",http://arxiv.org/pdf/1802.09371v1
885,3381,Spotlight,Large-scale Distance Metric Learning with Uncertainty,"Qi Qian, Alibaba Group; Shenghuo Zhu, Alibaba Group; Rong Jin, Alibaba Group; Jiasheng Tang, Alibaba Group; Hao Li, Alibaba Group",,,
886,3489,Spotlight,Guide Me: Interacting with Deep Networks,"Christian Rupprecht, Technische Unitversit?t M?nchen; Iro Laina, ; Federico Tombari, ; Nassir Navab, Technical University of Munich; Gregory Hager, Johns Hopkins University",Guide Me: Interacting with Deep Networks,"Interaction and collaboration between humans and intelligent machines has become increasingly important as machine learning methods move into real-world applications that involve end users. While much prior work lies at the intersection of natural language and vision, such as image captioning or image generation from text descriptions, less focus has been placed on the use of language to guide or improve the performance of a learned visual processing algorithm. In this paper, we explore methods to flexibly guide a trained convolutional neural network through user input to improve its performance during inference. We do so by inserting a layer that acts as a spatio-semantic guide into the network. This guide is trained to modify the network's activations, either directly via an energy minimization scheme or indirectly through a recurrent model that translates human language queries to interaction weights. Learning the verbal interaction is fully automatic and does not require manual text annotations. We evaluate the method on two datasets, showing that guiding a pre-trained network can improve performance, and provide extensive insights into the interaction between the guide and the CNN.",http://arxiv.org/pdf/1803.11544v1
887,3771,Spotlight,Art of singular vectors and universal adversarial perturbations,"Valentin Khrulkov, Skoltech; Ivan Oseledets, Skoltech",Art of singular vectors and universal adversarial perturbations,"Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called $(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 \% fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks.",http://arxiv.org/pdf/1709.03582v2
888,4093,Spotlight,Deflecting Adversarial Attacks with Pixel Deflection,"Aaditya Prakash, Brandeis University; Nick Moran, Bradeis University; Solomon Garber, Brandeis University; Antonella DiLillo, Brandeis University; James Storer, Brandeis University",Deflecting Adversarial Attacks with Pixel Deflection,"CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.",http://arxiv.org/pdf/1801.08926v3
889,3308,Spotlight,MovieGraphs: Towards Understanding Human-Centric Situations from Videos,"Paul Vicol, University of Toronto; Makarand Tapaswi, University of Toronto; Lluís Castrejón, ; Sanja Fidler,",MovieGraphs: Towards Understanding Human-Centric Situations from Videos,"There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to ""read"" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents.",http://arxiv.org/pdf/1712.06761v1
890,4057,Spotlight,Captioning Images with Style Transfer from Unaligned Text Corpora,"Alexander Mathews, Australian National University; Xuming He, ShanghaiTech; Lexing Xie, Australian National University, Data61",,,
891,853,Spotlight,Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions,"Torsten Sattler, ETH Zurich; Will Maddern, University of Oxford; Carl Toft, Chalmers University ; Akihiko Torii, Tokyo Institute of Technology; Lars Hammarstrand, Chalmers university of technol; Erik Stenborg, Chalmers University of Tech.; Daniel Safari, DTU; Marc Pollefeys, ETH; Josef Sivic, ; Fredrik Kahl, Chalmers; Tomas Pajdla,",Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions,"Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.",http://arxiv.org/pdf/1707.09092v3
892,2044,Spotlight,iVQA: Inverse Visual Question Answering,"Feng Liu, Southeast Univeristy; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh; Wankou Yang, Southeast University; Changyin Sun, Southeast University",Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis Tool,"In recent years, visual question answering (VQA) has become topical. The premise of VQA's significance as a benchmark in AI, is that both the image and textual question need to be well understood and mutually grounded in order to infer the correct answer. However, current VQA models perhaps `understand' less than initially hoped, and instead master the easier task of exploiting cues given away in the question and biases in the answer distribution. In this paper we propose the inverse problem of VQA (iVQA). The iVQA task is to generate a question that corresponds to a given image and answer pair. We propose a variational iVQA model that can generate diverse, grammatically correct and content correlated questions that match the given answer. Based on this model, we show that iVQA is an interesting benchmark for visuo-linguistic understanding, and a more challenging alternative to VQA because an iVQA model needs to understand the image better to be successful. As a second contribution, we show how to use iVQA in a novel reinforcement learning framework to diagnose any existing VQA model by way of exposing its belief set: the set of question-answer pairs that the VQA model would predict true for a given image. This provides a completely new window into what VQA models `believe' about images. We show that existing VQA models have more erroneous beliefs than previously thought, revealing their intrinsic weaknesses. Suggestions are then made on how to address these weaknesses going forward.",http://arxiv.org/pdf/1803.06936v1
893,4307,Oral,A Volumetric Descriptive Network for 3D Object Synthesis,"Jianwen Xie, UCLA; Zilong Zheng, ucla",,,
894,1988,Oral,Neural Inverse Kinematics for Unsupervised Motion Retargetting,"Ruben Villegas, University of Michigan; Jimei Yang, ; Duygu Ceylan, ; Honglak Lee, University of Michigan, USA",,,
895,1057,Oral,Group Consistent Similarity Learning via Deep CRFs for Person Re-Identification,"Dapeng Chen, CUHK; Dan Xu, ; Hongsheng Li, ; Nicu Sebe, University of Trento, Italy; Xiaogang Wang, Chinese University of Hong Kong",,,
896,1959,Spotlight,Learning Compositional Visual Concepts with Mutual Consistency,"Yunye Gong, Cornell University; Srikrishna Karanam, Siemens Corporate Technology; Ziyan Wu, Siemens Corporation; Kuan-Chuan Peng, Siemens Corporation; Jan Ernst, Siemens Corporation; Peter Doerschuk, Cornell University",Learning Compositional Visual Concepts with Mutual Consistency,"Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.",http://arxiv.org/pdf/1711.06148v2
897,3430,Spotlight,Learning Nested Structures in Deep Neural Networks,"Eunwoo Kim, Seoul National University; Chanho Ahn, Seoul National University; Songhwai Oh, Seoul National University",NestedNet: Learning Nested Sparse Structures in Deep Neural Networks,"Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed. While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep architecture for multiple devices with different resources. When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch. In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network. A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements. Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single network, such as coarse-to-fine hierarchical classification. In order to train the proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies. We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.",http://arxiv.org/pdf/1712.03781v2
898,1580,Spotlight,Context Embedding Networks,"Kun ho Kim, Caltech; Oisin Mac Aodha, Caltech; Pietro Perona, California Institute of Technology, USA",Context Embedding Networks,"Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. However, similarity is a multi-dimensional concept that varies from individual to individual. Existing models for learning embeddings from the crowd typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, the list of criteria is known in advance, or that the crowd workers are not influenced by the data that they see. To overcome these limitations we introduce Context Embedding Networks (CENs). In addition to learning interpretable embeddings from images, CENs also model worker biases for different attributes along with the visual context i.e. the visual attributes highlighted by a set of images. Experiments on two noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches.",http://arxiv.org/pdf/1710.01691v3
899,2630,Spotlight,Iterative Learning with Open-set Noisy Labels,"Yisen Wang, Tsinghua University; Xingjun Ma, The University of Melbourne; Weiyang Liu, Georgia Tech; James Bailey, The University of Melbourne; Hongyuan Zha, Georgia Institute of Technology; Le Song, Georgia Institute of Technology; Shu-Tao Xia, Tsinghua University",Iterative Learning with Open-set Noisy Labels,"Large-scale datasets possessing clean label annotations are crucial for training Convolutional Neural Networks (CNNs). However, labeling large-scale data can be very costly and error-prone, and even high-quality datasets are likely to contain noisy (incorrect) labels. Existing works usually employ a closed-set assumption, whereby the samples associated with noisy labels possess a true class contained within the set of known classes in the training data. However, such an assumption is too restrictive for many applications, since samples associated with noisy labels might in fact possess a true class that is not present in the training data. We refer to this more complex scenario as the \textbf{open-set noisy label} problem and show that it is nontrivial in order to make accurate predictions. To address this problem, we propose a novel iterative learning framework for training CNNs on datasets with open-set noisy labels. Our approach detects noisy labels and learns deep discriminative features in an iterative fashion. To benefit from the noisy label detection, we design a Siamese network to encourage clean labels and noisy labels to be dissimilar. A reweighting module is also applied to simultaneously emphasize the learning from clean labels and reduce the effect caused by noisy labels. Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets demonstrate that our proposed model can robustly train CNNs in the presence of a high proportion of open-set as well as closed-set noisy labels.",http://arxiv.org/pdf/1804.00092v1
900,3298,Spotlight,Learning Transferable Architectures for Scalable Image Recognition,"Barret Zoph, Google; Vijay Vasudevan, Google; Jonathon Shlens, Google; Quoc Le, Google",Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation,"Learning-based approaches to robotic manipulation are limited by the scalability of data collection and accessibility of labels. In this paper, we present a multi-task domain adaptation framework for instance grasping in cluttered scenes by utilizing simulated robot experiments. Our neural network takes monocular RGB images and the instance segmentation mask of a specified target object as inputs, and predicts the probability of successfully grasping the specified object for each candidate motor command. The proposed transfer learning framework trains a model for instance grasping in simulation and uses a domain-adversarial loss to transfer the trained model to real robots using indiscriminate grasping data, which is available both in simulation and the real world. We evaluate our model in real-world robot experiments, comparing it with alternative model architectures as well as an indiscriminate grasping baseline.",http://arxiv.org/pdf/1710.06422v2
901,1957,Spotlight,SBNet: Sparse Block’s Network for Fast Inference,"Mengye Ren, Uber ATG; Andrei Pokrovsky, Uber ATG; Bin Yang, Uber ATG, UofT; Raquel Urtasun, University of Toronto",Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation,"Learning-based approaches to robotic manipulation are limited by the scalability of data collection and accessibility of labels. In this paper, we present a multi-task domain adaptation framework for instance grasping in cluttered scenes by utilizing simulated robot experiments. Our neural network takes monocular RGB images and the instance segmentation mask of a specified target object as inputs, and predicts the probability of successfully grasping the specified object for each candidate motor command. The proposed transfer learning framework trains a model for instance grasping in simulation and uses a domain-adversarial loss to transfer the trained model to real robots using indiscriminate grasping data, which is available both in simulation and the real world. We evaluate our model in real-world robot experiments, comparing it with alternative model architectures as well as an indiscriminate grasping baseline.",http://arxiv.org/pdf/1710.06422v2
902,1956,Spotlight,Language-Based Image Editing with Recurrent attentive Models,"Yelong Shen, Microsoft; Jianbo Chen, UC Berkeley; Jianfeng Gao, ; JingJing Liu, Microsoft; Xiaodong Liu, Microsoft",Language-Based Image Editing with Recurrent Attentive Models,"We investigate the problem of Language-Based Image Editing (LBIE) in this work. Given a source image and a natural language description, we want to generate a target image by editing the source im- age based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each re- gion of the image a termination gate to dynamically determine in each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework has been validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the- art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset, laying the foundation for future research.",http://arxiv.org/pdf/1711.06288v1
903,4217,Spotlight,Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks,"Ruth Fong, University of Oxford; Andrea Vedaldi, U Oxford",Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks,"In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation.   A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts.",http://arxiv.org/pdf/1801.03454v2
904,37,Spotlight,End-to-End Dense Video Captioning with Masked Transformer,"Luowei Zhou, University of Michigan; Yingbo Zhou, Salesforce; Jason Corso, ; Richard Socher, Meta-Mind; Caiming Xiong, Salesforce",End-to-End Dense Video Captioning with Masked Transformer,"Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.",http://arxiv.org/pdf/1804.00819v1
905,914,Spotlight,LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH),"Pelin Dogan, ETH Zurich; Albert Li, Disney Research; Leonid Sigal, University of British Columbia; Markus Gross,",LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH),"The alignment of heterogeneous sequential data (video to text) is an important and challenging problem. Standard techniques for such alignment, including Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from inherent drawbacks. Mainly, the Markov assumption implies that, given the immediate past, future alignment decisions are independent of further history. The separation between similarity computation and alignment decision also prevents end-to-end training. In this paper, we propose an end-to-end neural architecture where alignment actions are implemented as moving data between stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture supports a large variety of alignment tasks, including one-to-one, one-to-many, skipping unmatched elements, and (with extensions) non-monotonic alignment. Extensive experiments on synthetic and real datasets show that our algorithm outperforms state-of-the-art baselines.",http://arxiv.org/pdf/1803.00057v1
906,2784,Spotlight,Path Aggregation Network for Instance Segmentation,"Shu Liu, CUHK; Lu Qi, CUHK; Haifang Qin, ; Jianping Shi, SenseTime; Jiaya Jia, Chinese University of Hong Kong",Path Aggregation Network for Instance Segmentation,"The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes.",http://arxiv.org/pdf/1803.01534v1
907,1916,Spotlight,The iNaturalist Species Classification and Detection Dataset,"Grant van Horn, California Institute of Technology; Oisin Mac Aodha, Caltech; Yang Song, Google; Yin Cui, CornellTech; Chen Sun, Google; Alex Shepard, iNaturalist; Hartwig Adam, Google; Pietro Perona, California Institute of Technology, USA; Serge Belongie,",,,
908,2708,Spotlight,Multimodal Explanations: Justifying Decisions and Pointing to the Evidence,"Lisa Anne Hendricks, UC Berkeley; Trevor Darrell, UC Berkeley, USA; Anna Rohrbach, UC Berkeley; Zeynep Akata, University of Amsterdam; Bernt Schiele, MPI Informatics Germany; Marcus Rohrbach, UC Berkeley; Dong Huk Park, UC Berkeley",Multimodal Explanations: Justifying Decisions and Pointing to the Evidence,"Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.",http://arxiv.org/pdf/1802.08129v1
909,3662,Oral,StarGAN: Unified Generative Adversarial Networks for Controllable Multi-Domain Image-to-Image Translation,"Jaegul Choo, Korea University; Jung-Woo Ha, NAVER Corp; Munyoung Kim, The College of New Jersey; Yunjey Choi, Korea University; Minje Choi, Korea University; Sunghun Kim, HKUST",,,
910,768,Oral,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,"Ting-Chun Wang, NVIDIA; Ming-Yu Liu, NVIDIA; Jun-Yan Zhu, UC Berkeley; Andrew Tao, NVIDIA; Bryan Catanzaro, NVIDIA; Jan Kautz, NVIDIA",High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,"We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",http://arxiv.org/pdf/1711.11585v1
911,145,Oral,Semi-parametric Image Synthesis,"Xiaojuan Qi, CUHK; Qifeng Chen, Intel Labs; Jiaya Jia, Chinese University of Hong Kong; Vladlen Koltun, Intel Labs",SPARCO : a semi-parametric approach for image reconstruction of chromatic objects,"The emergence of optical interferometers with three and more telescopes allows image reconstruction of astronomical objects at the milliarcsecond scale. However, some objects contain components with very different spectral energy distributions (SED; i.e. different temperatures), which produces strong chromatic effects on the interferograms that have to be managed with care by image reconstruction algorithms. For example, the gray approximation for the image reconstruction process results in a degraded image if the total (u, v)-coverage given by the spectral supersynthesis is used. The relative flux contribution of the central object and an extended structure changes with wavelength for different temperatures. For young stellar objects, the known characteristics of the central object (i.e., stellar SED), or even the fit of the spectral index and the relative flux ratio, can be used to model the central star while reconstructing the image of the extended structure separately. Methods. We present a new method, called SPARCO (semi-parametric algorithm for the image reconstruction of chromatic objects), which describes the spectral characteristics of both the central object and the extended structure to consider them properly when reconstructing the image of the surrounding environment. We adapted two image-reconstruction codes (Macim, Squeeze, and MiRA) to implement this new prescription. SPARCO is applied using Macim, Squeeze and MiRA on a young stellar object model and also on literature data on HR5999 in the near-infrared with the VLTI. This method paves the way to improved aperture-synthesis imaging of several young stellar objects with existing datasets. More generally, the approach can be used on astrophysical sources with similar features such as active galactic nuclei, planetary nebulae, and asymptotic giant branch stars.",http://arxiv.org/pdf/1403.3343v1
912,167,Spotlight,BlockDrop: Dynamic Inference Paths in Residual Networks,"Zuxuan Wu, University of Maryland; Tushar Nagarajan, University of Texas at Austin; Abhishek Kumar, ; Steven Rennie, ; Larry Davis, University of Maryland, USA; Kristen Grauman, ; Rogerio Feris, IBM",BlockDrop: Dynamic Inference Paths in Residual Networks,"Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\% on average, going as high as 36\% for some images, while maintaining the same 76.4\% top-1 accuracy on ImageNet.",http://arxiv.org/pdf/1711.08393v2
913,490,Spotlight,Interpretable Convolutional Neural Networks,"Quanshi Zhang, UCLA; Yingnian Wu, ; Song-Chun Zhu,",Visualizing Convolutional Neural Network Protein-Ligand Scoring,Protein-ligand scoring is an important step in a structure-based drug design pipeline. Selecting a correct binding pose and predicting the binding affinity of a protein-ligand complex enables effective virtual screening. Machine learning techniques can make use of the increasing amounts of structural data that are becoming publicly available. Convolutional neural network (CNN) scoring functions in particular have shown promise in pose selection and affinity prediction for protein-ligand complexes. Neural networks are known for being difficult to interpret. Understanding the decisions of a particular network can help tune parameters and training data to maximize performance. Visualization of neural networks helps decompose complex scoring functions into pictures that are more easily parsed by humans. Here we present three methods for visualizing how individual protein-ligand complexes are interpreted by 3D convolutional neural networks. We also present a visualization of the convolutional filters and their weights. We describe how the intuition provided by these visualizations aids in network design.,http://arxiv.org/pdf/1803.02398v1
914,581,Spotlight,Deep Cross-media Knowledge Transfer,"Xin Huang, Peking University; Yuxin Peng, Peking University",Deep Cross-media Knowledge Transfer,"Cross-media retrieval is a research hotspot in multimedia area, which aims to perform retrieval across different media types such as image and text. The performance of existing methods usually relies on labeled data for model training. However, cross-media data is very labor consuming to collect and label, so how to transfer valuable knowledge in existing data to new data is a key problem towards application. For achieving the goal, this paper proposes deep cross-media knowledge transfer (DCKT) approach, which transfers knowledge from a large-scale cross-media dataset to promote the model training on another small-scale cross-media dataset. The main contributions of DCKT are: (1) Two-level transfer architecture is proposed to jointly minimize the media-level and correlation-level domain discrepancies, which allows two important and complementary aspects of knowledge to be transferred: intra-media semantic and inter-media correlation knowledge. It can enrich the training information and boost the retrieval accuracy. (2) Progressive transfer mechanism is proposed to iteratively select training samples with ascending transfer difficulties, via the metric of cross-media domain consistency with adaptive feedback. It can drive the transfer process to gradually reduce vast cross-media domain discrepancy, so as to enhance the robustness of model training. For verifying the effectiveness of DCKT, we take the largescale dataset XMediaNet as source domain, and 3 widelyused datasets as target domain for cross-media retrieval. Experimental results show that DCKT achieves promising improvement on retrieval accuracy.",http://arxiv.org/pdf/1803.03777v1
915,1255,Spotlight,Interleaved Structured Sparse Convolutional Neural Networks,"Guotian Xie, Sun Yat-Sen University; Ting Zhang, Microsoft  Research  Asia; Jianhuang Lai, Sun Yat-sen University; Jingdong Wang, Microsoft Research",,,
916,1449,Spotlight,A Variational U-Net for Conditional Appearance and Shape Generation,"Ekaterina Sutter, HCI, IWR,Heidelberg University; Patrick Esser, Heidelberg University; Bjorn Ommer, Heidelberg",,,
917,1535,Spotlight,Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation,"Yen-Cheng Liu, National Taiwan University; Yu-Ying Yeh, National Taiwan University; Tzu-Chien Fu, Northwestern University; Wei-Chen Chiu, National Chiao Tung University; Sheng-De Wang, National Taiwan University; Yu-Chiang  Frank Wang, Academia Sinica",Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation,"While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.",http://arxiv.org/pdf/1705.01314v3
918,1883,Spotlight,Learning deep structured active contours end-to-end,"Diego Marcos, ; Devis Tuia, Wageningen University; Benjamin Kellenberger, Wageningen University and Research; Lisa Zhang, University of Toronto; Min Bai, ; Renjie Liao, ; Raquel Urtasun, University of Toronto",Learning deep structured active contours end-to-end,"The world is covered with millions of buildings, and precisely knowing each instance's position and extents is vital to a multitude of applications. Recently, automated building footprint segmentation models have shown superior detection accuracy thanks to the usage of Convolutional Neural Networks (CNN). However, even the latest evolutions struggle to precisely delineating borders, which often leads to geometric distortions and inadvertent fusion of adjacent building instances. We propose to overcome this issue by exploiting the distinct geometric properties of buildings. To this end, we present Deep Structured Active Contours (DSAC), a novel framework that integrates priors and constraints into the segmentation process, such as continuous boundaries, smooth edges, and sharp corners. To do so, DSAC employs Active Contour Models (ACM), a family of constraint- and prior-based polygonal models. We learn ACM parameterizations per instance using a CNN, and show how to incorporate all components in a structured output model, making DSAC trainable end-to-end. We evaluate DSAC on three challenging building instance segmentation datasets, where it compares favorably against state-of-the-art. Code will be made available.",http://arxiv.org/pdf/1803.06329v1
919,2514,Spotlight,Deep Learning under Privileged Information Using  Heteroscedastic Dropout,"Ozan Sener, Stanford University; Silvio Savarese, ; John Lambert, Stanford University",,,
920,2905,Spotlight,Smooth Neighbors on Teacher Graphs for Semi-supervised Learning,"Yucen Luo, Tsinghua University; Jun Zhu, Tsinghua University; Mengxi Li, Tsinghua University; Yong Ren, Tsinghua University; Bo Zhang,",Smooth Neighbors on Teacher Graphs for Semi-supervised Learning,"The recently proposed self-ensembling methods have achieved promising results in deep semi-supervised learning, which penalize inconsistent predictions of unlabeled data under different perturbations. However, they only consider adding perturbations to each single data point, while ignoring the connections between data samples. In this paper, we propose a novel method, called Smooth Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on the predictions of the teacher model, i.e., the implicit self-ensemble of models. Then the graph serves as a similarity measure with respect to which the representations of ""similar"" neighboring points are learned to be smooth on the low-dimensional manifold. We achieve state-of-the-art results on semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular, the improvements are significant when the labels are fewer. For the non-augmented MNIST with only 20 labels, the error rate is reduced from previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.",http://arxiv.org/pdf/1711.00258v2
921,2964,Spotlight,Interpret Neural Networks by Identifying Critical Data Routing Paths,"Yulong Wang, Tsinghua University; Hang Su, Tsinghua University; Xiaolin Hu, tsinghua",,,
922,3218,Spotlight,Deep Spatio-Temporal Random Fields for Efficient Video Segmentation,"Siddhartha Chandra, INRIA; Camille Couprie, Facebook Artificial Intelligence Research; Iasonas Kokkinos, FAIR/UCL",,,
923,3815,Spotlight,Customized Image Narrative Generation via Interactive Visual Question Generation and Answering,"Andrew Shin, The University of Tokyo; Yoshitaka Ushiku, ; Tatsuya Harada, University of Tokyo",,,
924,266,Oral,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume","Deqing Sun, NVIDIA; Xiaodong Yang, NVIDIA; Ming-Yu Liu, NVIDIA; Jan Kautz, NVIDIA","PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume","We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct the cost volume, which is processed by a CNN to estimate the optical flow. PWCNet is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our model will be publicly available.",http://arxiv.org/pdf/1709.02371v2
925,3626,Oral,Revisiting Deep Intrinsic Image Decompositions,"Qingnan Fan, Shandong University; David Wipf, Microsoft Research Asia; Jiaolong Yang, Microsoft Research Asia; Gang Hua, Microsoft Research; Baoquan Chen,",Revisiting Deep Intrinsic Image Decompositions,"While invaluable for many computer vision applications, decomposing a natural image into intrinsic reflectance and shading layers represents a challenging, underdetermined inverse problem. As opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data. The downside is that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images in synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes. In contrast to many previous learning-based approaches, which are often tailored to the structure of a particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets. We then apply flexibly supervised loss layers that are customized for each source of ground truth labels. The resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks, and runs considerably faster than most at test time.",http://arxiv.org/pdf/1701.02965v4
926,3723,Oral,Multi-Cell Classification by Convolutional Dictionary Learning with Class Proportion Priors,"Florence Yellin, Johns Hopkins University; Benjamin Haeffele, Johns Hopkins University; Rene Vidal, Johns Hopkins University",,,
927,1676,Spotlight,Learning Spatial-Aware Regressions for Visual Tracking,"Chong Sun, DalianUniversityofTechnology; Dong Wang, DUT; Huchuan Lu, Dalian University of Technology; Ming-Hsuan Yang, UC Merced",Learning Spatial-Aware Regressions for Visual Tracking,"In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on three benchmark datasets validate the effectiveness of the proposed method.",http://arxiv.org/pdf/1706.07457v1
928,2951,Spotlight,High Performance Visual Tracking with Siamese Region Proposal Network,"Bo Li, SenseTime ; Wei Wu, ; Zheng Zhu, Institute of Automation, CAS; Junjie Yan,",,,
929,136,Spotlight,LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation,"Tak-Wai Hui, The Chinese University of Hong Kong; Chen-Change Loy, the Chinese University of Hong Kong; Xiaoou Tang, Chinese University of Hong Kong",,,
930,1280,Spotlight,VITAL: VIsual Tracking via Adversarial Learning,"Yibing Song, Tencent AI Lab; Chao Ma, ; Xiaohe Wu, Harbin Institute of technology; Lijun Gong, City University of Hong Kong; Linchao Bao, Tencent AI Lab; Wangmeng Zuo, Harbin Institute of Technology; Chunhua Shen, University of Adelaide; Rynson Lau, City University of Hong Kong; Ming-Hsuan Yang, UC Merced",,,
931,325,Spotlight,Super SloMo: High Quality Estimation of Multiple Intermediate Frames for  Video Interpolation,"Huaizu Jiang, UMass Amherst; Deqing Sun, NVIDIA; Varun Jampani, NVIDIA Research; Ming-Hsuan Yang, UC Merced; Erik Miller, ; Jan Kautz, NVIDIA",Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation,"Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",http://arxiv.org/pdf/1712.00080v1
932,103,Spotlight,"Real-World Repetition Estimation by Div, Grad and Curl","Tom Runia, University of Amsterdam; Cees Snoek, University of Amsterdam; Arnold Smeulders, University of Amsterdam, Netherlands","Real-World Repetition Estimation by Div, Grad and Curl","We consider the problem of estimating repetition in video, such as performing push-ups, cutting a melon or playing violin. Existing work shows good results under the assumption of static and stationary periodicity. As realistic video is rarely perfectly static and stationary, the often preferred Fourier-based measurements is inapt. Instead, we adopt the wavelet transform to better handle non-static and non-stationary video dynamics. From the flow field and its differentials, we derive three fundamental motion types and three motion continuities of intrinsic periodicity in 3D. On top of this, the 2D perception of 3D periodicity considers two extreme viewpoints. What follows are 18 fundamental cases of recurrent perception in 2D. In practice, to deal with the variety of repetitive appearance, our theory implies measuring time-varying flow and its differentials (gradient, divergence and curl) over segmented foreground motion. For experiments, we introduce the new QUVA Repetition dataset, reflecting reality by including non-static and non-stationary videos. On the task of counting repetitions in video, we obtain favorable results compared to a deep learning alternative.",http://arxiv.org/pdf/1802.09971v1
933,533,Spotlight,Recurrent Pixel Embedding for Instance Grouping,"Shu Kong, University of California, Irvine; Charless Fowlkes, University of California, Irvine, USA",Recurrent Pixel Embedding for Instance Grouping,"We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on only correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.",http://arxiv.org/pdf/1712.08273v1
934,1202,Spotlight,Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective,"Jing Zhang, ; Tong Zhang, Australian National University; Yuchao Dai, Australian National University; Mehrtash Harandi, Australian National University; Richard Hartley, Australian National University Australia",Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective,"The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the form of per-pixel labeling. Such supervision, while labor-intensive and not always possible, tends to hinder the generalization ability of the learned models. By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could be applied in the wild. This raises a natural question that ""Is it possible to learn saliency maps without using labeled data while improving the generalization ability?"". To this end, we present a novel perspective to unsupervised saliency detection through learning from multiple noisy labeling generated by ""weak"" and ""noisy"" unsupervised handcrafted saliency methods. Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly. Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way. Extensive experimental results on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-of-the-art supervised deep saliency methods.",http://arxiv.org/pdf/1803.10910v1
935,110,Spotlight,Learning Intrinsic Image Decomposition from Watching the World,"Zhengqi Li, Cornell University; Noah Snavely, Cornell University / Google",Learning Intrinsic Image Decomposition from Watching the World,"Single-view intrinsic image decomposition is a highly ill-posed problem, and so a promising approach is to learn from large amounts of data. However, it is difficult to collect ground truth training data at scale for intrinsic images. In this paper, we explore a different approach to learning intrinsic images: observing image sequences over time depicting the same scene under changing illumination, and learning single-view decompositions that are consistent with these changes. This approach allows us to learn without ground truth decompositions, and to instead exploit information available from multiple images when training. Our trained model can then be applied at test time to single views. We describe a new learning framework based on this idea, including new loss functions that can be efficiently evaluated over entire sequences. While prior learning-based methods achieve good performance on specific benchmarks, we show that our approach generalizes well to several diverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild and Shading Annotations in the Wild.",http://arxiv.org/pdf/1804.00582v1
936,53,Spotlight,TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays,"Xiaosong Wang, NIH; Yifan Peng, NIH NLM; Le Lu, Nvidia Corp; Zhiyong Lu, ; Ronald Summers,",,,
937,500,Spotlight,Generating Synthetic X-ray Images of a Person from the Surface Geometry,"Brian Teixeira, Siemens Healthineers; Vivek Singh, Siemens Healthineers; Kai Ma, Siemens Healthineers; Birgi Tamersoy, Siemens Healthineers; Terrence Chen, Siemens Healthineers; Yifan Wu, Temple University ; Elena Balashova, Princeton University; Dorin Comaniciu, Siemens Healthineers",,,
938,747,Spotlight,Embodied Real-World Active Perception,"Fei Xia, Stanford University; Amir Zamir, Stanford, UC Berkeley; Zhi-Yang He, Stanford University; Alexander Sax, Stanford University; Jitendra Malik, ; Silvio Savarese,",A probabilistic tour of visual attention and gaze shift computational models,"In this paper a number of problems are considered which are related to the modelling of eye guidance under visual attention in a natural setting. From a crude discussion of a variety of available models spelled in probabilistic terms, it appears that current approaches in computational vision are hitherto far from achieving the goal of an active observer relying upon eye guidance to accomplish real-world tasks. We argue that this challenging goal not only requires to embody, in a principled way, the problem of eye guidance within the action/perception loop, but to face the inextricable link tying up visual attention, emotion and executive control, in so far as recent neurobiological findings are weighed up.",http://arxiv.org/pdf/1607.01232v1
939,3342,Spotlight,Reinforcement Cutting-Agent Learning for Video Object Segmentation,"Junwei Han, Northwestern Polytechnical U.; Le Yang, Northwestern Polytechnical Uni; Dingwen Zhang, ; Xiaojun Chang, Carnegie Mellon University; Xiaodan Liang, Carnegie Mellon University",,,
940,3074,Oral,Feature Space Transfer for Data Augmentation,"Bo Liu, UCSD; Xudong Wang, UCSD; Mandar Dixit, UC San Diego; Roland Kwitt, ; Nuno Vasconcelos, UCSD, USA",Feature Transfer Learning for Deep Face Recognition with Long-Tail Data,"Real-world face recognition datasets exhibit long-tail characteristics, which results in biased classifiers in conventionally-trained deep neural networks, or insufficient data when long-tail classes are ignored. In this paper, we propose to handle long-tail classes in the training of a face recognition engine by augmenting their feature space under a center-based feature transfer framework. A Gaussian prior is assumed across all the head (regular) classes and the variance from regular classes are transferred to the long-tail class representation. This encourages the long-tail distribution to be closer to the regular distribution, while enriching and balancing the limited training data. Further, an alternating training regimen is proposed to simultaneously achieve less biased decision boundaries and a more discriminative feature representation. We conduct empirical studies that mimic long-tail datasets by limiting the number of samples and the proportion of long-tail classes on the MS-Celeb-1M dataset. We compare our method with baselines not designed to handle long-tail classes and also with state-of-the-art methods on face recognition benchmarks. State-of-the-art results on LFW, IJB-A and MS-Celeb-1M datasets demonstrate the effectiveness of our feature transfer approach and training strategy. Finally, our feature transfer allows smooth visual interpolation, which demonstrates disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations.",http://arxiv.org/pdf/1803.09014v1
941,487,Oral,Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input,"Adel Bibi, KAUST; Modar Alfadly, King Abdullah University of Science and Technology; Bernard Ghanem,",,,
942,1532,Oral,Detail-Preserving Pooling in Deep Networks,"Faraz Saeedan, TU Darmstadt; Nicolas Weber, ; Michael Goesele, TU Darmstadt; Stefan Roth,",Depth from Monocular Images using a Semi-Parallel Deep Neural Network (SPDNN) Hybrid Architecture,"Convolutional Neural Network (CNN) techniques are applied to the problem of determining the depth from a single camera image (monocular depth). Fully connected CNN topologies preserve all details of the input images, enabling the detection of fine details, but miss larger features; networks that employ 2x2, 4x4 and 8x8 max-pooling operators can determine larger features at the expense of finer details. After designing, training and optimising a set of topologies, these networks may be combined into a single network topology using graph optimization techniques. This ""Semi Parallel Deep Neural Network (SPDNN)"" eliminates duplicate common network layers, reducing network size and computational effort significantly, and can be further optimized by retraining to achieve an improved level of convergence over the individual topologies. In this study, four models are trained and have been evaluated at 2 stages on the KITTI dataset. The ground truth images in the first part of the experiment come from the benchmark, and for the second part, the ground truth images are the depth map results from applying a state-of-the-art stereo matching method. The results of this evaluation demonstrate that using post-processing techniques to refine the target of the network increases the accuracy of depth estimation on individual mono images. The second evaluation shows that using segmentation data as the input can improve the depth estimation results to a point where performance is comparable with stereo depth estimation. The computational time is also discussed in this study.",http://arxiv.org/pdf/1703.03867v2
943,3524,Spotlight,Rethinking Feature Distribution for Loss Functions in Image Classification,"Weitao Wan, Tsinghua University; Yuanyi Zhong, UIUC; Tianpeng Li, Tsinghua University; Jiansheng Chen, Tsinghua University",Rethinking Feature Distribution for Loss Functions in Image Classification,"We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.",http://arxiv.org/pdf/1803.02988v1
944,3990,Spotlight,"Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions","Bichen Wu, UC Berkeley; Xiangyu  Yue, UC Berkeley; Alvin Wan, UC Berkeley; Peter Jin, UC Berkeley; Sicheng Zhao, UC Berkeley; Noah Golmant, UC Berkeley; Amir Gholaminejad, UC Berkeley; Joseph Gonzalez, UC Berkeley; Kurt Keutzer, UC Berkeley","Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions","Neural networks rely on convolutions to aggregate spatial information. However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. In this paper, we present a parameter-free, FLOP-free ""shift"" operation as an alternative to spatial convolutions. We fuse shifts and point-wise convolutions to construct end-to-end trainable shift-based modules, with a hyperparameter characterizing the tradeoff between accuracy and efficiency. To demonstrate the operation's efficacy, we replace ResNet's 3x3 convolutions with shift-based modules for improved CIFAR10 and CIFAR100 accuracy using 60% fewer parameters; we additionally demonstrate the operation's resilience to parameter reduction on ImageNet, outperforming ResNet family members. We finally show the shift operation's applicability across domains, achieving strong performance with fewer parameters on classification, face verification and style transfer.",http://arxiv.org/pdf/1711.08141v2
945,1594,Spotlight,Sketch-a-Classifier: Sketch-based Photo Classifier Generation,"Conghui Hu, Queen Mary University of Londo; Da Li, ; Yi-Zhe Song, ; Tao Xiang, Queen Mary University of London; Timothy Hospedales, University of Edinburgh",,,
946,2351,Spotlight,Light field intrinsics with a deep encoder-decoder network,"Anna Alperovich, University of Konstanz; Ole  Johannsen, University of Konstanz; Michael Strecke, University of Konstanz; Bastian Goldluecke,",,,
947,2425,Spotlight,Learning Multi-grid Generative ConvNets by Minimal Contrastive Divergence,"Ruiqi Gao, UCLA; Yang Lu, University of California Los Angeles; Junpei Zhou, ; Song-Chun Zhu, ; Yingnian Wu,",Learning Multi-grid Generative ConvNets by Minimal Contrastive Divergence,"This paper proposes a minimal contrastive divergence method for learning energy-based generative ConvNet models of images at multiple grids (or scales) simultaneously. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We call this learning method the multi-grid minimal contrastive divergence. We show that this method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD.",http://arxiv.org/pdf/1709.08868v1
948,2989,Spotlight,Manifold Learning in Quotient Spaces,"Éloi Mehr, LIP6; André Lieutier, ; Fernando Sanchez Bermudez, ; Vincent Guitteny, ; Nicolas Thome, Conservatoire national des arts et métiers; Matthieu Cord,",Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds,"Recent work has explored methods for learning continuous vector space word representations reflecting the underlying semantics of words. Simple vector space arithmetic using cosine distances has been shown to capture certain types of analogies, such as reasoning about plurals from singulars, past tense from present tense, etc. In this paper, we introduce a new approach to capture analogies in continuous word representations, based on modeling not just individual word vectors, but rather the subspaces spanned by groups of words. We exploit the property that the set of subspaces in n-dimensional Euclidean space form a curved manifold space called the Grassmannian, a quotient subgroup of the Lie group of rotations in n- dimensions. Based on this mathematical model, we develop a modified cosine distance model based on geodesic kernels that captures relation-specific distances across word categories. Our experiments on analogy tasks show that our approach performs significantly better than the previous approaches for the given task.",http://arxiv.org/pdf/1507.07636v1
949,3741,Spotlight,Learning Intelligent Dialogs for Bounding Box Annotation,"Ksenia Konyushkova, Google; Jasper Uijlings, Google; Christoph Lampert, ; Vittorio Ferrari, google",Learning Intelligent Dialogs for Bounding Box Annotation,"We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.",http://arxiv.org/pdf/1712.08087v2
950,4098,Spotlight,Boosting Adversarial Attacks with Momentum,"Yinpeng Dong, Tsinghua Univeristy; Fangzhou Liao, Tsinghua University; Tianyu Pang, Tsinghua University; Hang Su, Tsinghua University; Jun Zhu, Tsinghua University; Xiaolin Hu, tsinghua; Jianguo Li, Intel Lab",Boosting Adversarial Attacks with Momentum,"Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.",http://arxiv.org/pdf/1710.06081v3
951,601,Spotlight,NISP: Pruning Networks using Neuron Importance Score Propagation,"Ruichi Yu, ; Ang Li, Google DeepMind; Chun-Fu (Richard) Chen, IBM T.J. Watson Research Cente; Jui-Hsin Lai, ; Vlad Morariu, University of Maryland; Xintong Han, University of Maryland; Mingfei Gao, University of Maryland; Ching-Yung Lin, ; Larry Davis, University of Maryland, USA",NISP: Pruning Networks using Neuron Importance Score Propagation,"To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the ""final response layer"" (FRL), which is the second-to-last layer before classification, for a pruned network to retrain its predictive power. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, and formulate network pruning as a binary integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and then fine-tuned to retain its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss.",http://arxiv.org/pdf/1711.05908v3
952,1018,Spotlight,PointGrid: A Deep Network for 3D Shape Understanding,"Truc Le, University of Missouri - Columbia; Ye Duan, University of Missouri - Columbia",,,
953,1328,Spotlight,Tell Me Where To Look: Guided Attention Inference Network,"Kunpeng Li, Northeastern University; Ziyan Wu, Siemens Corporation; Kuan-Chuan Peng, Siemens Corporation; Jan Ernst, Siemens Corporation; Yun Fu, Northeastern University",Tell Me Where to Look: Guided Attention Inference Network,"Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) first time make attention maps an explicit and natural component of the end-to-end training, (2) provide self-guidance directly on these maps by exploring supervision form the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on Pascal VOC 2012 val. and test set. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.",http://arxiv.org/pdf/1802.10171v1
954,3108,Spotlight,3D Semantic Segmentation with Submanifold Sparse Convolutional Networks,"Benjamin Graham, Facebook AI Research; Laurens van der Maaten, Facebook; Martin Engelcke, University of Oxford",3D Semantic Segmentation with Submanifold Sparse Convolutional Networks,"Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard ""dense"" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.",http://arxiv.org/pdf/1711.10275v1
955,844,Spotlight,TOM-Net: Learning Transparent Object Matting from a Single Image,"Guanying Chen, The University of Hong Kong; Kai Han, ; Kwan-Yee Kenneth Wong, The University of Hong Kong",TOM-Net: Learning Transparent Object Matting from a Single Image,"This paper addresses the problem of transparent object matting. Existing image matting approaches for transparent objects often require tedious capturing procedures and long processing time, which limit their practical use. In this paper, we first formulate transparent object matting as a refractive flow estimation problem. We then propose a deep learning framework, called TOM-Net, for learning the refractive flow. Our framework comprises two parts, namely a multi-scale encoder-decoder network for producing a coarse prediction, and a residual network for refinement. At test time, TOM-Net takes a single image as input, and outputs a matte (consisting of an object mask, an attenuation mask and a refractive flow field) in a fast feed-forward pass. As no off-the-shelf dataset is available for transparent object matting, we create a large-scale synthetic dataset consisting of 158K images of transparent objects rendered in front of images sampled from the Microsoft COCO dataset. We also collect a real dataset consisting of 876 samples using 14 transparent objects and 60 background images. Promising experimental results have been achieved on both synthetic and real data, which clearly demonstrate the effectiveness of our approach.",http://arxiv.org/pdf/1803.04636v3
956,2821,Poster,Translating and Segmenting Multimodal Medical Volumes with Cycle- and Shape-Consistency Generative Adversarial Network,"Zizhao Zhang, University of Florida; Lin Yang, ; Yefeng Zheng, Simens",Translating and Segmenting Multimodal Medical Volumes with Cycle- and Shape-Consistency Generative Adversarial Network,"Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized computed tomography (CT) data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 3D images using unpaired training data, 2) ensuring consistent anatomical structures, which could be changed by geometric distortion in cross-modality synthesis and 3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss, which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively.",http://arxiv.org/pdf/1802.09655v1
957,3262,Poster,An Unsupervised Learning Model for Deformable Medical Image Registration,"Guha Balakrishnan, MIT; Adrian Dalca, ; Amy Zhao, MIT; Mert Sabuncu, Cornell; John Guttag,",An Unsupervised Learning Model for Deformable Medical Image Registration,"We present an efficient learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an energy function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a CNN, and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph .",http://arxiv.org/pdf/1802.02604v2
958,2986,Poster,Deep Lesion Graph in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database,"Ke Yan, National Institute of Health; Xiaosong Wang, NIH; Le Lu, Nvidia Corp; Ling Zhang, NIH; Adam Harrison, National Institutes of Health; MOHAMMADHADI Bagheri, NIH; Ronald Summers,",Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database,"Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals' picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. A large-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. We propose algorithms for intra-patient lesion matching and missing annotation mining. Experimental results validate their effectiveness.",http://arxiv.org/pdf/1711.10535v2
959,3035,Poster,Learning distributions of shape trajectories from longitudinal datasets: a hierarchical model on a manifold of diffeomorphisms,"Alexandre Bône, Brain and Spine Institute; Olivier Colliot, Institut du Cerveau et de la Moelle épinière; Stanley Durrleman, Institut du Cerveau et de la Moelle épinière",Learning distributions of shape trajectories from longitudinal datasets: a hierarchical model on a manifold of diffeomorphisms,"We propose a method to learn a distribution of shape trajectories from longitudinal data, i.e. the collection of individual objects repeatedly observed at multiple time-points. The method allows to compute an average spatiotemporal trajectory of shape changes at the group level, and the individual variations of this trajectory both in terms of geometry and time dynamics. First, we formulate a non-linear mixed-effects statistical model as the combination of a generic statistical model for manifold-valued longitudinal data, a deformation model defining shape trajectories via the action of a finite-dimensional set of diffeomorphisms with a manifold structure, and an efficient numerical scheme to compute parallel transport on this manifold. Second, we introduce a MCMC-SAEM algorithm with a specific approach to shape sampling, an adaptive scheme for proposal variances, and a log-likelihood tempering strategy to estimate our model. Third, we validate our algorithm on 2D simulated data, and then estimate a scenario of alteration of the shape of the hippocampus 3D brain structure during the course of Alzheimer's disease. The method shows for instance that hippocampal atrophy progresses more quickly in female subjects, and occurs earlier in APOE4 mutation carriers. We finally illustrate the potential of our method for classifying pathological trajectories versus normal ageing.",http://arxiv.org/pdf/1803.10119v1
960,3345,Poster,CNN Driven Sparse Multi-Level B-spline Image Registration,"Pingge Jiang, Drexel University; James Shackleford, Drexel University",,,
961,4335,Poster,Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation,"Adrian Dalca, ; John Guttag, ; Mert Sabuncu, Cornell",,,
962,4109,Poster,3D Registration of Curves and Surfaces using Local Differential Information,"Carolina Raposo, Institute of Systems and Robot; Joao Barreto, University of Coimbra, Portugal",3D Registration of Curves and Surfaces using Local Differential Information,"This article presents for the first time a global method for registering 3D curves with 3D surfaces without requiring an initialization. The algorithm works with 2-tuples point+vector that consist in pairs of points augmented with the information of their tangents or normals. A closed-form solution for determining the alignment transformation from a pair of matching 2-tuples is proposed. In addition, the set of necessary conditions for two 2-tuples to match is derived. This allows fast search of correspondences that are used in an hypothesise-and-test framework for accomplishing global registration. Comparative experiments demonstrate that the proposed algorithm is the first effective solution for curve vs surface registration, with the method achieving accurate alignment in situations of small overlap and large percentage of outliers in a fraction of a second. The proposed framework is extended to the cases of curve vs curve and surface vs surface registration, with the former being particularly relevant since it is also a largely unsolved problem.",http://arxiv.org/pdf/1804.00637v1
963,4238,Poster,Learning Representations for Single Cells in Microscopy Images,"Juan Caicedo, Broad Institute of Harvard and; Claire Mcquin, Broad Institute of Harvard and MIT; Allen Goodman, Broad Institute of Harvard and MIT; Shantanu Singh, Broad Institute of Harvard and MIT; Anne Carpenter, Broad Institute of Harvard and MIT",A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation,"Estimating count and density maps from crowd images has a wide range of applications such as video surveillance, traffic monitoring, public safety and urban planning. In addition, techniques developed for crowd counting can be applied to related tasks in other fields of study such as cell microscopy, vehicle counting and environmental survey. The task of crowd counting and density map estimation is riddled with many challenges such as occlusions, non-uniform density, intra-scene and inter-scene variations in scale and perspective. Nevertheless, over the last few years, crowd count analysis has evolved from earlier methods that are often limited to small variations in crowd density and scales to the current state-of-the-art methods that have developed the ability to perform successfully on a wide range of scenarios. The success of crowd counting methods in the recent years can be largely attributed to deep learning and publications of challenging datasets. In this paper, we provide a comprehensive survey of recent Convolutional Neural Network (CNN) based approaches that have demonstrated significant improvements over earlier methods that rely largely on hand-crafted representations. First, we briefly review the pioneering methods that use hand-crafted representations and then we delve in detail into the deep learning-based approaches and recently published datasets. Furthermore, we discuss the merits and drawbacks of existing CNN-based approaches and identify promising avenues of research in this rapidly evolving field.",http://arxiv.org/pdf/1707.01202v1
964,125,Poster,Guided Proofreading of Automatic Segmentations for Connectomics,"Daniel Haehn, Harvard University; Verena Kaynig, ; James Tompkin, Brown University; Jeff Lichtman, Harvard University; Hanspeter Pfister, Harvard University",Guided Proofreading of Automatic Segmentations for Connectomics,"Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.",http://arxiv.org/pdf/1704.00848v1
965,3989,Poster,Wide Compression: Tensor Ring Nets,"Wenqi Wang, Purdue University; YIfan Sun, Technicolor Research; Brian Eriksson, Adobe; Wenlin Wang, Duke University; Vaneet Aggarwal, Purdue University",Wide Compression: Tensor Ring Nets,"Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep neural networks. Our results show that our TR-Nets approach {is able to compress LeNet-5 by $11\times$ without losing accuracy}, and can compress the state-of-the-art Wide ResNet by $243\times$ with only 2.3\% degradation in {Cifar10 image classification}. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.",http://arxiv.org/pdf/1802.09052v1
966,3991,Poster,Improvements to context based self-supervised learning,"Terrell Mundhenk, LLNL; Daniel Ho, LLNL; Barry Chen, LLNL",Improvements to context based self-supervised learning,"We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and ""linear tests"" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci.llnl.gov/selfsupervised/.",http://arxiv.org/pdf/1711.06379v3
967,4149,Poster,Learning Structure and Strength of CNN Filters for Small Sample Size Training,"Rohit Keshari, IIIT Delhi; Mayank Vatsa, IIIT Dehli; Richa Singh, IIT Dehli; Afzel Noore, WVU",Learning Structure and Strength of CNN Filters for Small Sample Size Training,"Convolutional Neural Networks have provided state-of-the-art results in several computer vision problems. However, due to a large number of parameters in CNNs, they require a large number of training samples which is a limiting factor for small sample size problems. To address this limitation, we propose SSF-CNN which focuses on learning the structure and strength of filters. The structure of the filter is initialized using a dictionary-based filter learning algorithm and the strength of the filter is learned using the small sample training data. The architecture provides the flexibility of training with both small and large training databases and yields good accuracies even with small size training data. The effectiveness of the algorithm is first demonstrated on MNIST, CIFAR10, and NORB databases, with a varying number of training samples. The results show that SSF-CNN significantly reduces the number of parameters required for training while providing high accuracies the test databases. On small sample size problems such as newborn face recognition and Omniglot, it yields state-of-the-art results. Specifically, on the IIITD Newborn Face Database, the results demonstrate improvement in rank-1 identification accuracy by at least 10%.",http://arxiv.org/pdf/1803.11405v1
968,1024,Poster,Boosting Self-Supervised Learning via Knowledge Transfer,"Mehdi Noroozi, University of Bern; Ananthachari Kavalkazhani Vinjimoor, UMBC; Hamed Pirsiavash, ; Paolo Favaro, Bern University, Switzerland",,,
969,1487,Poster,The power of ensembles for active learning in image classification,"William Beluch, Bosch Center for Artificial Intelligence; Tim Genewein, Robert Bosch Center for AI; Andreas Nürnberger, Otto-von-Guericke-Universität Magdeburg ; Jan Köhler, Bosch Center for AI",,,
970,1720,Poster,Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition,"Jinmian Ye, University of Electronic Science and Technology of China; Linnan Wang, Brown; Guangxi Li, UESTC; Di Chen, ; Shandian Zhe, School of Computing, University of Utah; Zenglin Xu, University of Electronic Science and Technology of China",Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition,"Recurrent Neural Networks (RNNs) are powerful sequence modeling tools. However, when dealing with high dimensional inputs, the training of RNNs becomes computational expensive due to the large number of model parameters. This hinders RNNs from solving many important computer vision tasks, such as Action Recognition in Videos and Image Captioning. To overcome this problem, we propose a compact and flexible structure, namely Block-Term tensor decomposition, which greatly reduces the parameters of RNNs and improves their training efficiency. Compared with alternative low-rank approximations, such as tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only more concise (when using the same rank), but also able to attain a better approximation to the original RNNs with much fewer parameters. On three challenging tasks, including Action Recognition in Videos, Image Captioning and Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes 17,388 times fewer parameters than the standard LSTM to achieve an accuracy improvement over 15.6\% in the Action Recognition task on the UCF11 dataset.",http://arxiv.org/pdf/1712.05134v1
971,1931,Poster,Spatially-Adaptive Filter Units for Deep Neural Networks,"Domen Tabernik, University of Ljubljana; Matej Kristan, University of Ljubljana; Ales Leonardis, University of Birmingham, UK",Spatially-Adaptive Filter Units for Deep Neural Networks,"Classical deep convolutional networks increase receptive field size by either gradual resolution reduction or application of hand-crafted dilated convolutions to prevent increase in the number of parameters. In this paper we propose a novel displaced aggregation unit (DAU) that does not require hand-crafting. In contrast to classical filters with units (pixels) placed on a fixed regular grid, the displacement of the DAUs are learned, which enables filters to spatially-adapt their receptive field to a given problem. We extensively demonstrate the strength of DAUs on a classification and semantic segmentation tasks. Compared to ConvNets with regular filter, ConvNets with DAUs achieve comparable performance at faster convergence and up to 3-times reduction in parameters. Furthermore, DAUs allow us to study deep networks from novel perspectives. We study spatial distributions of DAU filters and analyze the number of parameters allocated for spatial coverage in a filter.",http://arxiv.org/pdf/1711.11473v2
972,2201,Poster,SO-Net: Self-Organizing Network for Point Cloud Analysis,"Jiaxin Li, National University of Singapore; Ben Chen, National Univ of Singapore; Gim Hee Lee, National University of SIngapore",SO-Net: Self-Organizing Network for Point Cloud Analysis,"This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website. https://github.com/lijx10/SO-Net",http://arxiv.org/pdf/1803.04249v4
973,2237,Poster,SGAN: An Alternative Training of Generative Adversarial Networks,"Tatjana Chavdarova, Idiap and EPFL; Francois Fleuret, Idiap Research Institute",SGAN: An Alternative Training of Generative Adversarial Networks,"The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them.   We consider an alternative training process, named SGAN, in which several adversarial ""local"" pairs of networks are trained independently so that a ""global"" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones.   The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent.   Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.",http://arxiv.org/pdf/1712.02330v1
974,2335,Poster,SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis,"Wengling Chen, Georgia Institute of Technolog; James Hays, Georgia Tech",SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis,"Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or require a database to retrieve images from. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes realistic looking images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new building block suit for both the generator and discriminator which improves the information flow and utilizes input images at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.",http://arxiv.org/pdf/1801.02753v1
975,2383,Poster,Explicit Loss-Error-Aware Quantization for Deep Neural Networks,"Aojun Zhou, Intel labs china; Anbang Yao,",,,
976,2624,Poster,Towards Universal Representation for Unseen Action Recognition,"Yi Zhu, University of California Merced; Yang Long, Newcastle University; Yu Guan, Newcastle University; Shawn Newsam, ; Ling Shao, University of East Anglia",Towards Universal Representation for Unseen Action Recognition,"Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover 'building-blocks' from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks.",http://arxiv.org/pdf/1803.08460v1
977,2711,Poster,Deep Image Prior,"Dmitry Ulyanov, Skoltech; Andrea Vedaldi, U Oxford; Victor Lempitsky,",Deep Image Prior,"Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.   Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep_image_prior .",http://arxiv.org/pdf/1711.10925v3
978,380,Poster,ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing,"Chen-Hsuan Lin, CMU; Ersin Yumer, Argo AI; Oliver Wang, Adobe; Eli Shechtman, Adobe Research; Simon Lucey,",ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing,"We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.",http://arxiv.org/pdf/1803.01837v1
979,2205,Poster,CartoonGAN: Generative Adversarial Networks for Photo Cartoonization,"Yang Chen, Tsinghua University; Yu-Kun Lai, Cardiff University; Yong-Jin Liu,",,,
