Date,Time,Location,#,Session,Session Title,Paper ID,Paper Title,Authors,Title-arxiv,Abstract,URL
"Saturday, July 22, 2017",0900â€“1030,Kamehameha III,1,Spotlight 1-1A,Machine Learning 1,305,Exclusivity-Consistency Regularized Multi-View Subspace Clustering,"Xiaobo Wang, Xiaojie Guo, Zhen Lei, Changqing Zhang, Stan Z. Li",,,
,,,,,,373,Borrowing Treasures From the Wealthy: Deep Transfer Learning Through Selective Joint Fine-Tuning,"Weifeng Ge, Yizhou Yu",Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning,"Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.   Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model.",http://arxiv.org/pdf/1702.08690v2
,,,,,,968,The More You Know: Using Knowledge Graphs for Image Classification,"Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta",The More You Know: Using Knowledge Graphs for Image Classification,"One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.",http://arxiv.org/pdf/1612.04844v2
,,,,,,1358,Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,"Martin Simonovsky, Nikos Komodakis",Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,"A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.",http://arxiv.org/pdf/1704.02901v1
,,,,,,2704,Convolutional Neural Network Architecture for Geometric Matching,"Ignacio Rocco, Relja ArandjeloviÄ‡, Josef Sivic",Convolutional neural network architecture for geometric matching,"We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.",http://arxiv.org/pdf/1703.05593v2
,,,,,,2715,Deep Affordance-Grounded Sensorimotor Object Recognition,"Spyridon Thermos, Georgios Th. Papadopoulos, Petros Daras, Gerasimos Potamianos",Deep Affordance-grounded Sensorimotor Object Recognition,"It is well-established by cognitive neuroscience that human perception of objects constitutes a complex process, where object appearance information is combined with evidence about the so-called object ""affordances"", namely the types of actions that humans typically perform when interacting with them. This fact has recently motivated the ""sensorimotor"" approach to the challenging task of automatic object recognition, where both information sources are fused to improve robustness. In this work, the aforementioned paradigm is adopted, surpassing current limitations of sensorimotor object recognition research. Specifically, the deep learning paradigm is introduced to the problem for the first time, developing a number of novel neuro-biologically and neuro-physiologically inspired architectures that utilize state-of-the-art neural networks for fusing the available information sources in multiple ways. The proposed methods are evaluated using a large RGB-D corpus, which is specifically collected for the task of sensorimotor object recognition and is made publicly available. Experimental results demonstrate the utility of affordance information to object recognition, achieving an up to 29% relative error reduction by its inclusion.",http://arxiv.org/pdf/1704.02787v1
,,,,,,3235,Discovering Causal Signals in Images,"David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard SchÃ¶lkopf, LÃ©on Bottou",Discovering Causal Signals in Images,"The purpose of this paper is to point out and assay observable causal signals within collections of static images. We achieve this goal in two steps. First, we take a learning approach to observational causal inference, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, when given samples from their joint distribution. Second, we use our causal direction finder to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of (1) a relation between the direction of causality and the difference between objects and their contexts, and (2) observable causal signals in collections of static images.",http://arxiv.org/pdf/1605.08179v1
,,,,,,3653,On Compressing Deep Models by Low Rank and Sparse Decomposition,"Xiyu Yu, Tongliang Liu, Xinchao Wang, Dacheng Tao",,,
,,,,Oral 1-1A,,201,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,"Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas",PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",http://arxiv.org/pdf/1612.00593v2
,,,,,,649,Universal Adversarial Perturbations,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard",Universal Adversarial Perturbations Against Semantic Image Segmentation,"While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.",http://arxiv.org/pdf/1704.05712v1
,,,,,,1385,Unsupervised Pixel-Level Domain Adaptation With Generative Adversarial Networks,"Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan",Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks,"Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",http://arxiv.org/pdf/1612.05424v1
,,,,,,1948,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"Christian Ledig, Lucas Theis, Ferenc HuszÃ¡r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",http://arxiv.org/pdf/1609.04802v5
"Saturday, July 22, 2017","0900â€“1030","KalÄÅkaua Ballroom A-B",2,"Spotlight 1-1B","3D Vision 1",147,Global Hypothesis Generation for 6D Object Pose Estimation,"Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull, Stefan Gumhold, Bogdan Savchynskyy, Carsten Rother",Global Hypothesis Generation for 6D Object Pose Estimation,"This paper addresses the task of estimating the 6D pose of a known 3D object from a single RGB-D image. Most modern approaches solve this task in three steps: i) Compute local features; ii) Generate a pool of pose-hypotheses; iii) Select and refine a pose from the pool. This work focuses on the second step. While all existing approaches generate the hypotheses pool via local reasoning, e.g. RANSAC or Hough-voting, we are the first to show that global reasoning is beneficial at this stage. In particular, we formulate a novel fully-connected Conditional Random Field (CRF) that outputs a very small number of pose-hypotheses. Despite the potential functions of the CRF being non-Gaussian, we give a new and efficient two-step optimization procedure, with some guarantees for optimality. We utilize our global hypotheses generation procedure to produce results that exceed state-of-the-art for the challenging ""Occluded Object Dataset"".",http://arxiv.org/pdf/1612.02287v3
,,,,,,380,A Practical Method for Fully Automatic Intrinsic Camera Calibration Using Directionally Encoded Light,"Mahdi Abbaspour Tehrani, Thabo Beeler, Anselm GrundhÃ¶fer",,,
,,,,,,1075,CATS: A Color and Thermal Stereo Benchmark,"Wayne Treible, Philip Saponaro, Scott Sorensen, Abhishek Kolagunda, Michael O'Neal, Brian Phelan, Kelly Sherbondy, Chandra Kambhamettu",,,
,,,,,,1248,Elastic Shape-From-Template With Spatially Sparse Deforming Forces,"Abed Malti, CÃ©dric Herzet",,,
,,,,,,1473,Distinguishing the Indistinguishable: Exploring Structural Ambiguities via Geodesic Context,"Qingan Yan, Long Yang, Ling Zhang, Chunxia Xiao",,,
,,,,,,2250,Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation,"Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe",Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation,"This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multi- scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through extensive experimental evaluation we demonstrate the effective- ness of the proposed approach and establish new state of the art results on publicly available datasets.",http://arxiv.org/pdf/1704.02157v1
,,,,,,2685,Dynamic Time-Of-Flight,"Michael Schober, Amit Adam, Omer Yair, Shai Mazor, Sebastian Nowozin",Design and Implementation of Flight Visual Simulation System,"The design requirement for flight visual simulation system is studied and the overall structure and development process are proposed in this paper. Through the construction of 3D scene model library and aircraft model, the rendering and interaction of visual scene are implemented. The changes of aircraft flight attitude in visual system are controlled by real-time calculation of aircraft aerodynamic and dynamic equations and flight simulation effect is enhanced by this kind of control. Several key techniques for optimizing 3D model and relative methods for large terrain modeling are explored for improving loading ability and rendering speed of the system. Experiment shows that, with specific function and performance guaranteed as a premise, the system achieves expected results, that is, precise real-time calculation of flight attitude and smooth realistic screen effect.",http://arxiv.org/pdf/1212.0365v1
,,,,,,2827,Training Object Class Detectors With Click Supervision,"Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari",Training object class detectors with click supervision,"Training object class detectors typically requires a large set of images with objects annotated by bounding boxes. However, manually drawing bounding boxes is very time consuming. In this paper we greatly reduce annotation time by proposing center-click annotations: we ask annotators to click on the center of an imaginary bounding box which tightly encloses the object instance. We then incorporate these clicks into existing Multiple Instance Learning techniques for weakly supervised object localization, to jointly localize object bounding boxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS COCO show that: (1) our scheme delivers high-quality detectors, performing substantially better than those produced by weakly supervised techniques, with a modest extra annotation effort; (2) these detectors in fact perform in a range close to those trained from manually drawn bounding boxes; (3) as the center-click task is very fast, our scheme reduces total annotation time by 9x to 18x.",http://arxiv.org/pdf/1704.06189v2
,,,,Oral 1-1B,,646,Semantic Scene Completion From a Single Depth Image,"Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, Thomas Funkhouser",Semantic Scene Completion from a Single Depth Image,"This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.",http://arxiv.org/pdf/1611.08974v1
,,,,,,668,3DMatch: Learning Local Geometric Descriptors From RGB-D Reconstructions,"Andy Zeng, Shuran Song, Matthias NieÃŸner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser",,,
,,,,,,950,Multi-View Supervision for Single-View Reconstruction via Differentiable Ray Consistency,"Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, Jitendra Malik",Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency,"We study the notion of consistency between a 3D shape and a 2D observation and propose a differentiable formulation which allows computing gradients of the 3D shape given an observation from an arbitrary view. We do so by reformulating view consistency using a differentiable ray consistency (DRC) term. We show that this formulation can be incorporated in a learning framework to leverage different types of multi-view observations e.g. foreground masks, depth, color images, semantics etc. as supervision for learning single-view 3D prediction. We present empirical analysis of our technique in a controlled setting. We also show that this approach allows us to improve over existing techniques for single-view reconstruction of objects from the PASCAL VOC dataset.",http://arxiv.org/pdf/1704.06254v1
,,,,,,1821,On-The-Fly Adaptation of Regression Forests for Online Camera Relocalisation,"Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, Philip H. S. Torr",On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation,"Camera relocalisation is a key problem in computer vision, with applications as diverse as simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation.",http://arxiv.org/pdf/1702.02779v1
"Saturday, July 22, 2017",0900â€“1030,KalÄÅkaua Ballroom C,3,Spotlight 1-1C,Low- & Mid-Level Vision,23,Designing Effective Inter-Pixel Information Flow for Natural Image Matting,"YaÄŸiz Aksoy, TunÃ§ Ozan Aydin, Marc Pollefeys",,,
,,,,,,460,Deep Video Deblurring for Hand-Held Cameras,"Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, Oliver Wang",Deep Video Deblurring,"Motion blur from camera shake is a major problem in videos captured by hand-held devices. Unlike single-image deblurring, video-based approaches can take advantage of the abundant information that exists across neighboring frames. As a result the best performing methods rely on aligning nearby frames. However, aligning images is a computationally expensive and fragile procedure, and methods that aggregate information must therefore be able to identify which regions have been accurately aligned and which have not, a task which requires high level scene understanding. In this work, we introduce a deep learning solution to video deblurring, where a CNN is trained end-to-end to learn how to accumulate information across frames. To train this network, we collected a dataset of real videos recorded with a high framerate camera, which we use to generate synthetic motion blur for supervision. We show that the features learned from this dataset extend to deblurring motion blur that arises due to camera shake in a wide range of videos, and compare the quality of results to a number of other baselines.",http://arxiv.org/pdf/1611.08387v1
,,,,,,877,Instance-Level Salient Object Segmentation,"Guanbin Li, Yuan Xie, Liang Lin, Yizhou Yu",Instance-Level Salient Object Segmentation,"Image saliency detection has recently witnessed rapid progress due to deep convolutional neural networks. However, none of the existing methods is able to identify object instances in the detected salient regions. In this paper, we present a salient instance segmentation method that produces a saliency mask with distinct object instance labels for an input image. Our method consists of three steps, estimating saliency map, detecting salient object contours and identifying salient object instances. For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours. Once integrated with multiscale combinatorial grouping and a MAP-based subset optimization framework, our method can generate very promising salient object instance segmentation results. To promote further research and evaluation of salient instance segmentation, we also construct a new database of 1000 images and their pixelwise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks for salient region detection as well as on our new dataset for salient instance segmentation.",http://arxiv.org/pdf/1704.03604v1
,,,,,,1509,Deep Multi-Scale Convolutional Neural Network for Dynamic Scene Deblurring,"Seungjun Nah, Tae Hyun Kim, Kyoung Mu Lee",Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring,"Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem since blurs are caused by camera shake, scene depth as well as multiple object motions. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores blurred images caused by various sources in an end-to-end manner. Furthermore, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Moreover, we propose a new large scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.",http://arxiv.org/pdf/1612.02177v1
,,,,,,1532,Diversified Texture Synthesis With Feed-Forward Networks,"Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang",Diversified Texture Synthesis with Feed-forward Networks,"Recent progresses on deep discriminative and generative modeling have shown promising results on texture synthesis. However, existing feed-forward based methods trade off generality for efficiency, which suffer from many issues, such as shortage of generality (i.e., build one network per texture), lack of diversity (i.e., always produce visually identical output) and suboptimality (i.e., generate less satisfying visual effects). In this work, we focus on solving these issues for improved texture synthesis. We propose a deep generative feed-forward network which enables efficient synthesis of multiple textures within one single network and meaningful interpolation between them. Meanwhile, a suite of important techniques are introduced to achieve better convergence and diversity. With extensive experiments, we demonstrate the effectiveness of the proposed model and techniques for synthesizing a large number of textures and show its applications with the stylization.",http://arxiv.org/pdf/1703.01664v1
,,,,,,1969,Radiometric Calibration for Internet Photo Collections,"Zhipeng Mo, Boxin Shi, Sai-Kit Yeung, Yasuyuki Matsushita",,,
,,,,,,2866,Deeply Aggregated Alternating Minimization for Image Restoration,"Youngjung Kim, Hyungjoo Jung, Dongbo Min, Kwanghoon Sohn",Deeply Aggregated Alternating Minimization for Image Restoration,"Regularization-based image restoration has remained an active research topic in computer vision and image processing. It often leverages a guidance signal captured in different fields as an additional cue. In this work, we present a general framework for image restoration, called deeply aggregated alternating minimization (DeepAM). We propose to train deep neural network to advance two of the steps in the conventional AM algorithm: proximal mapping and ?- continuation. Both steps are learned from a large dataset in an end-to-end manner. The proposed framework enables the convolutional neural networks (CNNs) to operate as a prior or regularizer in the AM algorithm. We show that our learned regularizer via deep aggregation outperforms the recent data-driven approaches as well as the nonlocalbased methods. The flexibility and effectiveness of our framework are demonstrated in several image restoration tasks, including single image denoising, RGB-NIR restoration, and depth super-resolution.",http://arxiv.org/pdf/1612.06508v1
,,,,,,3051,End-To-End Instance Segmentation With Recurrent Attention,"Mengye Ren, Richard S. Zemel",End-to-End Instance Segmentation with Recurrent Attention,"While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves competitive results on the CVPPP, KITTI, and Cityscapes datasets.",http://arxiv.org/pdf/1605.09410v4
,,,,Oral 1-1C,,369,SRN: Side-output Residual Network for Object Symmetry Detection in the Wild,"Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye",SRN: Side-output Residual Network for Object Symmetry Detection in the Wild,"In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry groundtruth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to realworld images, and the state-of-the-art performance of our symmetry detection approach. The benchmark and the code for SRN are publicly available at https://github.com/KevinKecc/SRN.",http://arxiv.org/pdf/1703.02243v2
,,,,,,1081,Deep Image Matting,"Ning Xu, Brian Price, Scott Cohen, Thomas Huang",Deep Image Matting,"Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.",http://arxiv.org/pdf/1703.03872v3
,,,,,,1574,Wetness and Color From a Single Multispectral Image,"Mihoko Shimano, Hiroki Okawa, Yuta Asano, Ryoma Bise, Ko Nishino, Imari Sato",,,
,,,,,,1641,FC4: Fully Convolutional Color Constancy With Confidence-Weighted Pooling,"Yuanming Hu, Baoyuan Wang, Stephen Lin",,,
"Saturday, July 22, 2017",1030â€“1230,Kamehameha I,4,Poster 1-1,3D Computer Vision,24,Face Normals â€œIn-The-Wildâ€ù Using Fully Convolutional Networks,"George Trigeorgis, Patrick Snape, Iasonas Kokkinos, Stefanos Zafeiriou",,,
,,,,,,39,A Non-Convex Variational Approach to Photometric Stereo Under Inaccurate Lighting,"Yvain QuÃ©au, Tao Wu, FranÃ§ois Lauze, Jean-Denis Durou, Daniel Cremers",,,
,,,,,,179,A Linear Extrinsic Calibration of Kaleidoscopic Imaging System From Single 3D Point,"Kosuke Takahashi, Akihiro Miyata, Shohei Nobuhara, Takashi Matsuyama",A Linear Extrinsic Calibration of Kaleidoscopic Imaging System from Single 3D Point,"This paper proposes a new extrinsic calibration of kaleidoscopic imaging system by estimating normals and distances of the mirrors. The problem to be solved in this paper is a simultaneous estimation of all mirror parameters consistent throughout multiple reflections. Unlike conventional methods utilizing a pair of direct and mirrored images of a reference 3D object to estimate the parameters on a per-mirror basis, our method renders the simultaneous estimation problem into solving a linear set of equations. The key contribution of this paper is to introduce a linear estimation of multiple mirror parameters from kaleidoscopic 2D projections of a single 3D point of unknown geometry. Evaluations with synthesized and real images demonstrate the performance of the proposed algorithm in comparison with conventional methods.",http://arxiv.org/pdf/1703.02826v3
,,,,,,579,Polarimetric Multi-View Stereo,"Zhaopeng Cui, Jinwei Gu, Boxin Shi, Ping Tan, Jan Kautz",,,
,,,,,,702,An Exact Penalty Method for Locally Convergent Maximum Consensus,"Huu Le, Tat-Jun Chin, David Suter",,,
,,,,,,2317,Deep Supervision With Shape Concepts for Occlusion-Aware 3D Object Parsing,"Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D. Hager, Manmohan Chandraker",Deep Supervision with Shape Concepts for Occlusion-Aware 3D Object Parsing,"Monocular 3D object parsing is highly desirable in various scenarios including occlusion reasoning and holistic scene interpretation. We present a deep convolutional neural network (CNN) architecture to localize semantic parts in 2D image and 3D space while inferring their visibility states, given a single RGB image. Our key insight is to exploit domain knowledge to regularize the network by deeply supervising its hidden layers, in order to sequentially infer intermediate concepts associated with the final task. To acquire training data in desired quantities with ground truth 3D shape and relevant concepts, we render 3D object CAD models to generate large-scale synthetic data and simulate challenging occlusion configurations between objects. We train the network only on synthetic data and demonstrate state-of-the-art performances on real image benchmarks including an extended version of KITTI, PASCAL VOC, PASCAL3D+ and IKEA for 2D and 3D keypoint localization and instance segmentation. The empirical results substantiate the utility of our deep supervision scheme by demonstrating effective transfer of knowledge from synthetic data to real images, resulting in less overfitting compared to standard end-to-end training.",http://arxiv.org/pdf/1612.02699v3
,,,,,,2455,Amodal Detection of 3D Objects: Inferring 3D Bounding Boxes From 2D Ones in RGB-Depth Images,"Zhuo Deng, Longin Jan Latecki",,,
,,,,,Analyzing Humans in Images,143,Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition and Detection,"Guillermo Garcia-Hernando, Tae-Kyun Kim",Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition and Detection,"A human action can be seen as transitions between one's body poses over time, where the transition depicts a temporal relation between two poses. Recognizing actions thus involves learning a classifier sensitive to these pose transitions as well as to static poses. In this paper, we introduce a novel method called transitions forests, an ensemble of decision trees that both learn to discriminate static poses and transitions between pairs of two independent frames. During training, node splitting is driven by alternating two criteria: the standard classification objective that maximizes the discrimination power in individual frames, and the proposed one in pairwise frame transitions. Growing the trees tends to group frames that have similar associated transitions and share same action label incorporating temporal information that was not available otherwise. Unlike conventional decision trees where the best split in a node is determined independently of other nodes, the transition forests try to find the best split of nodes jointly (within a layer) for incorporating distant node transitions. When inferring the class label of a new frame, it is passed down the trees and the prediction is made based on previous frame predictions and the current one in an efficient and online manner. We apply our method on varied skeleton action recognition and online detection datasets showing its suitability over several baselines and state-of-the-art approaches.",http://arxiv.org/pdf/1607.02737v3
,,,,,,189,Scene Flow to Action Map: A New Representation for RGB-D Based Action Recognition With Convolutional Neural Networks,"Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang, Chang Tang, Philip Ogunbona",,,
,,,,,,970,Detecting Masked Faces in the Wild With LLE-CNNs,"Shiming Ge, Jia Li, Qiting Ye, Zhao Luo",,,
,,,,,,1289,A Domain Based Approach to Social Relation Recognition,"Qianru Sun, Bernt Schiele, Mario Fritz",A Domain Based Approach to Social Relation Recognition,"Social relations are the foundation of human daily life. Developing techniques to analyze such relations from visual data bears great potential to build machines that better understand us and are capable of interacting with us at a social level. Previous investigations have remained partial due to the overwhelming diversity and complexity of the topic and consequently have only focused on a handful of social relations. In this paper, we argue that the domain-based theory from social psychology is a great starting point to systematically approach this problem. The theory provides coverage of all aspects of social relations and equally is concrete and predictive about the visual attributes and behaviors defining the relations included in each domain. We provide the first dataset built on this holistic conceptualization of social life that is composed of a hierarchical label space of social domains and social relations. We also contribute the first models to recognize such domains and relations and find superior performance for attribute based features. Beyond the encouraging performance of the attribute based approach, we also find interpretable features that are in accordance with the predictions from social psychology literature. Beyond our findings, we believe that our contributions more tightly interleave visual recognition and social psychology theory that has the potential to complement the theoretical work in the area with empirical and data-driven models of social life.",http://arxiv.org/pdf/1704.06456v1
,,,,,,1698,Spatio-Temporal Naive-Bayes Nearest-Neighbor (ST-NBNN) for Skeleton-Based Action Recognition,"Junwu Weng, Chaoqun Weng, Junsong Yuan",,,
,,,,,,2928,Personalizing Gesture Recognition Using Hierarchical Bayesian Neural Networks,"Ajjen Joshi, Soumya Ghosh, Margrit Betke, Stan Sclaroff, Hanspeter Pfister",,,
,,,,,Applications,237,Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core,"Wadim Kehl, Federico Tombari, Slobodan Ilic, Nassir Navab",,,
,,,,,,1304,Multi-Scale FCN With Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting in the Wild,"Dafang He, Xiao Yang, Chen Liang, Zihan Zhou, Alexander G. Ororbi II, Daniel Kifer, C. Lee Giles",,,
,,,,,,2662,Viraliency: Pooling Local Virality,"Xavier Alameda-Pineda, Andrea Pilzer, Dan Xu, Nicu Sebe, Elisa Ricci",Viraliency: Pooling Local Virality,"In our overly-connected world, the automatic recognition of virality - the quality of an image or video to be rapidly and widely spread in social networks - is of crucial importance, and has recently awaken the interest of the computer vision community. Concurrently, recent progress in deep learning architectures showed that global pooling strategies allow the extraction of activation maps, which highlight the parts of the image most likely to contain instances of a certain class. We extend this concept by introducing a pooling layer that learns the size of the support area to be averaged: the learned top-N average (LENA) pooling. We hypothesize that the latent concepts (feature maps) describing virality may require such a rich pooling strategy. We assess the effectiveness of the LENA layer by appending it on top of a convolutional siamese architecture and evaluate its performance on the task of predicting and localizing virality. We report experiments on two publicly available datasets annotated for virality and show that our method outperforms state-of-the-art approaches.",http://arxiv.org/pdf/1703.03937v2
,,,,,Biomedical Image/Video Analysis,2395,A Non-Local Low-Rank Framework for Ultrasound Speckle Reduction,"Lei Zhu, Chi-Wing Fu, Michael S. Brown, Pheng-Ann Heng",,,
,,,,,Image Motion & Tracking,162,Video Acceleration Magnification,"Yichao Zhang, Silvia L. Pintea, Jan C. van Gemert",Video Acceleration Magnification,"The ability to amplify or reduce subtle image changes over time is useful in contexts such as video editing, medical video analysis, product quality control and sports. In these contexts there is often large motion present which severely distorts current video amplification methods that magnify change linearly. In this work we propose a method to cope with large motions while still magnifying small changes. We make the following two observations: i) large motions are linear on the temporal scale of the small changes; ii) small changes deviate from this linearity. We ignore linear motion and propose to magnify acceleration. Our method is pure Eulerian and does not require any optical flow, temporal alignment or region annotations. We link temporal second-order derivative filtering to spatial acceleration magnification. We apply our method to moving objects where we show motion magnification and color magnification. We provide quantitative as well as qualitative evidence for our method while comparing to the state-of-the-art.",http://arxiv.org/pdf/1704.04186v2
,,,,,,670,Superpixel-Based Tracking-By-Segmentation Using Markov Chains,"Donghun Yeo, Jeany Son, Bohyung Han, Joon Hee Han",,,
,,,,,,1251,BranchOut: Regularization for Online Ensemble Tracking With Convolutional Neural Networks,"Bohyung Han, Jack Sim, Hartwig Adam",,,
,,,,,,1257,Learning Motion Patterns in Videos,"Pavel Tokmakov, Karteek Alahari, Cordelia Schmid",Video Primal Sketch: A Unified Middle-Level Representation for Video,"This paper presents a middle-level video representation named Video Primal Sketch (VPS), which integrates two regimes of models: i) sparse coding model using static or moving primitives to explicitly represent moving corners, lines, feature points, etc., ii) FRAME /MRF model reproducing feature statistics extracted from input video to implicitly represent textured motion, such as water and fire. The feature statistics include histograms of spatio-temporal filters and velocity distributions. This paper makes three contributions to the literature: i) Learning a dictionary of video primitives using parametric generative models; ii) Proposing the Spatio-Temporal FRAME (ST-FRAME) and Motion-Appearance FRAME (MA-FRAME) models for modeling and synthesizing textured motion; and iii) Developing a parsimonious hybrid model for generic video representation. Given an input video, VPS selects the proper models automatically for different motion patterns and is compatible with high-level action representations. In the experiments, we synthesize a number of textured motion; reconstruct real videos using the VPS; report a series of human perception experiments to verify the quality of reconstructed videos; demonstrate how the VPS changes over the scale transition in videos; and present the close connection between VPS and high-level action models.",http://arxiv.org/pdf/1502.02965v1
,,,,,Low- & Mid-Level Vision,839,Deep Level Sets for Salient Object Detection,"Ping Hu, Bing Shuai, Jun Liu, Gang Wang",DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection,"A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network (FCNN) with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing properties of salient object detection with great feature redundancy reduction. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.",http://arxiv.org/pdf/1510.05484v2
,,,,,,1789,Binary Constraint Preserving Graph Matching,"Bo Jiang, Jin Tang, Chris Ding, Bin Luo",,,
,,,,,,1818,From Local to Global: Edge Profiles to Camera Motion in Blurred Images,"Subeesh Vasu, A. N. Rajagopalan",,,
,,,,,,2033,What Is the Space of Attenuation Coefficients in Underwater Computer Vision?,"Derya Akkaynak, Tali Treibitz, Tom Shlesinger, Yossi Loya, Raz Tamir, David Iluz",,,
,,,,,,2360,Robust Energy Minimization for BRDF-Invariant Shape From Light Fields,"Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Manmohan Chandraker",,,
,,,,,,2429,Boundary-Aware Instance Segmentation,"Zeeshan Hayder, Xuming He, Mathieu Salzmann",Boundary-aware Instance Segmentation,"We address the problem of instance-level semantic segmentation, which aims at jointly detecting, segmenting and classifying every individual object in an image. In this context, existing methods typically propose candidate objects, usually as bounding boxes, and directly predict a binary mask within each such proposal. As a consequence, they cannot recover from errors in the object candidate generation process, such as too small or shifted boxes.   In this paper, we introduce a novel object segment representation based on the distance transform of the object masks. We then design an object mask network (OMN) with a new residual-deconvolution architecture that infers such a representation and decodes it into the final binary object mask. This allows us to predict masks that go beyond the scope of the bounding boxes and are thus robust to inaccurate object candidates. We integrate our OMN into a Multitask Network Cascade framework, and learn the resulting boundary-aware instance segmentation (BAIS) network in an end-to-end manner. Our experiments on the PASCAL VOC 2012 and the Cityscapes datasets demonstrate the benefits of our approach, which outperforms the state-of-the-art in both object proposal generation and instance segmentation.",http://arxiv.org/pdf/1612.03129v2
,,,,,,2497,Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes,"S. Alireza Golestaneh, Lina J. Karam",Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes,"The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address the blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.",http://arxiv.org/pdf/1703.07478v3
,,,,,,2606,Model-Based Iterative Restoration for Binary Document Image Compression With Dictionary Learning,"Yandong Guo, Cheng Lu, Jan P. Allebach, Charles A. Bouman",Model-based Iterative Restoration for Binary Document Image Compression with Dictionary Learning,"The inherent noise in the observed (e.g., scanned) binary document image degrades the image quality and harms the compression ratio through breaking the pattern repentance and adding entropy to the document images. In this paper, we design a cost function in Bayesian framework with dictionary learning. Minimizing our cost function produces a restored image which has better quality than that of the observed noisy image, and a dictionary for representing and encoding the image. After the restoration, we use this dictionary (from the same cost function) to encode the restored image following the symbol-dictionary framework by JBIG2 standard with the lossless mode. Experimental results with a variety of document images demonstrate that our method improves the image quality compared with the observed image, and simultaneously improves the compression ratio. For the test images with synthetic noise, our method reduces the number of flipped pixels by 48.2% and improves the compression ratio by 36.36% as compared with the best encoding methods. For the test images with real noise, our method visually improves the image quality, and outperforms the cutting-edge method by 28.27% in terms of the compression ratio.",http://arxiv.org/pdf/1704.07019v1
,,,,,,2965,FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence,"Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin, Kwanghoon Sohn",FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence,"We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks.",http://arxiv.org/pdf/1702.00926v1
,,,,,Machine Learning,38,Learning by Association â€” A Versatile Semi-Supervised Training Method for Neural Networks,"Philip Haeusser, Alexander Mordvintsev, Daniel Cremers",FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence,"We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks.",http://arxiv.org/pdf/1702.00926v1
,,,,,,153,Dilated Residual Networks,"Fisher Yu, Vladlen Koltun, Thomas Funkhouser",Dilated Residual Networks,"Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model's depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (`degridding'), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.",http://arxiv.org/pdf/1705.09914v1
,,,,,,367,Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction,"Richard Zhang, Phillip Isola, Alexei A. Efros",Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction,"We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.",http://arxiv.org/pdf/1611.09842v3
,,,,,,755,Nonnegative Matrix Underapproximation for Robust Multiple Model Fitting,"Mariano Tepper, Guillermo Sapiro",Nonnegative Matrix Underapproximation for Robust Multiple Model Fitting,"In this work, we introduce a highly efficient algorithm to address the nonnegative matrix underapproximation (NMU) problem, i.e., nonnegative matrix factorization (NMF) with an additional underapproximation constraint. NMU results are interesting as, compared to traditional NMF, they present additional sparsity and part-based behavior, explaining unique data features. To show these features in practice, we first present an application to the analysis of climate data. We then present an NMU-based algorithm to robustly fit multiple parametric models to a dataset. The proposed approach delivers state-of-the-art results for the estimation of multiple fundamental matrices and homographies, outperforming other alternatives in the literature and exemplifying the use of efficient NMU computations.",http://arxiv.org/pdf/1611.01408v5
,,,,,,875,Truncated Max-Of-Convex Models,"Pankaj Pansari, M. Pawan Kumar",Truncated Max-of-Convex Models,"Truncated convex models (TCM) are a special case of pairwise random fields that have been widely used in computer vision. However, by restricting the order of the potentials to be at most two, they fail to capture useful image statistics. We propose a natural generalization of TCM to high-order random fields, which we call truncated max-of-convex models (TMCM). The energy function of TMCM consistsof two types of potentials: (i) unary potential, which has no restriction on its form; and (ii) clique potential, which is the sum of the m largest truncated convex distances over all label pairs in a clique. The use of a convex distance function encourages smoothness, while truncation allows for discontinuities in the labeling. By using m > 1, TMCM provides robustness towards errors in the definition of the cliques. In order to minimize the energy function of a TMCM over all possible labelings, we design an efficient st-MINCUT based range expansion algorithm. We prove the accuracy of our algorithm by establishing strong multiplicative bounds for several special cases of interest. Using synthetic and standard real data sets, we demonstrate the benefit of our high-order TMCM over pairwise TCM, as well as the benefit of our range expansion algorithm over other st-MINCUT based approaches.",http://arxiv.org/pdf/1512.07815v2
,,,,,,908,Additive Component Analysis,"Calvin Murdock, Fernando De la Torre",Component Selection in the Additive Regression Model,"Similar to variable selection in the linear regression model, selecting significant components in the popular additive regression model is of great interest. However, such components are unknown smooth functions of independent variables, which are unobservable. As such, some approximation is needed. In this paper, we suggest a combination of penalized regression spline approximation and group variable selection, called the lasso-type spline method (LSM), to handle this component selection problem with a diverging number of strongly correlated variables in each group. It is shown that the proposed method can select significant components and estimate nonparametric additive function components simultaneously with an optimal convergence rate simultaneously. To make the LSM stable in computation and able to adapt its estimators to the level of smoothness of the component functions, weighted power spline bases and projected weighted power spline bases are proposed. Their performance is examined by simulation studies across two set-ups with independent predictors and correlated predictors, respectively, and appears superior to the performance of competing methods. The proposed method is extended to a partial linear regression model analysis with real data, and gives reliable results.",http://arxiv.org/pdf/1101.0047v1
,,,,,,1065,Subspace Clustering via Variance Regularized Ridge Regression,"Chong Peng, Zhao Kang, Qiang Cheng",,,
,,,,,,1074,The Incremental Multiresolution Matrix Factorization Algorithm,"Vamsi K. Ithapu, Risi Kondor, Sterling C. Johnson, Vikas Singh",The Incremental Multiresolution Matrix Factorization Algorithm,"Multiresolution analysis and matrix factorization are foundational tools in computer vision. In this work, we study the interface between these two distinct topics and obtain techniques to uncover hierarchical block structure in symmetric matrices -- an important aspect in the success of many vision problems. Our new algorithm, the incremental multiresolution matrix factorization, uncovers such structure one feature at a time, and hence scales well to large matrices. We describe how this multiscale analysis goes much farther than what a direct global factorization of the data can identify. We evaluate the efficacy of the resulting factorizations for relative leveraging within regression tasks using medical imaging data. We also use the factorization on representations learned by popular deep networks, providing evidence of their ability to infer semantic relationships even when they are not explicitly trained to do so. We show that this algorithm can be used as an exploratory tool to improve the network architecture, and within numerous other settings in vision.",http://arxiv.org/pdf/1705.05804v1
,,,,,,1299,Transformation-Grounded Image Generation Network for Novel 3D View Synthesis,"Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, Alexander C. Berg",Transformation-Grounded Image Generation Network for Novel 3D View Synthesis,"We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Instead of taking a 'blank slate' approach, we first explicitly infer the parts of the geometry visible both in the input and novel views and then re-cast the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.",http://arxiv.org/pdf/1703.02921v1
,,,,,,1424,Learning Dynamic Guidance for Depth Image Enhancement,"Shuhang Gu, Wangmeng Zuo, Shi Guo, Yunjin Chen, Chongyu Chen, Lei Zhang",,,
,,,,,,1857,A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural Network for Photo Aesthetic Assessment,"Shuang Ma, Jing Liu, Chang Wen Chen",,,
,,,,,,2083,Teaching Compositionality to CNNs,"Austin Stone, Huayan Wang, Michael Stark, Yi Liu, D. Scott Phoenix, Dileep George",Teaching Compositionality to CNNs,"Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application-specific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.",http://arxiv.org/pdf/1706.04313v1
,,,,,,2148,Using Ranking-CNN for Age Estimation,"Shixing Chen, Caojin Zhang, Ming Dong, Jialiang Le, Mike Rao",,,
,,,,,,2293,Accurate Single Stage Detector Using Recurrent Rolling Convolution,"Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan, Yu-Wing Tai, Li Xu",Accurate Single Stage Detector Using Recurrent Rolling Convolution,"Most of the recent successful methods in accurate object detection and localization used some variants of R-CNN style two stage Convolutional Neural Networks (CNN) where plausible regions were proposed in the first stage then followed by a second stage for decision refinement. Despite the simplicity of training and the efficiency in deployment, the single stage detection methods have not been as competitive when evaluated in benchmarks consider mAP for high IoU thresholds. In this paper, we proposed a novel single stage end-to-end trainable object detection network to overcome this limitation. We achieved this by introducing Recurrent Rolling Convolution (RRC) architecture over multi-scale feature maps to construct object classifiers and bounding box regressors which are ""deep in context"". We evaluated our method in the challenging KITTI dataset which measures methods under IoU threshold of 0.7. We showed that with RRC, a single reduced VGG-16 based model already significantly outperformed all the previously published results. At the time this paper was written our models ranked the first in KITTI car detection (the hard level), the first in cyclist detection and the second in pedestrian detection. These results were not reached by the previous single stage methods. The code is publicly available.",http://arxiv.org/pdf/1704.05776v1
,,,,,,2412,A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation,"Chunpeng Wu, Wei Wen, Tariq Afzal, Yongmei Zhang, Yiran Chen, Hai (Helen) Li",A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation,"Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.",http://arxiv.org/pdf/1703.04071v4
,,,,,,2539,The Impact of Typicality for Informative Representative Selection,"Jawadul H. Bappy, Sujoy Paul, Ertem Tuncel, Amit K. Roy-Chowdhury",,,
,,,,,,2542,Infinite Variational Autoencoder for Semi-Supervised Learning,"M. Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel",Infinite Variational Autoencoder for Semi-Supervised Learning,"This paper presents an infinite variational autoencoder (VAE) whose capacity adapts to suit the input data. This is achieved using a mixture model where the mixing coefficients are modeled by a Dirichlet process, allowing us to integrate over the coefficients when performing inference. Critically, this then allows us to automatically vary the number of autoencoders in the mixture based on the data. Experiments show the flexibility of our method, particularly for semi-supervised learning, where only a small number of training samples are available.",http://arxiv.org/pdf/1611.07800v2
,,,,,,2635,SurfNet: Generating 3D Shape Surfaces Using Deep Residual Networks,"Ayan Sinha, Asim Unmesh, Qixing Huang, Karthik Ramani",SurfNet: Generating 3D shape surfaces using deep residual networks,"3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images.",http://arxiv.org/pdf/1703.04079v1
,,,,,,2723,Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning,"Rudrasis Chakraborty, SÃ¸ren Hauberg, Baba C. Vemuri",Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning,"Principal Component Analysis (PCA) is a fundamental method for estimating a linear subspace approximation to high-dimensional data. Many algorithms exist in literature to achieve a statistically robust version of PCA called RPCA. In this paper, we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces (the Grassmann manifold). Points on this manifold are defined as the subspaces spanned by $K$-tuples of observations. We show that the intrinsic Grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a Gaussian distribution. Similar results are also shown to hold for the RPCA. Further, we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate. When the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance (accuracy and computation time) gain over a recently published RPCA methods with publicly accessible code. We have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. Experimental results depicting stability of our proposed method are also presented. Furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online RPCA algorithm. In terms of reconstruction error and time required, both our algorithms outperform the competition.",http://arxiv.org/pdf/1702.01005v1
,,,,,,2966,Variational Bayesian Multiple Instance Learning With Gaussian Processes,"Manuel HauÃŸmann, Fred A. Hamprecht, Melih Kandemir",,,
,,,,,,3096,Temporal Attention-Gated Model for Robust Sequence Classification,"Wenjie Pei, Tadas BaltruÅ¡aitis, David M.J. Tax, Louis-Philippe Morency",Temporal Attention-Gated Model for Robust Sequence Classification,"Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper, we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.",http://arxiv.org/pdf/1612.00385v2
,,,,,,3138,Non-Uniform Subset Selection for Active Learning in Structured Data,"Sujoy Paul, Jawadul H. Bappy, Amit K. Roy-Chowdhury",,,
,,,,,,3160,Colorization as a Proxy Task for Visual Understanding,"Gustav Larsson, Michael Maire, Gregory Shakhnarovich",Colorization as a Proxy Task for Visual Understanding,"We investigate and improve self-supervision as a drop-in replacement for ImageNet pretraining, focusing on automatic colorization as the proxy task. Self-supervised training has been shown to be more promising for utilizing unlabeled data than other, traditional unsupervised learning methods. We build on this success and evaluate the ability of our self-supervised network in several contexts. On VOC segmentation and classification tasks, we present results that are state-of-the-art among methods not using ImageNet labels for pretraining representations.   Moreover, we present the first in-depth analysis of self-supervision via colorization, concluding that formulation of the loss, training details and network architecture play important roles in its effectiveness. This investigation is further expanded by revisiting the ImageNet pretraining paradigm, asking questions such as: How much training data is needed? How many labels are needed? How much do features change when fine-tuned? We relate these questions back to self-supervision by showing that colorization provides a similarly powerful supervisory signal as various flavors of ImageNet pretraining.",http://arxiv.org/pdf/1703.04044v2
,,,,,,3260,Shading Annotations in the Wild,"Balazs Kovacs, Sean Bell, Noah Snavely, Kavita Bala",Shading Annotations in the Wild,"Understanding shading effects in images is critical for a variety of vision and graphics problems, including intrinsic image decomposition, shadow removal, image relighting, and inverse rendering. As is the case with other vision tasks, machine learning is a promising approach to understanding shading - but there is little ground truth shading data available for real-world images. We introduce Shading Annotations in the Wild (SAW), a new large-scale, public dataset of shading annotations in indoor scenes, comprised of multiple forms of shading judgments obtained via crowdsourcing, along with shading annotations automatically generated from RGB-D imagery. We use this data to train a convolutional neural network to predict per-pixel shading information in an image. We demonstrate the value of our data and network in an application to intrinsic images, where we can reduce decomposition artifacts produced by existing algorithms. Our database is available at http://opensurfaces.cs.cornell.edu/saw/.",http://arxiv.org/pdf/1705.01156v1
,,,,,,3345,LCNN: Lookup-Based Convolutional Neural Network,"Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi",LCNN: Lookup-based Convolutional Neural Network,"Porting state of the art deep learning algorithms to resource constrained compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose a fast, compact, and accurate model for convolutional neural networks that enables efficient learning and inference. We introduce LCNN, a lookup-based convolutional neural network that encodes convolutions by few lookups to a dictionary that is trained to cover the space of weights in CNNs. Training LCNN involves jointly learning a dictionary and a small set of linear combinations. The size of the dictionary naturally traces a spectrum of trade-offs between efficiency and accuracy. Our experimental results on ImageNet challenge show that LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using AlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at inference, but it also enables efficient training. In this paper, we show the benefits of LCNN in few-shot learning and few-iteration learning, two crucial aspects of on-device training of deep learning models.",http://arxiv.org/pdf/1611.06473v2
,,,,,Object Recognition & Scene Understanding,17,Physics Inspired Optimization on Semantic Transfer Features: An Alternative Method for Room Layout Estimation,"Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen, Li Zhang",,,
,,,,,,144,Pixelwise Instance Segmentation With a Dynamically Instantiated Network,"Anurag Arnab, Philip H. S. Torr",Pixelwise Instance Segmentation with a Dynamically Instantiated Network,"Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our state-of-the-art results (particularly at high IoU thresholds) on the Pascal VOC and Cityscapes datasets.",http://arxiv.org/pdf/1704.02386v1
,,,,,,225,Object Detection in Videos With Tubelet Proposal Networks,"Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui Liu, Xiaogang Wang",Object Detection in Videos with Tubelet Proposal Networks,"Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos is vital for object detection. To fully utilize temporal information, state-of-the-art methods are based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets efficiently, but the lengths are generally only several frames, which is not optimal for incorporating long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computationally expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. Experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos.",http://arxiv.org/pdf/1702.06355v2
,,,,,,226,AMVH: Asymmetric Multi-Valued Hashing,"Cheng Da, Shibiao Xu, Kun Ding, Gaofeng Meng, Shiming Xiang, Chunhong Pan",,,
,,,,,,372,Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion,"Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, Xiaoou Tang",,,
,,,,,,483,Deep Visual-Semantic Quantization for Efficient Image Retrieval,"Yue Cao, Mingsheng Long, Jianmin Wang, Shichen Liu",,,
,,,,,,758,Efficient Diffusion on Region Manifolds: Recovering Small Objects With Compact CNN Representations,"Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, OndÅ™ej Chum",Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact CNN Representations,"Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. We perform diffusion through a sparse linear system solver, yielding practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval.",http://arxiv.org/pdf/1611.05113v2
,,,,,,777,Feature Pyramid Networks for Object Detection,"Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie",Feature Pyramid Networks for Object Detection,"Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",http://arxiv.org/pdf/1612.03144v2
,,,,,,828,Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation,"Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, Wangmeng Zuo",Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation,"In domain adaptation, maximum mean discrepancy (MMD) has been widely adopted as a discrepancy metric between the distributions of source and target domains. However, existing MMD-based domain adaptation methods generally ignore the changes of class prior distributions, i.e., class weight bias across domains. This remains an open problem but ubiquitous for domain adaptation, which can be caused by changes in sample selection criteria and application scenarios. We show that MMD cannot account for class weight bias and results in degraded domain adaptation performance. To address this issue, a weighted MMD model is proposed in this paper. Specifically, we introduce class-specific auxiliary weights into the original MMD for exploiting the class prior probability on source and target domains, whose challenge lies in the fact that the class label in target domain is unavailable. To account for it, our proposed weighted MMD model is defined by introducing an auxiliary weight for each class in the source domain, and a classification EM algorithm is suggested by alternating between assigning the pseudo-labels, estimating auxiliary weights and updating model parameters. Extensive experiments demonstrate the superiority of our weighted MMD over conventional MMD for domain adaptation.",http://arxiv.org/pdf/1705.00609v1
,,,,,,1161,StyleNet: Generating Attractive Visual Captions With Styles,"Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng",,,
,,,,,,1665,Fine-Grained Recognition of Thousands of Object Categories With Single-Example Training,"Leonid Karlinsky, Joseph Shtok, Yochay Tzur, Asaf Tzadok",,,
,,,,,,1742,Improving Interpretability of Deep Neural Networks With Semantic Information,"Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang",Improving Interpretability of Deep Neural Networks with Semantic Information,"Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.",http://arxiv.org/pdf/1703.04096v2
,,,,,,2927,Video Captioning With Transferred Semantic Attributes,"Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei",Video Captioning with Transferred Semantic Attributes,"Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNN) to encode video content and Recurrent Neural Networks (RNN) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results when compared to state-of-the-art methods are also reported on M-VAD and MPII-MD.",http://arxiv.org/pdf/1611.07675v1
,,,,,,3054,Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features,"Arthur Daniel Costea, Robert Varga, Sergiu Nedevschi",,,
,,,,,Video Analytics,60,Temporal Convolutional Networks for Action Segmentation and Detection,"Colin Lea, Michael D. Flynn, RenÃ© Vidal, Austin Reiter, Gregory D. Hager",Temporal Convolutional Networks for Action Segmentation and Detection,"The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We introduce a new class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.",http://arxiv.org/pdf/1611.05267v1
,,,,,,141,Surveillance Video Parsing With Single Frame Supervision,"Si Liu, Changhu Wang, Ruihe Qian, Han Yu, Renda Bao, Yao Sun",Surveillance Video Parsing with Single Frame Supervision,"Surveillance video parsing, which segments the video frames into several labels, e.g., face, pants, left-leg, has wide applications. However,pixel-wisely annotating all frames is tedious and inefficient. In this paper, we develop a Single frame Video Parsing (SVP) method which requires only one labeled frame per video in training stage. To parse one particular frame, the video segment preceding the frame is jointly considered. SVP (1) roughly parses the frames within the video segment, (2) estimates the optical flow between frames and (3) fuses the rough parsing results warped by optical flow to produce the refined parsing result. The three components of SVP, namely frame parsing, optical flow estimation and temporal fusion are integrated in an end-to-end manner. Experimental results on two surveillance video datasets show the superiority of SVP over state-of-the-arts.",http://arxiv.org/pdf/1611.09587v1
,,,,,,471,Weakly Supervised Actor-Action Segmentation via Robust Multi-Task Ranking,"Yan Yan, Chenliang Xu, Dawen Cai, Jason J. Corso",,,
,,,,,,790,Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos,"De-An Huang, Joseph J. Lim, Li Fei-Fei, Juan Carlos Niebles",Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos,"We propose an unsupervised method for reference resolution in instructional videos, where the goal is to temporally link an entity (e.g., ""dressing"") to the action (e.g., ""mix yogurt"") that produced it. The key challenge is the inevitable visual-linguistic ambiguities arising from the changes in both visual appearance and referring expression of an entity in the video. This challenge is amplified by the fact that we aim to resolve references with no supervision. We address these challenges by learning a joint visual-linguistic model, where linguistic cues can help resolve visual ambiguities and vice versa. We verify our approach by learning our model unsupervisedly using more than two thousand unstructured cooking videos from YouTube, and show that our visual-linguistic model can substantially improve upon state-of-the-art linguistic only model on reference resolution in instructional videos.",http://arxiv.org/pdf/1703.02521v2
,,,,,,1035,Zero-Shot Action Recognition With Error-Correcting Output Codes,"Jie Qin, Li Liu, Ling Shao, Fumin Shen, Bingbing Ni, Jiaxin Chen, Yunhong Wang",,,
,,,,,,2488,Enhancing Video Summarization via Vision-Language Embedding,"Bryan A. Plummer, Matthew Brown, Svetlana Lazebnik",,,
,,,,,,3327,Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet,"Jianwen Xie, Song-Chun Zhu, Ying Nian Wu",Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet,"Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an ""analysis by synthesis"" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns.",http://arxiv.org/pdf/1606.00972v2
"Saturday, July 22, 2017","1330â€“1500","Kamehameha III",5,"Spotlight 1-2A","Object Recognition & Scene Understanding - Computer Vision & Language",99,Context-Aware Captions From Context-Agnostic Supervision,"Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik",Context-aware Captions from Context-agnostic Supervision,"We introduce a technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of ""siamese cat"" and ""tiger cat"", our system generates language that describes the ""siamese cat"" in a way that distinguishes it from ""tiger cat"". We start with a generic language model that is context-agnostic and add a listener to discriminate between closely-related concepts. Our approach offers two key advantages over previous work: 1) our listener does not need separate training, and 2) allows joint inference to decode sentences that satisfy both the speaker and listener -- yielding an introspective speaker. We first apply our introspective speaker to a justification task, i.e. to describe why an image contains a particular fine-grained category as opposed to another closely related category in the CUB-200-2011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one out of two semantically similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination.",http://arxiv.org/pdf/1701.02870v1
,,,,,,121,Visual Dialog,"Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, JosÃ© M. F. Moura, Devi Parikh, Dhruv Batra",SimDialog: A visual game dialog editor,"SimDialog is a visual editor for dialog in computer games. This paper presents the design of SimDialog, illustrating how script writers and non-programmers can easily create dialog for video games with complex branching structures and dynamic response characteristics. The system creates dialog as a directed graph. This allows for play using the dialog with a state-based cause and effect system that controls selection of non-player character responses and can provide a basic scoring mechanism for games.",http://arxiv.org/pdf/0804.4885v1
,,,,,,178,Discriminative Bimodal Networks for Visual Localization and Detection With Natural Language Queries,"Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, Honglak Lee",Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries,"Associating image regions with text queries has been recently explored as a new way to bridge visual and linguistic representations. A few pioneering approaches have been proposed based on recurrent neural language models trained generatively (e.g., generating captions), but achieving somewhat limited localization accuracy. To better address natural-language-based visual entity localization, we propose a discriminative approach. We formulate a discriminative bimodal neural network (DBNet), which can be trained by a classifier with extensive use of negative samples. Our training objective encourages better localization on single images, incorporates text phrases in a broad range, and properly pairs image regions with text phrases into positive and negative examples. Experiments on the Visual Genome dataset demonstrate the proposed DBNet significantly outperforms previous state-of-the-art methods both for localization on single images and for detection on multiple images. We we also establish an evaluation protocol for natural-language visual detection.",http://arxiv.org/pdf/1704.03944v2
,,,,,,617,Automatic Understanding of Image and Video Advertisements,"Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, Adriana Kovashka",,,
,,,,,,1145,Discover and Learn New Objects From Documentaries,"Kai Chen, Hang Song, Chen Change Loy, Dahua Lin",,,
,,,,,,1962,Spatial-Semantic Image Search by Visual Feature Synthesis,"Long Mai, Hailin Jin, Zhe Lin, Chen Fang, Jonathan Brandt, Feng Liu",,,
,,,,,,2243,Fully-Adaptive Feature Sharing in Multi-Task Networks With Applications in Person Attribute Classification,"Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris",Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification,"Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by hand-designed network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space subject to designer's bias, which can be both error-prone and tedious. In this work, we propose a principled approach for designing compact multi-task deep learning architectures. Our approach starts with a thin network and dynamically widens it in a greedy manner during training using a novel criterion that promotes grouping of similar tasks together. Our Extensive evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-the-art accuracy from strong baselines by much more expensive models.",http://arxiv.org/pdf/1611.05377v1
,,,,,,2390,Semantic Compositional Networks for Visual Captioning,"Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng",Semantic Compositional Networks for Visual Captioning,"A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.",http://arxiv.org/pdf/1611.08002v2
,,,,Oral 1-2A,,108,Deep Reinforcement Learning-Based Image Captioning With Embedding Reward,"Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Li-Jia Li",Deep Reinforcement Learning-based Image Captioning with Embedding Reward,"Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a ""policy network"" and a ""value network"" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.",http://arxiv.org/pdf/1704.03899v1
,,,,,,665,From Red Wine to Red Tomato: Composition With Context,"Ishan Misra, Abhinav Gupta, Martial Hebert",,,
,,,,,,2454,Captioning Images With Diverse Objects,"Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell, Kate Saenko",Captioning Images with Diverse Objects,"Recent captioning models are limited in their ability to scale and describe concepts unseen in paired image-text corpora. We propose the Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Our model takes advantage of external sources -- labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text. We propose minimizing a joint objective which can learn from these diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of image-caption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in MSCOCO image-caption training data, as well as many categories that are observed very rarely. Both automatic evaluations and human judgements show that our model considerably outperforms prior work in being able to describe many more categories of objects.",http://arxiv.org/pdf/1606.07770v2
,,,,,,3266,Self-Critical Sequence Training for Image Captioning,"Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, Vaibhava Goel",Self-critical Sequence Training for Image Captioning,"Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a ""baseline"" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 112.3.",http://arxiv.org/pdf/1612.00563v1
"Saturday, July 22, 2017",1330â€“1500,KalÄÅkaua Ballroom A-B,6,Spotlight 1-2B,Analyzing Humans 1,210,Crossing Nets: Combining GANs and VAEs With a Shared Latent Space for Hand Pose Estimation,"Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao",,,
,,,,,,554,Predicting Behaviors of Basketball Players From First Person Videos,"Shan Su, Jung Pyo Hong, Jianbo Shi, Hyun Soo Park",Social Behavior Prediction from First Person Videos,"This paper presents a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first person videos. The predicted behaviors reflect an individual physical space that affords to take the next actions while conforming to social behaviors by engaging to joint attention. Our key innovation is to use the 3D reconstruction of multiple first person cameras to automatically annotate each other's the visual semantics of social configurations.   We leverage two learning signals uniquely embedded in first person videos. Individually, a first person video records the visual semantics of a spatial and social layout around a person that allows associating with past similar situations. Collectively, first person videos follow joint attention that can link the individuals to a group. We learn the egocentric visual semantics of group movements using a Siamese neural network to retrieve future trajectories. We consolidate the retrieved trajectories from all players by maximizing a measure of social compatibility---the gaze alignment towards joint attention predicted by their social formation, where the dynamics of joint attention is learned by a long-term recurrent convolutional network. This allows us to characterize which social configuration is more plausible and predict future group trajectories.",http://arxiv.org/pdf/1611.09464v1
,,,,,,1270,LCR-Net: Localization-Classification-Regression for Human Pose,"GrÃ©gory Rogez, Philippe Weinzaepfel, Cordelia Schmid",,,
,,,,,,1626,Learning Residual Images for Face Attribute Manipulation,"Wei Shen, Rujie Liu",Learning Residual Images for Face Attribute Manipulation,"Face attributes are interesting due to their detailed description of human faces. Unlike prior researches working on attribute prediction, we address an inverse and more challenging problem called face attribute manipulation which aims at modifying a face image according to a given attribute value. Instead of manipulating the whole image, we propose to learn the corresponding residual image defined as the difference between images before and after the manipulation. In this way, the manipulation can be operated efficiently with modest pixel modification. The framework of our approach is based on the Generative Adversarial Network. It consists of two image transformation networks and a discriminative network. The transformation networks are responsible for the attribute manipulation and its dual operation and the discriminative network is used to distinguish the generated images from real images. We also apply dual learning to allow transformation networks to learn from each other. Experiments show that residual images can be effectively learned and used for attribute manipulations. The generated images remain most of the details in attribute-irrelevant areas.",http://arxiv.org/pdf/1612.05363v2
,,,,,,2433,Seeing What Is Not There: Learning Context to Determine Where Objects Are Missing,"Jin Sun, David W. Jacobs",Seeing What Is Not There: Learning Context to Determine Where Objects Are Missing,"Most of computer vision focuses on what is in an image. We propose to train a standalone object-centric context representation to perform the opposite task: seeing what is not there. Given an image, our context model can predict where objects should exist, even when no object instances are present. Combined with object detection results, we can perform a novel vision task: finding where objects are missing in an image. Our model is based on a convolutional neural network structure. With a specially designed training strategy, the model learns to ignore objects and focus on context only. It is fully convolutional thus highly efficient. Experiments show the effectiveness of the proposed approach in one important accessibility task: finding city street regions where curb ramps are missing, which could help millions of people with mobility disabilities.",http://arxiv.org/pdf/1702.07971v1
,,,,,,2684,Deep Learning on Lie Groups for Skeleton-Based Action Recognition,"Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool",Deep Learning on Lie Groups for Skeleton-based Action Recognition,"In recent years, skeleton-based action recognition has become a popular 3D classification problem. State-of-the-art methods typically first represent each motion sequence as a high-dimensional trajectory on a Lie group with an additional dynamic time warping, and then shallowly learn favorable Lie group features. In this paper we incorporate the Lie group structure into a deep network architecture to learn more appropriate Lie group features for 3D action recognition. Within the network structure, we design rotation mapping layers to transform the input Lie group features into desirable ones, which are aligned better in the temporal domain. To reduce the high feature dimensionality, the architecture is equipped with rotation pooling layers for the elements on the Lie group. Furthermore, we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification. Evaluations of the proposed network for standard 3D human action recognition datasets clearly demonstrate its superiority over existing shallow Lie group feature learning methods as well as most conventional deep learning methods.",http://arxiv.org/pdf/1612.05877v2
,,,,,,3245,Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations,"Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis",Harvesting Multiple Views for Marker-less 3D Human Pose Annotations,"Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings: (i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subject's appearance (i.e.,""personalization""), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information.",http://arxiv.org/pdf/1704.04793v1
,,,,,,3269,Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose,"Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis",Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose,"This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization only and recover 3D pose by a subsequent optimization step. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach allows us to train a ConvNet that outperforms all state-of-the-art approaches on standard benchmarks achieving relative error reduction greater than 35% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.",http://arxiv.org/pdf/1611.07828v1
,,,,Oral 1-2B,,244,Weakly Supervised Action Learning With RNN Based Fine-To-Coarse Modeling,"Alexander Richard, Hilde Kuehne, Juergen Gall",Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling,"We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.",http://arxiv.org/pdf/1703.08132v2
,,,,,,528,Disentangled Representation Learning GAN for Pose-Invariant Face Recognition,"Luan Tran, Xi Yin, Xiaoming Liu",Representation Learning by Rotating Your Faces,"The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.",http://arxiv.org/pdf/1705.11136v1
,,,,,,2880,ArtTrack: Articulated Multi-Person Tracking in the Wild,"Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, Bernt Schiele",ArtTrack: Articulated Multi-person Tracking in the Wild,"In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public MPII Human Pose benchmark and on a new MPII Video Pose dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes.",http://arxiv.org/pdf/1612.01465v3
,,,,,,3550,Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields,"Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh",Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields,"We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",http://arxiv.org/pdf/1611.08050v2
"Saturday, July 22, 2017",1330â€“1500,KalÄÅkaua Ballroom C,7,Spotlight 1-2C,Image Motion & Tracking; Video Analysis,67,Template Matching With Deformable Diversity Similarity,"Itamar Talmi, Roey Mechrez, Lihi Zelnik-Manor",Template Matching with Deformable Diversity Similarity,"We propose a novel measure for template matching named Deformable Diversity Similarity -- based on the diversity of feature matches between a target image window and the template. We rely on both local appearance and geometric information that jointly lead to a powerful approach for matching. Our key contribution is a similarity measure, that is robust to complex deformations, significant background clutter, and occlusions. Empirical evaluation on the most up-to-date benchmark shows that our method outperforms the current state-of-the-art in its detection accuracy while improving computational complexity.",http://arxiv.org/pdf/1612.02190v2
,,,,,,140,Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification,"Weihua Chen, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang",Beyond triplet loss: a deep quadruplet network for person re-identification,"Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.",http://arxiv.org/pdf/1704.01719v1
,,,,,,799,Agent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization,"Kuo-Hao Zeng, Shih-Han Chou, Fu-Hsiang Chan, Juan Carlos Niebles, Min Sun",Agent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization,"For survival, a living agent must have the ability to assess risk (1) by temporally anticipating accidents before they occur, and (2) by spatially localizing risky regions in the environment to move away from threats. In this paper, we take an agent-centric approach to study the accident anticipation and risky region localization tasks. We propose a novel soft-attention Recurrent Neural Network (RNN) which explicitly models both spatial and appearance-wise non-linear interaction between the agent triggering the event and another agent or static-region involved. In order to test our proposed method, we introduce the Epic Fail (EF) dataset consisting of 3000 viral videos capturing various accidents. In the experiments, we evaluate the risk assessment accuracy both in the temporal domain (accident anticipation) and spatial domain (risky region localization) on our EF dataset and the Street Accident (SA) dataset. Our method consistently outperforms other baselines on both datasets.",http://arxiv.org/pdf/1705.06560v1
,,,,,,959,Bidirectional Multirate Reconstruction for Temporal Modeling in Videos,"Linchao Zhu, Zhongwen Xu, Yi Yang",Bidirectional Multirate Reconstruction for Temporal Modeling in Videos,"Despite the recent success of neural networks in image feature learning, a major problem in the video domain is the lack of sufficient labeled data for learning to model temporal information. In this paper, we propose an unsupervised temporal modeling method that learns from untrimmed videos. The speed of motion varies constantly, e.g., a man may run quickly or slowly. We therefore train a Multirate Visual Recurrent Model (MVRM) by encoding frames of a clip with different intervals. This learning process makes the learned model more capable of dealing with motion speed variance. Given a clip sampled from a video, we use its past and future neighboring clips as the temporal context, and reconstruct the two temporal transitions, i.e., present$\rightarrow$past transition and present$\rightarrow$future transition, reflecting the temporal information in different views. The proposed method exploits the two transitions simultaneously by incorporating a bidirectional reconstruction which consists of a backward reconstruction and a forward reconstruction. We apply the proposed method to two challenging video tasks, i.e., complex event detection and video captioning, in which it achieves state-of-the-art performance. Notably, our method generates the best single feature for event detection with a relative improvement of 10.4% on the MEDTest-13 dataset and achieves the best performance in video captioning across all evaluation metrics on the YouTube2Text dataset.",http://arxiv.org/pdf/1611.09053v1
,,,,,,985,Action-Decision Networks for Visual Tracking With Deep Reinforcement Learning,"Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, Jin Young Choi",,,
,,,,,,1002,TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering,"Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim",TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering,"Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.",http://arxiv.org/pdf/1704.04497v1
,,,,,,2894,Making 360Â° Video Watchable in 2D: Learning Videography for Click Free Viewing,"Yu-Chuan Su, Kristen Grauman",TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering,"Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.",http://arxiv.org/pdf/1704.04497v1
,,,,,,3289,Unsupervised Adaptive Re-Identification in Open World Dynamic Camera Networks,"Rameswar Panda, Amran Bhuiyan, Vittorio Murino, Amit K. Roy-Chowdhury",Unsupervised Adaptive Re-identification in Open World Dynamic Camera Networks,"Person re-identification is an open and challenging problem in computer vision. Existing approaches have concentrated on either designing the best feature representation or learning optimal matching metrics in a static setting where the number of cameras are fixed in a network. Most approaches have neglected the dynamic and open world nature of the re-identification problem, where a new camera may be temporarily inserted into an existing system to get additional information. To address such a novel and very practical problem, we propose an unsupervised adaptation scheme for re-identification models in a dynamic camera network. First, we formulate a domain perceptive re-identification method based on geodesic flow kernel that can effectively find the best source camera (already installed) to adapt with a newly introduced target camera, without requiring a very expensive training phase. Second, we introduce a transitive inference algorithm for re-identification that can exploit the information from best source camera to improve the accuracy across other camera pairs in a network of multiple cameras. Extensive experiments on four benchmark datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art unsupervised learning based alternatives whilst being extremely efficient to compute.",http://arxiv.org/pdf/1706.03112v1
,,,,Oral 1-2C,,526,Context-Aware Correlation Filter Tracking,"Matthias Mueller, Neil Smith, Bernard Ghanem",,,
,,,,,,1277,Deep 360 Pilot: Learning a Deep Agent for Piloting Through 360Â° Sports Videos,"Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju Chang, Min Sun",,,
,,,,,,1321,Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data,"Joel Janai, Fatma GÃ_ney, Jonas Wulff, Michael J. Black, Andreas Geiger",,,
,,,,,,2448,CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos,"Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu Chang",CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos,"Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. We will update the camera-ready version and publish the source codes online soon.",http://arxiv.org/pdf/1703.01515v2
"Saturday, July 22, 2017",1500â€“1700,Kamehameha I,8,Poster 1-2,3D Computer Vision,111,Exploiting 2D Floorplan for Building-Scale Panorama RGBD Alignment,"Erik Wijmans, Yasutaka Furukawa",Exploiting 2D Floorplan for Building-scale Panorama RGBD Alignment,"This paper presents a novel algorithm that utilizes a 2D floorplan to align panorama RGBD scans. While effective panorama RGBD alignment techniques exist, such a system requires extremely dense RGBD image sampling. Our approach can significantly reduce the number of necessary scans with the aid of a floorplan image. We formulate a novel Markov Random Field inference problem as a scan placement over the floorplan, as opposed to the conventional scan-to-scan alignment. The technical contributions lie in multi-modal image correspondence cues (between scans and schematic floorplan) as well as a novel coverage potential avoiding an inherent stacking bias. The proposed approach has been evaluated on five challenging large indoor spaces. To the best of our knowledge, we present the first effective system that utilizes a 2D floorplan image for building-scale 3D pointcloud alignment. The source code and the data will be shared with the community to further enhance indoor mapping research.",http://arxiv.org/pdf/1612.02859v1
,,,,,,346,A Combinatorial Solution to Non-Rigid 3D Shape-To-Image Matching,"Florian Bernard, Frank R. Schmidt, Johan Thunberg, Daniel Cremers",A Combinatorial Solution to Non-Rigid 3D Shape-to-Image Matching,"We propose a combinatorial solution for the problem of non-rigidly matching a 3D shape to 3D image data. To this end, we model the shape as a triangular mesh and allow each triangle of this mesh to be rigidly transformed to achieve a suitable matching to the image. By penalising the distance and the relative rotation between neighbouring triangles our matching compromises between image and shape information. In this paper, we resolve two major challenges: Firstly, we address the resulting large and NP-hard combinatorial problem with a suitable graph-theoretic approach. Secondly, we propose an efficient discretisation of the unbounded 6-dimensional Lie group SE(3). To our knowledge this is the first combinatorial formulation for non-rigid 3D shape-to-image matching. In contrast to existing local (gradient descent) optimisation methods, we obtain solutions that do not require a good initialisation and that are within a bound of the optimal solution. We evaluate the proposed method on the two problems of non-rigid 3D shape-to-shape and non-rigid 3D shape-to-image registration and demonstrate that it provides promising results.",http://arxiv.org/pdf/1611.05241v2
,,,,,,536,NID-SLAM: Robust Monocular SLAM Using Normalised Information Distance,"Geoffrey Pascoe, Will Maddern, Michael Tanner, Pedro PiniÃ©s, Paul Newman",,,
,,,,,,853,End-To-End Training of Hybrid CNN-CRF Models for Stereo,"Patrick KnÃ¶belreiter, Christian Reinbacher, Alexander Shekhovtsov, Thomas Pock",End-to-End Training of Hybrid CNN-CRF Models for Stereo,"We propose a novel and principled hybrid CNN+CRF model for stereo estimation. Our model allows to exploit the advantages of both, convolutional neural networks (CNNs) and conditional random fields (CRFs) in an unified approach. The CNNs compute expressive features for matching and distinctive color edges, which in turn are used to compute the unary and binary costs of the CRF. For inference, we apply a recently proposed highly parallel dual block descent algorithm which only needs a small fixed number of iterations to compute a high-quality approximate minimizer. As the main contribution of the paper, we propose a theoretically sound method based on the structured output support vector machine (SSVM) to train the hybrid CNN+CRF model on large-scale data end-to-end. Our trained models perform very well despite the fact that we are using shallow CNNs and do not apply any kind of post-processing to the final output of the CRF. We evaluate our combined models on challenging stereo benchmarks such as Middlebury 2014 and Kitti 2015 and also investigate the performance of each individual component.",http://arxiv.org/pdf/1611.10229v2
,,,,,,951,Learning Shape Abstractions by Assembling Volumetric Primitives,"Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, Jitendra Malik",Learning Shape Abstractions by Assembling Volumetric Primitives,"We present a learning framework for abstracting complex shapes by learning to assemble objects using 3D volumetric primitives. In addition to generating simple and geometrically interpretable explanations of 3D objects, our framework also allows us to automatically discover and exploit consistent structure in the data. We demonstrate that using our method allows predicting shape representations which can be leveraged for obtaining a consistent parsing across the instances of a shape collection and constructing an interpretable shape similarity measure. We also examine applications for image-based prediction as well as shape manipulation.",http://arxiv.org/pdf/1612.00404v2
,,,,,,1117,Locality-Sensitive Deconvolution Networks With Gated Fusion for RGB-D Indoor Semantic Segmentation,"Yanhua Cheng, Rui Cai, Zhiwei Li, Xin Zhao, Kaiqi Huang",,,
,,,,,,1314,Acquiring Axially-Symmetric Transparent Objects Using Single-View Transmission Imaging,"Jaewon Kim, Ilya Reshetouski, Abhijeet Ghosh",,,
,,,,,,2144,Regressing Robust and Discriminative 3D Morphable Models With a Very Deep Neural Network,"Anh Tuáº¥n Tráº§n, Tal Hassner, Iacopo Masi, GÃ©rard Medioni",Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network,"The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied ""in the wild"", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.",http://arxiv.org/pdf/1612.04904v1
,,,,,,2554,End-To-End 3D Face Reconstruction With Deep Neural Networks,"Pengfei Dou, Shishir K. Shah, Ioannis A. Kakadiaris",End-to-end 3D face reconstruction with deep neural networks,"Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction.",http://arxiv.org/pdf/1704.05020v1
,,,,,,2775,DUST: Dual Union of Spatio-Temporal Subspaces for Monocular Multiple Object 3D Reconstruction,"Antonio Agudo, Francesc Moreno-Noguer",,,
,,,,,Analyzing Humans in Images,314,Finding Tiny Faces,"Peiyun Hu, Deva Ramanan",Finding Tiny Faces,"Though tremendous strides have been made in object recognition, one of the remaining open challenges is detecting small objects. We explore three aspects of the problem in the context of finding small faces: the role of scale invariance, image resolution, and contextual reasoning. While most recognition approaches aim to be scale-invariant, the cues for recognizing a 3px tall face are fundamentally different than those for recognizing a 300px tall face. We take a different approach and train separate detectors for different scales. To maintain efficiency, detectors are trained in a multi-task fashion: they make use of features extracted from multiple layers of single (deep) feature hierarchy. While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99% of the template extends beyond the object of interest). Finally, we explore the role of scale in pre-trained deep networks, providing ways to extrapolate networks tuned for limited scales to rather extreme ranges. We demonstrate state-of-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when compared to prior art on WIDER FACE, our results reduce error by a factor of 2 (our models produce an AP of 82% while prior art ranges from 29-64%).",http://arxiv.org/pdf/1612.04402v2
,,,,,,574,Dynamic Facial Analysis: From Bayesian Filtering to Recurrent Neural Network,"Jinwei Gu, Xiaodong Yang, Shalini De Mello, Jan Kautz",,,
,,,,,,851,Deep Temporal Linear Encoding Networks,"Ali Diba, Vivek Sharma, Luc Van Gool",Deep Temporal Linear Encoding Networks,"The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.",http://arxiv.org/pdf/1611.06678v1
,,,,,,1006,Joint Registration and Representation Learning for Unconstrained Face Identification,"Munawar Hayat, Salman H. Khan, Naoufel Werghi, Roland Goecke",,,
,,,,,,1034,3D Human Pose Estimation From a Single Image via Distance Matrix Regression,Francesc Moreno-Noguer,3D Human Pose Estimation from a Single Image via Distance Matrix Regression,"This paper addresses the problem of 3D human pose estimation from a single image. We follow a standard two-step pipeline by first detecting the 2D position of the $N$ body joints, and then using these observations to infer 3D pose. For the first step, we use a recent CNN-based detector. For the second step, most existing approaches perform 2$N$-to-3$N$ regression of the Cartesian joint coordinates. We show that more precise pose estimates can be obtained by representing both the 2D and 3D human poses using $N\times N$ distance matrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network architectures, which by construction, enforce positivity and symmetry of the predicted matrices. The approach has also the advantage to naturally handle missing observations and allowing to hypothesize the position of non-observed joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate consistent performance gains over state-of-the-art. Qualitative evaluation on the images in-the-wild of the LSP dataset, using the regressor learned on Human3.6M, reveals very promising generalization results.",http://arxiv.org/pdf/1611.09010v1
,,,,,,1100,One-Shot Metric Learning for Person Re-Identification,"Slawomir BÄ…k, Peter Carr","A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets","Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 8 feature extraction algorithms and 19 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 13 other publicly available datasets: VIPeR, GRID, CAVIAR, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK03, RAiD, iLIDSVID, HDA+ and Market1501. The evaluation codebase and results will be made publicly available for community use.",http://arxiv.org/pdf/1605.09653v3
,,,,,,1192,Generalized Rank Pooling for Activity Recognition,"Anoop Cherian, Basura Fernando, Mehrtash Harandi, Stephen Gould",Generalized Rank Pooling for Activity Recognition,"Most popular deep models for action recognition split video sequences into short sub-sequences consisting of a few frames; frame-based features are then pooled for recognizing the activity. Usually, this pooling step discards the temporal order of the frames, which could otherwise be used for better recognition. Towards this end, we propose a novel pooling method, generalized rank pooling (GRP), that takes as input, features from the intermediate layers of a CNN that is trained on tiny sub-sequences, and produces as output the parameters of a subspace which (i) provides a low-rank approximation to the features and (ii) preserves their temporal order. We propose to use these parameters as a compact representation for the video sequence, which is then used in a classification setup. We formulate an objective for computing this subspace as a Riemannian optimization problem on the Grassmann manifold, and propose an efficient conjugate gradient scheme for solving it. Experiments on several activity recognition datasets show that our scheme leads to state-of-the-art performance.",http://arxiv.org/pdf/1704.02112v2
,,,,,,2714,Deep Representation Learning for Human Motion Prediction and Classification,"Judith BÃ_tepage, Michael J. Black, Danica Kragic, Hedvig KjellstrÃ¶m",Deep representation learning for human motion prediction and classification,"Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.",http://arxiv.org/pdf/1702.07486v2
,,,,,,3171,Interspecies Knowledge Transfer for Facial Keypoint Detection,"Maheen Rashid, Xiuye Gu, Yong Jae Lee",Interspecies Knowledge Transfer for Facial Keypoint Detection,"We present a method for localizing facial keypoints on animals by transferring knowledge gained from human faces. Instead of directly finetuning a network trained to detect keypoints on human faces to animal faces (which is sub-optimal since human and animal faces can look quite different), we propose to first adapt the animal images to the pre-trained human detection network by correcting for the differences in animal and human face shape. We first find the nearest human neighbors for each animal image using an unsupervised shape matching method. We use these matches to train a thin plate spline warping network to warp each animal face to look more human-like. The warping network is then jointly finetuned with a pre-trained human facial keypoint detection network using an animal dataset. We demonstrate state-of-the-art results on both horse and sheep facial keypoint detection, and significant improvement over simple finetuning, especially when training data is scarce. Additionally, we present a new dataset with 3717 images with horse face and facial keypoint annotations.",http://arxiv.org/pdf/1704.04023v1
,,,,,,3624,Recurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization,"Runpeng Cui, Hu Liu, Changshui Zhang",,,
,,,,,Applications,3469,Modeling Sub-Event Dynamics in First-Person Action Recognition,"Hasan F. M. Zaki, Faisal Shafait, Ajmal Mian",,,
,,,,,Computational Photography,137,Turning an Urban Scene Video Into a Cinemagraph,"Hang Yan, Yebin Liu, Yasutaka Furukawa",Turning an Urban Scene Video into a Cinemagraph,"This paper proposes an algorithm that turns a regular video capturing urban scenes into a high-quality endless animation, known as a Cinemagraph. The creation of a Cinemagraph usually requires a static camera in a carefully configured scene. The task becomes challenging for a regular video with a moving camera and objects. Our approach first warps an input video into the viewpoint of a reference camera. Based on the warped video, we propose effective temporal analysis algorithms to detect regions with static geometry and dynamic appearance, where geometric modeling is reliable and visually attractive animations can be created. Lastly, the algorithm applies a sequence of video processing techniques to produce a Cinemagraph movie. We have tested the proposed approach on numerous challenging real scenes. To our knowledge, this work is the first to automatically generate Cinemagraph animations from regular movies in the wild.",http://arxiv.org/pdf/1612.01235v1
,,,,,,2806,Light Field Reconstruction Using Deep Convolutional Network on EPI,"Gaochang Wu, Mandan Zhao, Liangyong Wang, Qionghai Dai, Tianyou Chai, Yebin Liu",,,
,,,,,Image Motion & Tracking,900,FlowNet 2.0: Evolution of Optical Flow Estimation With Deep Networks,"Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox",FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,"The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a sub-network specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",http://arxiv.org/pdf/1612.01925v1
,,,,,Low- & Mid-Level Vision,213,Attention-Aware Face Hallucination via Deep Reinforcement Learning,"Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, Guanbin Li",,,
,,,,,,286,Simple Does It: Weakly Supervised Instance and Semantic Segmentation,"Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele",Simple Does It: Weakly Supervised Instance and Semantic Segmentation,"Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach that does not require modification of the segmentation training procedure. We show that when carefully designing the input labels from given bounding boxes, even a single round of training is enough to improve over previously reported weakly supervised results. Overall, our weak supervision approach reaches ~95% of the quality of the fully supervised model, both for semantic labelling and instance segmentation.",http://arxiv.org/pdf/1603.07485v2
,,,,,,450,Anti-Glare: Tightly Constrained Optimization for Eyeglass Reflection Removal,"Tushar Sandhan, Jin Young Choi",,,
,,,,,,492,Deep Joint Rain Detection and Removal From a Single Image,"Wenhan Yang, Robby T. Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, Shuicheng Yan",Deep Joint Rain Detection and Removal from a Single Image,"In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in the new rain image models and a novel deep learning architecture. We first modify an existing model comprising a rain streak layer and a background layer, by adding a binary map that locates rain streak regions. Second, we create a new model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the first model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and outputs better representation for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our novel models and architecture, outperforming the state-of-the-art methods significantly. Our codes and data sets will be publicly available.",http://arxiv.org/pdf/1609.07769v3
,,,,,,1149,Radiometric Calibration From Faces in Images,"Chen Li, Stephen Lin, Kun Zhou, Katsushi Ikeuchi",,,
,,,,,,1333,Webly Supervised Semantic Segmentation,"Bin Jin, Maria V. Ortiz Segovia, Sabine SÃ_sstrunk",,,
,,,,,,1495,Removing Rain From Single Images via a Deep Detail Network,"Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, John Paisley",,,
,,,,,,1512,Deep Crisp Boundaries,"Yupei Wang, Xin Zhao, Kaiqi Huang",,,
,,,,,,1722,Coarse-To-Fine Segmentation With Shape-Tailored Continuum Scale Spaces,"Naeemullah Khan, Byung-Woo Hong, Anthony Yezzi, Ganesh Sundaramoorthi",Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces,"We formulate a general energy and method for segmentation that is designed to have preference for segmenting the coarse structure over the fine structure of the data, without smoothing across boundaries of regions. The energy is formulated by considering data terms at a continuum of scales from the scale space computed from the Heat Equation within regions, and integrating these terms over all time. We show that the energy may be approximately optimized without solving for the entire scale space, but rather solving time-independent linear equations at the native scale of the image, making the method computationally feasible. We provide a multi-region scheme, and apply our method to motion segmentation. Experiments on a benchmark dataset shows that our method is less sensitive to clutter or other undesirable fine-scale structure, and leads to better performance in motion segmentation.",http://arxiv.org/pdf/1603.07745v1
,,,,,,1770,Large Kernel Matters â€” Improve Semantic Segmentation by Global Convolutional Network,"Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun",Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces,"We formulate a general energy and method for segmentation that is designed to have preference for segmenting the coarse structure over the fine structure of the data, without smoothing across boundaries of regions. The energy is formulated by considering data terms at a continuum of scales from the scale space computed from the Heat Equation within regions, and integrating these terms over all time. We show that the energy may be approximately optimized without solving for the entire scale space, but rather solving time-independent linear equations at the native scale of the image, making the method computationally feasible. We provide a multi-region scheme, and apply our method to motion segmentation. Experiments on a benchmark dataset shows that our method is less sensitive to clutter or other undesirable fine-scale structure, and leads to better performance in motion segmentation.",http://arxiv.org/pdf/1603.07745v1
,,,,,,1825,Single Image Reflection Suppression,"Nikolaos Arvanitopoulos, Radhakrishna Achanta, Sabine SÃ_sstrunk",ImplementaciÃ_n de un Filtro de PolarizaciÃ_n basado en la DescomposiciÃ_n del Valor Singular (SVD),"In this job we have introduced a filtered adaptive technique that allows highlighting selective seismic events according to its polarization. This technique consists in the implementation of a polarizing filter based on the Single Value Decomposition (SVD). The SVD have a great variety of application in the processing of signals and images, in this case a seismic reading is considered an image of the underground. With this technique, was achieved to make a reconstruction of the seismic image generated after the two first autoimages reproduce the polarization attributes (rectilinearity and planarity) of the moving particles, suppress the high frequency random sound, improve the signal-noise ratio and this way we were able to obtain a clear and coherent image of the associated seismic events to the primary energies of reflection.",http://arxiv.org/pdf/1311.3706v1
,,,,,,2581,CASENet: Deep Category-Aware Semantic Edge Detection,"Zhiding Yu, Chen Feng, Ming-Yu Liu, Srikumar Ramalingam",CASENet: Deep Category-Aware Semantic Edge Detection,"Boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation, object recognition, stereo, and object proposal generation. Recently, the problem of edge detection has been revisited and significant progress has been made with deep learning. While classical edge detection is a challenging binary problem in itself, the category-aware semantic edge detection by nature is an even more challenging multi-label problem. We model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes. To this end, we propose a novel end-to-end deep semantic edge learning architecture based on ResNet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features. We then propose a multi-label loss function to supervise the fused activations. We show that our proposed architecture benefits this problem with better performance, and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as SBD and Cityscapes.",http://arxiv.org/pdf/1705.09759v1
,,,,,,3113,Reflectance Adaptive Filtering Improves Intrinsic Image Estimation,"Thomas Nestmeyer, Peter V. Gehler",Reflectance Adaptive Filtering Improves Intrinsic Image Estimation,"Separating an image into reflectance and shading layers poses a challenge for learning approaches because no large corpus of precise and realistic ground truth decompositions exists. The Intrinsic Images in the Wild~(IIW) dataset provides a sparse set of relative human reflectance judgments, which serves as a standard benchmark for intrinsic images. A number of methods use IIW to learn statistical dependencies between the images and their reflectance layer. Although learning plays an important role for high performance, we show that a standard signal processing technique achieves performance on par with current state-of-the-art. We propose a loss function for CNN learning of dense reflectance predictions. Our results show a simple pixel-wise decision, without any context or prior knowledge, is sufficient to provide a strong baseline on IIW. This sets a competitive baseline which only two other approaches surpass. We then develop a joint bilateral filtering method that implements strong prior knowledge about reflectance constancy. This filtering operation can be applied to any intrinsic image algorithm and we improve several previous results achieving a new state-of-the-art on IIW. Our findings suggest that the effect of learning-based approaches may have been over-estimated so far. Explicit prior knowledge is still at least as important to obtain high performance in intrinsic image decompositions.",http://arxiv.org/pdf/1612.05062v2
,,,,,Machine Learning,273,Conditional Similarity Networks,"Andreas Veit, Serge Belongie, Theofanis Karaletsos",Stability Analysis of Network of Similar-Plants via Feedback Network,"Here, we study a networked control system with similar linear time-invariant plants. Using master stability function method we propose a network optimization method to minimize the feedback network order in the sense Frobenius norm. Then we verify our results with numerical example. We show that this method outperforms the known feedback network optimization methods namely matching condition.",http://arxiv.org/pdf/1511.08876v1
,,,,,,360,Spatially Adaptive Computation Time for Residual Networks,"Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, Ruslan Salakhutdinov",Spatially Adaptive Computation Time for Residual Networks,"This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.",http://arxiv.org/pdf/1612.02297v1
,,,,,,451,Xception: Deep Learning With Depthwise Separable Convolutions,FranÃ§ois Chollet,Xception: Deep Learning with Depthwise Separable Convolutions,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",http://arxiv.org/pdf/1610.02357v3
,,,,,,474,Feedback Networks,"Amir R. Zamir, Te-Lin Wu, Lin Sun, William B. Shen, Bertram E. Shi, Jitendra Malik, Silvio Savarese",Feedback Capacity over Networks,"In this paper, we investigate the fundamental limitations of feedback mechanism in dealing with uncertainties for network systems. The study of maximum capability of feedback control was pioneered in Xie and Guo (2000) for scalar systems with nonparametric nonlinear uncertainty. In a network setting, nodes with unknown and nonlinear dynamics are interconnected through a directed interaction graph. Nodes can design feedback controls based on all available information, where the objective is to stabilize the network state. Using information structure and decision pattern as criteria, we specify three categories of network feedback laws, namely the global-knowledge/global-decision, network-flow/local-decision, and local-flow/local-decision feedback. We establish a series of network capacity characterizations for these three fundamental types of network control laws. First of all, we prove that for global-knowledge/global-decision and network-flow/local-decision control where nodes know the information flow across the entire network, there exists a critical number $\big(3/2+\sqrt{2}\big)/\|A_{\mathrm{G}}\|_\infty$, where $3/2+\sqrt{2}$ is as known as the Xie-Guo constant and $A_{\mathrm{G}}$ is the network adjacency matrix, defining exactly how much uncertainty in the node dynamics can be overcome by feedback. Interestingly enough, the same feedback capacity can be achieved under max-consensus enhanced local flows where nodes only observe information flows from neighbors as well as extreme (max and min) states in the network. Next, for local-flow/local-decision control, we prove that there exists a structure-determined value being a lower bound of the network feedback capacity. These results reveal the important connection between network structure and fundamental capabilities of in-network feedback control.",http://arxiv.org/pdf/1701.03186v1
,,,,,,661,Online Summarization via Submodular and Convex Optimization,"Ehsan Elhamifar, M. Clara De Paolis Kaluza",,,
,,,,,,753,Deep MANTA: A Coarse-To-Fine Many-Task Network for Joint 2D and 3D Vehicle Analysis From Monocular Image,"Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, CÃ©line TeuliÃ¨re, Thierry Chateau",Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image,"In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.",http://arxiv.org/pdf/1703.07570v1
,,,,,,1329,Improving Pairwise Ranking for Multi-Label Image Classification,"Yuncheng Li, Yale Song, Jiebo Luo",Improving Pairwise Ranking for Multi-label Image Classification,"Learning to rank has recently emerged as an attractive technique to train deep convolutional neural networks for various computer vision tasks. Pairwise ranking, in particular, has been successful in multi-label image classification, achieving state-of-the-art results on various benchmarks. However, most existing approaches use the hinge loss to train their models, which is non-smooth and thus is difficult to optimize especially with deep networks. Furthermore, they employ simple heuristics, such as top-k or thresholding, to determine which labels to include in the output from a ranked list of labels, which limits their use in the real-world setting. In this work, we propose two techniques to improve pairwise ranking based multi-label image classification: (1) we propose a novel loss function for pairwise ranking, which is smooth everywhere and thus is easier to optimize; and (2) we incorporate a label decision module into the model, estimating the optimal confidence thresholds for each visual concept. We provide theoretical analyses of our loss function in the Bayes consistency and risk minimization framework, and show its benefit over existing pairwise ranking formulations. We demonstrate the effectiveness of our approach on three large-scale datasets, VOC2007, NUS-WIDE and MS-COCO, achieving the best reported results in the literature.",http://arxiv.org/pdf/1704.03135v3
,,,,,,1709,Active Convolution: Learning the Shape of Convolution for Image Classification,"Yunho Jeon, Junmo Kim",Active Convolution: Learning the Shape of Convolution for Image Classification,"In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline.",http://arxiv.org/pdf/1703.09076v1
,,,,,,1916,Linking Image and Text With 2-Way Nets,"Aviv Eisenschtat, Lior Wolf",,,
,,,,,,2100,Stacked Generative Adversarial Networks,"Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie",Stacked Generative Adversarial Networks,"In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.",http://arxiv.org/pdf/1612.04357v4
,,,,,,2110,Image Splicing Detection via Camera Response Function Analysis,"Can Chen, Scott McCloskey, Jingyi Yu",,,
,,,,,,2121,Building a Regular Decision Boundary With Deep Networks,Edouard Oyallon,Building a Regular Decision Boundary with Deep Networks,"In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow.",http://arxiv.org/pdf/1703.01775v1
,,,,,,2516,More Is Less: A More Complicated Network With Less Inference Complexity,"Xuanyi Dong, Junshi Huang, Yi Yang, Shuicheng Yan",More is Less: A More Complicated Network with Less Inference Complexity,"In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity. The core idea is to equip each original convolutional layer with another low-cost collaborative layer (LCCL), and the element-wise multiplication of the ReLU outputs of these two parallel layers produces the layer-wise output. The combined layer is potentially more discriminative than the original convolutional layer, and its inference is faster for two reasons: 1) the zero cells of the LCCL feature maps will remain zero after element-wise multiplication, and thus it is safe to skip the calculation of the corresponding high-cost convolution in the original convolutional layer, 2) LCCL is very fast if it is implemented as a 1*1 convolution or only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012 benchmarks show that our proposed network structure can accelerate the inference process by 32\% on average with negligible performance drop.",http://arxiv.org/pdf/1703.08651v2
,,,,,,2621,"Joint Graph Decomposition and Node Labeling: Problem, Algorithms, Applications","Evgeny Levinkov, Jonas Uhrig, Siyu Tang, Mohamed Omran, Eldar Insafutdinov, Alexander Kirillov, Carsten Rother, Thomas Brox, Bernt Schiele, Bjoern Andres","Joint Graph Decomposition and Node Labeling: Problem, Algorithms, Applications","We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph. This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, the problem we state generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define two local search algorithms that converge monotonously to a local optimum, offering a feasible solution at any time. To demonstrate their effectiveness in tackling computer vision tasks, we apply these algorithms to instances of the problem that we construct from published data, using published algorithms. We report state-of-the-art application-specific accuracy for the three above-mentioned applications.",http://arxiv.org/pdf/1611.04399v2
,,,,,,2721,Scale-Aware Face Detection,"Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, Xiaolin Hu",Context-aware CNNs for person head detection,"Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under a full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent local R-CNN object detector, we extend it with two types of contextual cues. First, we leverage person-scene relations and propose a Global CNN model trained to predict positions and scales of heads directly from the full image. Second, we explicitly model pairwise relations among objects and train a Pairwise CNN model using a structured-output surrogate loss. The Local, Global and Pairwise models are combined into a joint CNN framework. To train and test our full model, we introduce a large dataset composed of 369,846 human heads annotated in 224,740 movie frames. We evaluate our method and demonstrate improvements of person head detection against several recent baselines in three datasets. We also show improvements of the detection speed provided by our model.",http://arxiv.org/pdf/1511.07917v1
,,,,,,3350,Deep Unsupervised Similarity Learning Using Partially Ordered Sets,"Miguel A. Bautista, Artsiom Sanakoyeu, BjÃ¶rn Ommer",Deep Unsupervised Similarity Learning using Partially Ordered Sets,"Unsupervised learning of visual similarities is of paramount importance to computer vision, particularly due to lacking training data for fine-grained similarities. Deep learning of similarities is often based on relationships between pairs or triplets of samples. Many of these relations are unreliable and mutually contradicting, implying inconsistencies when trained without supervision information that relates different tuples or triplets to each other. To overcome this problem, we use local estimates of reliable (dis-)similarities to initially group samples into compact surrogate classes and use local partial orders of samples to classes to link classes to each other. Similarity learning is then formulated as a partial ordering task with soft correspondences of all samples to classes. Adopting a strategy of self-supervision, a CNN is trained to optimally represent samples in a mutually consistent manner while updating the classes. The similarity learning and grouping procedure are integrated in a single model and optimized jointly. The proposed unsupervised approach shows competitive performance on detailed pose estimation and object classification.",http://arxiv.org/pdf/1704.02268v3
,,,,,,3692,Generative Hierarchical Learning of Sparse FRAME Models,"Jianwen Xie, Yifei Xu, Erik Nijkamp, Ying Nian Wu, Song-Chun Zhu",,,
,,,,,Object Recognition & Scene Understanding,69,Generating Holistic 3D Scene Abstractions for Text-Based Image Retrieval,"Ang Li, Jin Sun, Joe Yue-Hei Ng, Ruichi Yu, Vlad I. Morariu, Larry S. Davis",Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval,"Spatial relationships between objects provide important information for text-based image retrieval. As users are more likely to describe a scene from a real world perspective, using 3D spatial relationships rather than 2D relationships that assume a particular viewing direction, one of the main challenges is to infer the 3D structure that bridges images with users' text descriptions. However, direct inference of 3D structure from images requires learning from large scale annotated data. Since interactions between objects can be reduced to a limited set of atomic spatial relations in 3D, we study the possibility of inferring 3D structure from a text description rather than an image, applying physical relation models to synthesize holistic 3D abstract object layouts satisfying the spatial constraints present in a textual description. We present a generic framework for retrieving images from a textual description of a scene by matching images with these generated abstract object layouts. Images are ranked by matching object detection outputs (bounding boxes) to 2D layout candidates (also represented by bounding boxes) which are obtained by projecting the 3D scenes with sampled camera directions. We validate our approach using public indoor scene datasets and show that our method outperforms baselines built upon object occurrence histograms and learned 2D pairwise relations.",http://arxiv.org/pdf/1611.09392v2
,,,,,,446,Perceptual Generative Adversarial Networks for Small Object Detection,"Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi Feng, Shuicheng Yan",Perceptual Generative Adversarial Networks for Small Object Detection,"Detecting small objects is notoriously challenging due to their low resolution and noisy representation. Existing object detection pipelines usually detect small objects through learning representations of all the objects at multiple scales. However, the performance gain of such ad hoc architectures is usually limited to pay off the computational cost. In this work, we address the small object detection problem by developing a single architecture that internally lifts representations of small objects to ""super-resolved"" ones, achieving similar characteristics as large objects and thus more discriminative for detection. For this purpose, we propose a new Perceptual Generative Adversarial Network (Perceptual GAN) model that improves small object detection through narrowing representation difference of small objects from the large ones. Specifically, its generator learns to transfer perceived poor representations of the small objects to super-resolved ones that are similar enough to real large objects to fool a competing discriminator. Meanwhile its discriminator competes with the generator to identify the generated representation and imposes an additional perceptual requirement - generated representations of small objects must be beneficial for detection purpose - on the generator. Extensive evaluations on the challenging Tsinghua-Tencent 100K and the Caltech benchmark well demonstrate the superiority of Perceptual GAN in detecting small objects, including traffic signs and pedestrians, over well-established state-of-the-arts.",http://arxiv.org/pdf/1706.05274v1
,,,,,,605,Emotion Recognition in Context,"Ronak Kosti, Jose M. Alvarez, Adria Recasens, Agata Lapedriza",End-to-End Multimodal Emotion Recognition using Deep Neural Networks,"Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-Term Memory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where - by also taking advantage of the correlations of the each of the streams - we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.",http://arxiv.org/pdf/1704.08619v1
,,,,,,610,Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework,"Jongyoo Kim, Sanghoon Lee",,,
,,,,,,792,Dense Captioning With Joint Inference and Visual Context,"Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li",Dense Captioning with Joint Inference and Visual Context,"Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model, compact and efficient, achieves state-of-the-art accuracy on Visual Genome for dense captioning with a relative gain of 73\% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning.",http://arxiv.org/pdf/1611.06949v1
,,,,,,1062,CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,"Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick",CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,"When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.",http://arxiv.org/pdf/1612.06890v1
,,,,,,1325,Cross-View Image Matching for Geo-Localization in Urban Environments,"Yicong Tian, Chen Chen, Mubarak Shah",Cross-View Image Matching for Geo-localization in Urban Environments,"In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the $k$ nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.",http://arxiv.org/pdf/1703.07815v1
,,,,,,1440,Matrix Tri-Factorization With Manifold Regularizations for Zero-Shot Learning,"Xing Xu, Fumin Shen, Yang Yang, Dongxiang Zhang, Heng Tao Shen, Jingkuan Song",,,
,,,,,,1721,Self-Supervised Learning of Visual Features Through Embedding Images Into Text Topic Spaces,"Lluis Gomez, Yash Patel, MarÃ§al RusiÃ±ol, Dimosthenis Karatzas, C. V. Jawahar",Self-supervised learning of visual features through embedding images into text topic spaces,"End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features by mining a large scale corpus of multi-modal (text and image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in the text corpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches.",http://arxiv.org/pdf/1705.08631v1
,,,,,,2328,Learning Spatial Regularization With Image-Level Supervisions for Multi-Label Image Classification,"Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, Xiaogang Wang",Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification,"Multi-label image classification is a fundamental but challenging task in computer vision. Great progress has been achieved by exploiting semantic relations between labels in recent years. However, conventional approaches are unable to model the underlying spatial relations between labels in multi-label images, because spatial annotations of the labels are generally not provided. In this paper, we propose a unified deep neural network that exploits both semantic and spatial relations between labels with only image-level supervisions. Given a multi-label image, our proposed Spatial Regularization Network (SRN) generates attention maps for all labels and captures the underlying relations between them via learnable convolutions. By aggregating the regularized classification results with original results by a ResNet-101 network, the classification performance can be consistently improved. The whole deep neural network is trained end-to-end with only image-level annotations, thus requires no additional efforts on image annotations. Extensive evaluations on 3 public datasets with different types of labels show that our approach significantly outperforms state-of-the-arts and has strong generalization capability. Analysis of the learned SRN model demonstrates that it can effectively capture both semantic and spatial relations of labels for improving classification performance.",http://arxiv.org/pdf/1702.05891v2
,,,,,,2650,Semantically Consistent Regularization for Zero-Shot Recognition,"Pedro Morgado, Nuno Vasconcelos",Semantically Consistent Regularization for Zero-Shot Recognition,"The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.",http://arxiv.org/pdf/1704.03039v1
,,,,,,2689,Can Walking and Measuring Along Chord Bunches Better Describe Leaf Shapes?,"Bin Wang, Yongsheng Gao, Changming Sun, Michael Blumenstein, John La Salle",,,
,,,,,Video Analytics,159,Self-Learning Scene-Specific Pedestrian Detectors Using a Progressive Latent Model,"Qixiang Ye, Tianliang Zhang, Wei Ke, Qiang Qiu, Jie Chen, Guillermo Sapiro, Baochang Zhang",Self-learning Scene-specific Pedestrian Detectors using a Progressive Latent Model,"In this paper, a self-learning approach is proposed towards solving scene-specific pedestrian detection problem without any human' annotation involved. The self-learning approach is deployed as progressive steps of object discovery, object enforcement, and label propagation. In the learning procedure, object locations in each frame are treated as latent variables that are solved with a progressive latent model (PLM). Compared with conventional latent models, the proposed PLM incorporates a spatial regularization term to reduce ambiguities in object proposals and to enforce object localization, and also a graph-based label propagation to discover harder instances in adjacent frames. With the difference of convex (DC) objective functions, PLM can be efficiently optimized with a concave-convex programming and thus guaranteeing the stability of self-learning. Extensive experiments demonstrate that even without annotation the proposed self-learning approach outperforms weakly supervised learning approaches, while achieving comparable performance with transfer learning and fully supervised approaches.",http://arxiv.org/pdf/1611.07544v1
,,,,,,328,Predictive-Corrective Networks for Action Detection,"Achal Dave, Olga Russakovsky, Deva Ramanan",Predictive-Corrective Networks for Action Detection,"While deep feature learning has revolutionized techniques for static-image understanding, the same does not quite hold for video processing. Architectures and optimization techniques used for video are largely based off those for static images, potentially underutilizing rich video information. In this work, we rethink both the underlying network architecture and the stochastic learning paradigm for temporal data. To do so, we draw inspiration from classic theory on linear dynamic systems for modeling time series. By extending such models to include nonlinear mappings, we derive a series of novel recurrent neural networks that sequentially make top-down predictions about the future and then correct those predictions with bottom-up observations. Predictive-corrective networks have a number of desirable properties: (1) they can adaptively focus computation on ""surprising"" frames where predictions require large corrections, (2) they simplify learning in that only ""residual-like"" corrective terms need to be learned over time and (3) they naturally decorrelate an input data stream in a hierarchical fashion, producing a more reliable signal for learning at each layer of a network. We provide an extensive analysis of our lightweight and interpretable framework, and demonstrate that our model is competitive with the two-stream network on three challenging datasets without the need for computationally expensive optical flow.",http://arxiv.org/pdf/1704.03615v1
,,,,,,356,Budget-Aware Deep Semantic Video Segmentation,"Behrooz Mahasseni, Sinisa Todorovic, Alan Fern",,,
,,,,,,379,Unified Embedding and Metric Learning for Zero-Exemplar Event Detection,"Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders",Unified Embedding and Metric Learning for Zero-Exemplar Event Detection,"Event detection in unconstrained videos is conceived as a content-based video retrieval with two modalities: textual and visual. Given a text describing a novel event, the goal is to rank related videos accordingly. This task is zero-exemplar, no video examples are given to the novel event.   Related works train a bank of concept detectors on external data sources. These detectors predict confidence scores for test videos, which are ranked and retrieved accordingly. In contrast, we learn a joint space in which the visual and textual representations are embedded. The space casts a novel event as a probability of pre-defined events. Also, it learns to measure the distance between an event and its related videos.   Our model is trained end-to-end on publicly available EventNet. When applied to TRECVID Multimedia Event Detection dataset, it outperforms the state-of-the-art by a considerable margin.",http://arxiv.org/pdf/1705.02148v1
,,,,,,569,Spatiotemporal Pyramid Network for Video Action Recognition,"Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S. Yu",,,
,,,,,,810,"ER3: A Unified Framework for Event Retrieval, Recognition and Recounting","Zhanning Gao, Gang Hua, Dongqing Zhang, Nebojsa Jojic, Le Wang, Jianru Xue, Nanning Zheng",,,
,,,,,,1345,FusionSeg: Learning to Combine Motion and Appearance for Fully Automatic Segmentation of Generic Objects in Videos,"Suyog Dutt Jain, Bo Xiong, Kristen Grauman",FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos,"We propose an end-to-end learning framework for segmenting generic objects in videos. Our method learns to combine appearance and motion information to produce pixel level segmentation masks for all prominent objects in videos. We formulate this task as a structured prediction problem and design a two-stream fully convolutional neural network which fuses together motion and appearance in a unified framework. Since large-scale video datasets with pixel level segmentations are problematic, we show how to bootstrap weakly annotated videos together with existing image recognition datasets for training. Through experiments on three challenging video segmentation benchmarks, our method substantially improves the state-of-the-art for segmenting generic (unseen) objects. Code and pre-trained models are available on the project website.",http://arxiv.org/pdf/1701.05384v2
,,,,,,1990,"Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach","Aidean Sharghi, Jacob S. Laurel, Boqing Gong",,,
,,,,,,2945,Flexible Spatio-Temporal Networks for Video Prediction,"Chaochao Lu, Michael Hirsch, Bernhard SchÃ¶lkopf",,,
,,,,,,3131,Temporal Action Co-Segmentation in 3D Motion Capture Data and Videos,"Konstantinos Papoutsakis, Costas Panagiotakis, Antonis A. Argyros",,,
"Sunday, July 23, 2017",0830â€“1000,Kamehameha III,9,Spotlight 2-1A,Machine Learning 2,110,Dual Attention Networks for Multimodal Reasoning and Matching,"Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim",Dual Attention Networks for Multimodal Reasoning and Matching,"We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.",http://arxiv.org/pdf/1611.00471v2
,,,,,,122,DESIRE: Distant Future Prediction in Dynamic Scenes With Interacting Agents,"Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B. Choy, Philip H. S. Torr, Manmohan Chandraker",DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents,"We introduce a Deep Stochastic IOC RNN Encoderdecoder framework, DESIRE, for the task of future predictions of multiple interacting agents in dynamic scenes. DESIRE effectively predicts future locations of objects in multiple scenes by 1) accounting for the multi-modal nature of the future prediction (i.e., given the same context, future may vary), 2) foreseeing the potential future outcomes and make a strategic prediction based on that, and 3) reasoning not only from the past motion history, but also from the scene context as well as the interactions among the agents. DESIRE achieves these in a single end-to-end trainable neural network model, while being computationally efficient. The model first obtains a diverse set of hypothetical future prediction samples employing a conditional variational autoencoder, which are ranked and refined by the following RNN scoring-regression module. Samples are scored by accounting for accumulated future rewards, which enables better long-term strategic decisions similar to IOC frameworks. An RNN scene context fusion module jointly captures past motion histories, the semantic scene context and interactions among multiple agents. A feedback mechanism iterates over the ranking and refinement to further boost the prediction accuracy. We evaluate our model on two publicly available datasets: KITTI and Stanford Drone Dataset. Our experiments show that the proposed model significantly improves the prediction accuracy compared to other baseline methods.",http://arxiv.org/pdf/1704.04394v1
,,,,,,347,Interpretable Structure-Evolving LSTM,"Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing",Interpretable Structure-Evolving LSTM,"This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.",http://arxiv.org/pdf/1703.03055v1
,,,,,,804,ShapeOdds: Variational Bayesian Learning of Generative Shape Models,"Shireen Elhabian, Ross Whitaker",,,
,,,,,,1339,Fast Video Classification via Adaptive Cascading of Deep Models,"Haichen Shen, Seungyeop Han, Matthai Philipose, Arvind Krishnamurthy",Fast Video Classification via Adaptive Cascading of Deep Models,"Recent advances have enabled ""oracle"" classifiers that can classify across many classes and input distributions with high accuracy without retraining. However, these classifiers are relatively heavyweight, so that applying them to classify video is costly. We show that day-to-day video exhibits highly skewed class distributions over the short term, and that these distributions can be classified by much simpler models. We formulate the problem of detecting the short-term skews online and exploiting models based on it as a new sequential decision making problem dubbed the Online Bandit Problem, and present a new algorithm to solve it. When applied to recognizing faces in TV shows and movies, we realize end-to-end classification speedups of 2.5-8.5x/2.8-12.7x (on GPU/CPU) relative to a state-of-the-art convolutional neural network, at competitive accuracy.",http://arxiv.org/pdf/1611.06453v1
,,,,,,2271,Deep Metric Learning via Facility Location,"Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy",Deep Metric Learning via Facility Location,"Learning the representation and the similarity metric in an end-to-end fashion with deep networks have demonstrated outstanding results for clustering and retrieval. However, these recent approaches still suffer from the performance degradation stemming from the local metric training procedure which is unaware of the global structure of the embedding space.   We propose a global metric learning scheme for optimizing the deep metric embedding with the learnable clustering function and the clustering metric (NMI) in a novel structured prediction framework.   Our experiments on CUB200-2011, Cars196, and Stanford online products datasets show state of the art performance both on the clustering and retrieval tasks measured in the NMI and Recall@K evaluation metrics.",http://arxiv.org/pdf/1612.01213v2
,,,,,,3050,Semi-Supervised Deep Learning for Monocular Depth Map Prediction,"Yevhen Kuznietsov, JÃ¶rg StÃ_ckler, Bastian Leibe",Semi-Supervised Deep Learning for Monocular Depth Map Prediction,"Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.",http://arxiv.org/pdf/1702.02706v3
,,,,,,3568,Weakly Supervised Semantic Segmentation Using Web-Crawled Videos,"Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, Bohyung Han",Weakly Supervised Semantic Segmentation using Web-Crawled Videos,"We propose a novel algorithm for weakly supervised semantic segmentation based on image-level class labels only. In weakly supervised setting, it is commonly observed that trained model overly focuses on discriminative parts rather than the entire object area. Our goal is to overcome this limitation with no additional human intervention by retrieving videos relevant to target class labels from web repository, and generating segmentation labels from the retrieved videos to simulate strong supervision for semantic segmentation. During this process, we take advantage of image classification with discriminative localization technique to reject false alarms in retrieved videos and identify relevant spatio-temporal volumes within retrieved videos. Although the entire procedure does not require any additional supervision, the segmentation annotations obtained from videos are sufficiently strong to learn a model for semantic segmentation. The proposed algorithm substantially outperforms existing methods based on the same level of supervision and is even as competitive as the approaches relying on extra annotations.",http://arxiv.org/pdf/1701.00352v2
,,,,Oral 2-1A,,720,Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach,"Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, Lizhen Qu",Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach,"We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures --- stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers --- demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.",http://arxiv.org/pdf/1609.03683v2
,,,,,,767,Learning From Simulated and Unsupervised Images Through Adversarial Training,"Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, Russell Webb",Learning from Simulated and Unsupervised Images through Adversarial Training,"With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",http://arxiv.org/pdf/1612.07828v1
,,,,,,932,Inverse Compositional Spatial Transformer Networks,"Chen-Hsuan Lin, Simon Lucey",Inverse Compositional Spatial Transformer Networks,"In this paper, we establish a theoretical connection between the classical Lucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.",http://arxiv.org/pdf/1612.03897v1
,,,,,,1954,Densely Connected Convolutional Networks,"Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",Convolution in Convolution for Network in Network,"Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and $ 1\times 1 $ convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method.",http://arxiv.org/pdf/1603.06759v1
"Sunday, July 23, 2017","0830â€“1000","KalÄÅkaua Ballroom A-B","10","Spotlight 2-1B","Computational Photography",205,Video Frame Interpolation via Adaptive Convolution,"Simon Niklaus, Long Mai, Feng Liu",Video Frame Interpolation via Adaptive Convolution,"Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.",http://arxiv.org/pdf/1703.07514v1
,,,,,,340,FastMask: Segment Multi-Scale Object Candidates in One Shot,"Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha",FastMask: Segment Multi-scale Object Candidates in One Shot,"Objects appear to scale differently in natural images. This fact requires methods dealing with object-centric tasks (e.g. object proposal) to have robust performance over variances in object scales. In the paper, we present a novel segment proposal framework, namely FastMask, which takes advantage of hierarchical features in deep convolutional neural networks to segment multi-scale objects in one shot. Innovatively, we adapt segment proposal network into three different functional components (body, neck and head). We further propose a weight-shared residual neck module as well as a scale-tolerant attentional head module for efficient one-shot inference. On MS COCO benchmark, the proposed FastMask outperforms all state-of-the-art segment proposal methods in average recall being 2~5 times faster. Moreover, with a slight trade-off in accuracy, FastMask can segment objects in near real time (~13 fps) with 800*600 resolution images, demonstrating its potential in practical applications. Our implementation is available on https://github.com/voidrank/FastMask.",http://arxiv.org/pdf/1612.08843v4
,,,,,,571,Reconstructing Transient Images From Single-Photon Sensors,"Matthew O'Toole, Felix Heide, David B. Lindell, Kai Zang, Steven Diamond, Gordon Wetzstein",,,
,,,,,,1043,Deep Sketch Hashing: Fast Free-Hand Sketch-Based Image Retrieval,"Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, Ling Shao",Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval,"Free-hand sketch-based image retrieval (SBIR) is a specific cross-view retrieval task, in which queries are abstract and ambiguous sketches while the retrieval database is formed with natural images. Work in this area mainly focuses on extracting representative and shared features for sketches and natural images. However, these can neither cope well with the geometric distortion between sketches and images nor be feasible for large-scale SBIR due to the heavy continuous-valued distance computation. In this paper, we speed up SBIR by introducing a novel binary coding method, named \textbf{Deep Sketch Hashing} (DSH), where a semi-heterogeneous deep architecture is proposed and incorporated into an end-to-end binary coding framework. Specifically, three convolutional neural networks are utilized to encode free-hand sketches, natural images and, especially, the auxiliary sketch-tokens which are adopted as bridges to mitigate the sketch-image geometric distortion. The learned DSH codes can effectively capture the cross-view similarities as well as the intrinsic semantic correlations between different categories. To the best of our knowledge, DSH is the first hashing work specifically designed for category-level SBIR with an end-to-end deep architecture. The proposed DSH is comprehensively evaluated on two large-scale datasets of TU-Berlin Extension and Sketchy, and the experiments consistently show DSH's superior SBIR accuracies over several state-of-the-art methods, while achieving significantly reduced retrieval time and memory footprint.",http://arxiv.org/pdf/1703.05605v1
,,,,,,1637,DeshadowNet: A Multi-Context Embedding Deep Network for Shadow Removal,"Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, Rynson W. H. Lau",,,
,,,,,,2120,Illuminant-Camera Communication to Observe Moving Objects Under Strong External Light by Spread Spectrum Modulation,"Ryusuke Sagawa, Yutaka Satoh",,,
,,,,,,2133,Photorealistic Facial Texture Inference Using Deep Neural Networks,"Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, Hao Li",Photorealistic Facial Texture Inference Using Deep Neural Networks,"We present a data-driven inference method that can synthesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild. After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area. To extract the fine appearance details from this incomplete input, we introduce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolutional neural network. We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial detail description of the entire face. A complete and photorealistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate successful face reconstructions from a wide range of low resolution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study.",http://arxiv.org/pdf/1612.00523v1
,,,,,,3428,The Geometry of First-Returning Photons for Non-Line-Of-Sight Imaging,"Chia-Yin Tsai, Kiriakos N. Kutulakos, Srinivasa G. Narasimhan, Aswin C. Sankaranarayanan",,,
,,,,Oral 2-1B,,833,Unrolling the Shutter: CNN to Correct Motion Distortions,"Vijay Rengarajan, Yogesh Balaji, A. N. Rajagopalan",,,
,,,,,,1572,Light Field Blind Motion Deblurring,"Pratul P. Srinivasan, Ren Ng, Ravi Ramamoorthi",Light Field Blind Motion Deblurring,"We study the problem of deblurring light fields of general 3D scenes captured under 3D camera motion and present both theoretical and practical contributions. By analyzing the motion-blurred light field in the primal and Fourier domains, we develop intuition into the effects of camera motion on the light field, show the advantages of capturing a 4D light field instead of a conventional 2D image for motion deblurring, and derive simple methods of motion deblurring in certain cases. We then present an algorithm to blindly deblur light fields of general scenes without any estimation of scene geometry, and demonstrate that we can recover both the sharp light field and the 3D camera motion path of real and synthetically-blurred light fields.",http://arxiv.org/pdf/1704.05416v1
,,,,,,2872,Computational Imaging on the Electric Grid,"Mark Sheinin, Yoav Y. Schechner, Kiriakos N. Kutulakos",Study of noise effects in electrical impedance tomography with resistor networks,"We present a study of the numerical solution of the two dimensional electrical impedance tomography problem, with noisy measurements of the Dirichlet to Neumann map. The inversion uses parametrizations of the conductivity on optimal grids. The grids are optimal in the sense that finite volume discretizations on them give spectrally accurate approximations of the Dirichlet to Neumann map. The approximations are Dirichlet to Neumann maps of special resistor networks, that are uniquely recoverable from the measurements. Inversion on optimal grids has been proposed and analyzed recently, but the study of noise effects on the inversion has not been carried out. In this paper we present a numerical study of both the linearized and the nonlinear inverse problem. We take three different parametrizations of the unknown conductivity, with the same number of degrees of freedom. We obtain that the parametrization induced by the inversion on optimal grids is the most efficient of the three, because it gives the smallest standard deviation of the maximum a posteriori estimates of the conductivity, uniformly in the domain. For the nonlinear problem we compute the mean and variance of the maximum a posteriori estimates of the conductivity, on optimal grids. For small noise, we obtain that the estimates are unbiased and their variance is very close to the optimal one, given by the Cramer-Rao bound. For larger noise we use regularization and quantify the trade-off between reducing the variance and introducing bias in the solution. Both the full and partial measurement setups are considered.",http://arxiv.org/pdf/1105.1183v2
,,,,,,3565,Deep Outdoor Illumination Estimation,"Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, Jean-FranÃ§ois Lalonde",Deep Outdoor Illumination Estimation,"We present a CNN-based technique to estimate high-dynamic range outdoor illumination from a single low dynamic range image. To train the CNN, we leverage a large dataset of outdoor panoramas. We fit a low-dimensional physically-based outdoor illumination model to the skies in these panoramas giving us a compact set of parameters (including sun position, atmospheric conditions, and camera parameters). We extract limited field-of-view images from the panoramas, and train a CNN with this large set of input image--output lighting parameter pairs. Given a test image, this network can be used to infer illumination parameters that can, in turn, be used to reconstruct an outdoor illumination environment map. We demonstrate that our approach allows the recovery of plausible illumination conditions and enables photorealistic virtual object insertion from a single image. An extensive evaluation on both the panorama dataset and captured HDR environment maps shows that our technique significantly outperforms previous solutions to this problem.",http://arxiv.org/pdf/1611.06403v2
"Sunday, July 23, 2017",0830â€“1000,KalÄÅkaua Ballroom C,11,Spotlight 2-1C,3D Vision 2,270,Efficient Solvers for Minimal Problems by Syzygy-Based Reduction,"Viktor Larsson, Kalle Ã…strÃ¶m, Magnus Oskarsson",,,
,,,,,,439,HSfM: Hybrid Structure-from-Motion,"Hainan Cui, Xiang Gao, Shuhan Shen, Zhanyi Hu",,,
,,,,,,1071,Efficient Global Point Cloud Alignment Using Bayesian Nonparametric Mixtures,"Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III",Efficient Global Point Cloud Alignment using Bayesian Nonparametric Mixtures,"Point cloud alignment is a common problem in computer vision and robotics, with applications ranging from 3D object recognition to reconstruction. We propose a novel approach to the alignment problem that utilizes Bayesian nonparametrics to describe the point cloud and surface normal densities, and branch and bound (BB) optimization to recover the relative transformation. BB uses a novel, refinable, near-uniform tessellation of rotation space using 4D tetrahedra, leading to more efficient optimization compared to the common axis-angle tessellation. We provide objective function bounds for pruning given the proposed tessellation, and prove that BB converges to the optimum of the cost function along with providing its computational complexity. Finally, we empirically demonstrate the efficiency of the proposed approach as well as its robustness to real-world conditions such as missing data and partial overlap.",http://arxiv.org/pdf/1603.04868v3
,,,,,,1992,"A New Rank Constraint on Multi-View Fundamental Matrices, and Its Application to Camera Location Recovery","Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W. Jacobs, Amit Singer, Ronen Basri","A New Rank Constraint on Multi-view Fundamental Matrices, and its Application to Camera Location Recovery","Accurate estimation of camera matrices is an important step in structure from motion algorithms. In this paper we introduce a novel rank constraint on collections of fundamental matrices in multi-view settings. We show that in general, with the selection of proper scale factors, a matrix formed by stacking fundamental matrices between pairs of images has rank 6. Moreover, this matrix forms the symmetric part of a rank 3 matrix whose factors relate directly to the corresponding camera matrices. We use this new characterization to produce better estimations of fundamental matrices by optimizing an L1-cost function using Iterative Re-weighted Least Squares and Alternate Direction Method of Multiplier. We further show that this procedure can improve the recovery of camera locations, particularly in multi-view settings in which fewer images are available.",http://arxiv.org/pdf/1702.03023v1
,,,,,,2129,IM2CAD,"Hamid Izadinia, Qi Shan, Steven M. Seitz",,,
,,,,,,2513,ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes,"Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias NieÃŸner",ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes,"A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at http://www.scan-net.org.",http://arxiv.org/pdf/1702.04405v2
,,,,,,2807,Noise Robust Depth From Focus Using a Ring Difference Filter,"Jaeheung Surh, Hae-Gon Jeon, Yunwon Park, Sunghoon Im, Hyowon Ha, In So Kweon",,,
,,,,,,3074,Group-Wise Point-Set Registration Based on RÃ©nyi's Second Order Entropy,"Luis G. Sanchez Giraldo, Erion Hasanbelliu, Murali Rao, Jose C. Principe",,,
,,,,Oral 2-1C,,190,A Point Set Generation Network for 3D Object Reconstruction From a Single Image,"Haoqiang Fan, Hao Su, Leonidas J. Guibas",A Point Set Generation Network for 3D Object Reconstruction from a Single Image,"Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.",http://arxiv.org/pdf/1612.00603v2
,,,,,,1931,3D Point Cloud Registration for Localization Using a Deep Neural Network Auto-Encoder,"Gil Elbaz, Tamar Avraham, Anath Fischer",,,
,,,,,,2630,Flight Dynamics-Based Recovery of a UAV Trajectory Using Ground Cameras,"Artem Rozantsev, Sudipta N. Sinha, Debadeepta Dey, Pascal Fua",Flight Dynamics-based Recovery of a UAV Trajectory using Ground Cameras,"We propose a new method to estimate the 6-dof trajectory of a flying object such as a quadrotor UAV within a 3D airspace monitored using multiple fixed ground cameras. It is based on a new structure from motion formulation for the 3D reconstruction of a single moving point with known motion dynamics. Our main contribution is a new bundle adjustment procedure which in addition to optimizing the camera poses, regularizes the point trajectory using a prior based on motion dynamics (or specifically flight dynamics). Furthermore, we can infer the underlying control input sent to the UAV's autopilot that determined its flight trajectory.   Our method requires neither perfect single-view tracking nor appearance matching across views. For robustness, we allow the tracker to generate multiple detections per frame in each video. The true detections and the data association across videos is estimated using robust multi-view triangulation and subsequently refined during our bundle adjustment procedure. Quantitative evaluation on simulated data and experiments on real videos from indoor and outdoor scenes demonstrates the effectiveness of our method.",http://arxiv.org/pdf/1612.00192v1
,,,,,,3060,DSAC - Differentiable RANSAC for Camera Localization,"Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, Carsten Rother",DSAC - Differentiable RANSAC for Camera Localization,"RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.",http://arxiv.org/pdf/1611.05705v2
"Sunday, July 23, 2017",1000â€“1200,Kamehameha I,12,Poster 2-1,3D Computer Vision,299,Scalable Surface Reconstruction From Point Clouds With Extreme Scale and Density Diversity,"Christian Mostegel, Rudolf Prettenthaler, Friedrich Fraundorfer, Horst Bischof",Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity,"In this paper we present a scalable approach for robustly computing a 3D surface mesh from multi-scale multi-view stereo point clouds that can handle extreme jumps of point density (in our experiments three orders of magnitude). The backbone of our approach is a combination of octree data partitioning, local Delaunay tetrahedralization and graph cut optimization. Graph cut optimization is used twice, once to extract surface hypotheses from local Delaunay tetrahedralizations and once to merge overlapping surface hypotheses even when the local tetrahedralizations do not share the same topology.This formulation allows us to obtain a constant memory consumption per sub-problem while at the same time retaining the density independent interpolation properties of the Delaunay-based optimization. On multiple public datasets, we demonstrate that our approach is highly competitive with the state-of-the-art in terms of accuracy, completeness and outlier resilience. Further, we demonstrate the multi-scale potential of our approach by processing a newly recorded dataset with 2 billion points and a point density variation of more than four orders of magnitude - requiring less than 9GB of RAM per process.",http://arxiv.org/pdf/1705.00949v1
,,,,,,555,Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes With Deep Generative Networks,"Amir Arsalan Soltani, Haibin Huang, Jiajun Wu, Tejas D. Kulkarni, Joshua B. Tenenbaum",,,
,,,,,,718,General Models for Rational Cameras and the Case of Two-Slit Projections,"Matthew Trager, Bernd Sturmfels, John Canny, Martial Hebert, Jean Ponce",General models for rational cameras and the case of two-slit projections,"The rational camera model recently introduced in [19] provides a general methodology for studying abstract nonlinear imaging systems and their multi-view geometry. This paper builds on this framework to study ""physical realizations"" of rational cameras. More precisely, we give an explicit account of the mapping between between physical visual rays and image points (missing in the original description), which allows us to give simple analytical expressions for direct and inverse projections. We also consider ""primitive"" camera models, that are orbits under the action of various projective transformations, and lead to a general notion of intrinsic parameters. The methodology is general, but it is illustrated concretely by an in-depth study of two-slit cameras, that we model using pairs of linear projections. This simple analytical form allows us to describe models for the corresponding primitive cameras, to introduce intrinsic parameters with a clear geometric meaning, and to define an epipolar tensor characterizing two-view correspondences. In turn, this leads to new algorithms for structure from motion and self-calibration.",http://arxiv.org/pdf/1612.01160v4
,,,,,,1030,Accurate Depth and Normal Maps From Occlusion-Aware Focal Stack Symmetry,"Michael Strecke, Anna Alperovich, Bastian Goldluecke",,,
,,,,,,1213,A Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos,"Thomas SchÃ¶ps, Johannes L. SchÃ¶nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger",,,
,,,,,,1343,Non-Contact Full Field Vibration Measurement Based on Phase-Shifting,"Hiroyuki Kayaba, Yuji Kokumai",,,
,,,,,,2619,A Minimal Solution for Two-View Focal-Length Estimation Using Two Affine Correspondences,"Daniel Barath, Tekla Toth, Levente Hajder",A Minimal Solution for Two-view Focal-length Estimation using Two Affine Correspondences,"A minimal solution using two affine correspondences is presented to estimate the common focal length and the fundamental matrix between two semi-calibrated cameras - known intrinsic parameters except a common focal length. To the best of our knowledge, this problem is unsolved. The proposed approach extends point correspondence-based techniques with linear constraints derived from local affine transformations. The obtained multivariate polynomial system is efficiently solved by the hidden-variable technique. Observing the geometry of local affinities, we introduce novel conditions eliminating invalid roots. To select the best one out of the remaining candidates, a root selection technique is proposed outperforming the recent ones especially in case of high-level noise. The proposed 2-point algorithm is validated on both synthetic data and 104 publicly available real image pairs. A Matlab implementation of the proposed solution is included in the paper.",http://arxiv.org/pdf/1706.01649v1
,,,,,,3085,PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning,"Alexander Krull, Eric Brachmann, Sebastian Nowozin, Frank Michel, Jamie Shotton, Carsten Rother",PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning,"State-of-the-art computer vision algorithms often achieve efficiency by making discrete choices about which hypotheses to explore next. This allows allocation of computational resources to promising candidates, however, such decisions are non-differentiable. As a result, these algorithms are hard to train in an end-to-end fashion. In this work we propose to learn an efficient algorithm for the task of 6D object pose estimation. Our system optimizes the parameters of an existing state-of-the art pose estimation system using reinforcement learning, where the pose estimation system now becomes the stochastic policy, parametrized by a CNN. Additionally, we present an efficient training algorithm that dramatically reduces computation time. We show empirically that our learned pose estimation procedure makes better use of limited resources and improves upon the state-of-the-art on a challenging dataset. Our approach enables differentiable end-to-end training of complex algorithmic pipelines and learns to make optimal use of a given computational budget.",http://arxiv.org/pdf/1612.03779v2
,,,,,,3398,An Efficient Background Term for 3D Reconstruction and Tracking With Smooth Surface Models,"Mariano Jaimez, Thomas J. Cashman, Andrew Fitzgibbon, Javier Gonzalez-Jimenez, Daniel Cremers",,,
,,,,,Analyzing Humans in Images,1040,Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild,"Shan Li, Weihong Deng, JunPing Du",,,
,,,,,,1978,Procedural Generation of Videos to Train Deep Action Recognition Networks,"CÃ©sar Roberto de Souza, Adrien Gaidon, Yohann Cabon, Antonio Manuel LÃ_pez",Procedural Generation of Videos to Train Deep Action Recognition Networks,"Deep learning for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate the generation of synthetic training data for action recognition, as it has recently shown promising results for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines.We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". It contains a total of 37,536 videos, more than 1,000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix synthetic and real videos, even if the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos.",http://arxiv.org/pdf/1612.00881v1
,,,,,,2015,BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis,"Shanxin Yuan, Qi Ye, BjÃ¶rn Stenger, Siddhant Jain, Tae-Kyun Kim",BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis,"In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.",http://arxiv.org/pdf/1704.02612v1
,,,,,,3115,DenseReg: Fully Convolutional Dense Shape Regression In-The-Wild,"RÄ±za Alp GÃ_ler, George Trigeorgis, Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou, Iasonas Kokkinos",DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild,"In this paper we propose to learn a mapping from image pixels into a dense template grid through a fully convolutional network. We formulate this task as a regression problem and train our network by leveraging upon manually annotated facial landmarks ""in-the-wild"". We use such landmarks to establish a dense correspondence field between a three-dimensional object template and the input image, which then serves as the ground-truth for training our regression system. We show that we can combine ideas from semantic segmentation with regression networks, yielding a highly-accurate ""quantized regression"" architecture.   Our system, called DenseReg, allows us to estimate dense image-to-template correspondences in a fully convolutional manner. As such our network can provide useful correspondence information as a stand-alone system, while when used as an initialization for Statistical Deformable Models we obtain landmark localization results that largely outperform the current state-of-the-art on the challenging 300W benchmark. We thoroughly evaluate our method on a host of facial analysis tasks, and also demonstrate its use for other correspondence estimation tasks, such as modelling of the human ear. DenseReg code is made available at http://alpguler.com/DenseReg.html along with supplementary materials.",http://arxiv.org/pdf/1612.01202v1
,,,,,,3839,Adaptive Class Preserving Representation for Image Classification,"Jian-Xun Mi, Qiankun Fu, Weisheng Li",,,
,,,,,Applications,1639,Generalized Semantic Preserving Hashing for N-Label Cross-Modal Retrieval,"Devraj Mandal, Kunal N. Chaudhury, Soma Biswas",,,
,,,,,,2343,EAST: An Efficient and Accurate Scene Text Detector,"Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, Jiajun Liang",EAST: An Efficient and Accurate Scene Text Detector,"Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.",http://arxiv.org/pdf/1704.03155v1
,,,,,,3151,VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization,"Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, Hongkai Wen",,,
,,,,,Biomedical Image/Video Analysis,2808,Improving RANSAC-Based Segmentation Through CNN Encapsulation,"Dustin Morley, Hassan Foroosh",,,
,,,,,Computational Photography,32,Position Tracking for Virtual Reality Using Commodity WiFi,"Manikanta Kotaru, Sachin Katti",Position Tracking for Virtual Reality Using Commodity WiFi,"Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g., Samsung Gear). To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service. This paper presents WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes. We prototype WiCapture using off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm compared to sophisticated infrared based tracking systems like the Oculus, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment.",http://arxiv.org/pdf/1703.03468v1
,,,,,,787,Designing Illuminant Spectral Power Distributions for Surface Classification,"Henryk Blasinski, Joyce Farrell, Brian Wandell",,,
,,,,,,1627,One-Shot Hyperspectral Imaging Using Faced Reflectors,"Tsuyoshi Takatani, Takahito Aoto, Yasuhiro Mukaigawa",,,
,,,,,Image Motion & Tracking,884,Direct Photometric Alignment by Mesh Deformation,"Kaimo Lin, Nianjuan Jiang, Shuaicheng Liu, Loong-Fah Cheong, Minh Do, Jiangbo Lu",,,
,,,,,,1208,CNN-Based Patch Matching for Optical Flow With Thresholded Hinge Embedding Loss,"Christian Bailer, Kiran Varanasi, Didier Stricker",CNN-based Patch Matching for Optical Flow with Thresholded Hinge Embedding Loss,"Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.",http://arxiv.org/pdf/1607.08064v3
,,,,,,1696,Optical Flow Estimation Using a Spatial Pyramid Network,"Anurag Ranjan, Michael J. Black",Optical Flow Estimation using a Spatial Pyramid Network,"We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small (< 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning.",http://arxiv.org/pdf/1611.00850v2
,,,,,,3208,Deep Network Flow for Multi-Object Tracking,"Samuel Schulter, Paul Vernaza, Wongun Choi, Manmohan Chandraker",,,
,,,,,Low- & Mid-Level Vision,36,Material Classification Using Frequency- and Depth-Dependent Time-Of-Flight Distortion,"Kenichiro Tanaka, Yasuhiro Mukaigawa, Takuya Funatomi, Hiroyuki Kubo, Yasuyuki Matsushita, Yasushi Yagi",,,
,,,,,,585,Benchmarking Denoising Algorithms With Real Photographs,"Tobias PlÃ¶tz, Stefan Roth",,,
,,,,,,645,A Unified Approach of Multi-Scale Deep and Hand-Crafted Features for Defocus Estimation,"Jinsun Park, Yu-Wing Tai, Donghyeon Cho, In So Kweon",A Unified Approach of Multi-scale Deep and Hand-crafted Features for Defocus Estimation,"In this paper, we introduce robust and synergetic hand-crafted features and a simple but efficient deep feature from a convolutional neural network (CNN) architecture for defocus estimation. This paper systematically analyzes the effectiveness of different features, and shows how each feature can compensate for the weaknesses of other features when they are concatenated. For a full defocus map estimation, we extract image patches on strong edges sparsely, after which we use them for deep and hand-crafted feature extraction. In order to reduce the degree of patch-scale dependency, we also propose a multi-scale patch extraction strategy. A sparse defocus map is generated using a neural network classifier followed by a probability-joint bilateral filter. The final defocus map is obtained from the sparse defocus map with guidance from an edge-preserving filtered input image. Experimental results show that our algorithm is superior to state-of-the-art algorithms in terms of defocus estimation. Our work can be used for applications such as segmentation, blur magnification, all-in-focus image generation, and 3-D estimation.",http://arxiv.org/pdf/1704.08992v1
,,,,,,703,StyleBank: An Explicit Representation for Neural Image Style Transfer,"Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, Gang Hua",StyleBank: An Explicit Representation for Neural Image Style Transfer,"We propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style, for neural image style transfer. To transfer an image to a specific style, the corresponding filter bank is operated on top of the intermediate feature embedding produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt, where the learning is conducted in such a way that the auto-encoder does not encode any style information thanks to the flexibility introduced by the explicit filter bank representation. It also enables us to conduct incremental learning to add a new image style by learning a new filter bank while holding the auto-encoder fixed. The explicit style representation along with the flexible network design enables us to fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and hence provides new understanding on neural style transfer. Our method is easy to train, runs in real-time, and produces results that qualitatively better or at least comparable to existing methods.",http://arxiv.org/pdf/1703.09210v2
,,,,,,1148,Specular Highlight Removal in Facial Images,"Chen Li, Stephen Lin, Kun Zhou, Katsushi Ikeuchi",,,
,,,,,,1162,Image Super-Resolution via Deep Recursive Residual Network,"Ying Tai, Jian Yang, Xiaoming Liu",,,
,,,,,,1434,Deep Image Harmonization,"Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, Ming-Hsuan Yang",Deep Image Harmonization,"Compositing is one of the most common operations in photo editing. To generate realistic composites, the appearances of foreground and background need to be adjusted to make them compatible. Previous approaches to harmonize composites have focused on learning statistical relationships between hand-crafted appearance features of the foreground and background, which is unreliable especially when the contents in the two layers are vastly different. In this work, we propose an end-to-end deep convolutional neural network for image harmonization, which can capture both the context and semantic information of the composite images during harmonization. We also introduce an efficient way to collect large-scale and high-quality training data that can facilitate the training process. Experiments on the synthesized dataset and real composite images show that the proposed network outperforms previous state-of-the-art methods.",http://arxiv.org/pdf/1703.00069v1
,,,,,,1535,Learning Deep CNN Denoiser Prior for Image Restoration,"Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang",Learning Deep CNN Denoiser Prior for Image Restoration,"Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.",http://arxiv.org/pdf/1704.03264v1
,,,,,,1636,A Novel Tensor-Based Video Rain Streaks Removal Approach via Utilizing Discriminatively Intrinsic Priors,"Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian Deng, Yao Wang",,,
,,,,,,1703,"GMS: Grid-based Motion Statistics for Fast, Ultra-Robust Feature Correspondence","JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, Ming-Ming Cheng",,,
,,,,,,1710,Video Desnowing and Deraining Based on Matrix Decomposition,"Weihong Ren, Jiandong Tian, Zhi Han, Antoni Chan, Yandong Tang",,,
,,,,,,1989,Real-Time Video Super-Resolution With Spatio-Temporal Networks and Motion Compensation,"Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes Totz, Zehan Wang, Wenzhe Shi",Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation,"Convolutional neural networks have enabled accurate image super-resolution in real-time. However, recent attempts to benefit from temporal correlations in video super-resolution have been limited to naive or inefficient architectures. In this paper, we introduce spatio-temporal sub-pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real-time speed. Specifically, we discuss the use of early fusion, slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames. We also propose a novel joint motion compensation and video super-resolution algorithm that is orders of magnitude more efficient than competing methods, relying on a fast multi-resolution spatial transformer module that is end-to-end trainable. These contributions provide both higher accuracy and temporally more consistent videos, which we confirm qualitatively and quantitatively. Relative to single-frame models, spatio-temporal networks can either reduce the computational cost by 30% whilst maintaining the same quality or provide a 0.2dB gain for a similar computational cost. Results on publicly available datasets demonstrate that the proposed algorithms surpass current state-of-the-art performance in both accuracy and efficiency.",http://arxiv.org/pdf/1611.05250v2
,,,,,,2179,Deep Watershed Transform for Instance Segmentation,"Min Bai, Raquel Urtasun",Deep Watershed Transform for Instance Segmentation,"Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In our paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as basins in the energy map. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model more than doubles the performance of the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.",http://arxiv.org/pdf/1611.08303v2
,,,,,,2210,AnchorNet: A Weakly Supervised Network to Learn Geometry-Sensitive Features for Semantic Matching,"David Novotny, Diane Larlus, Andrea Vedaldi",AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching,"Despite significant progress of deep learning in recent years, state-of-the-art semantic matching methods still rely on legacy features such as SIFT or HoG. We argue that the strong invariance properties that are key to the success of recent deep architectures on the classification task make them unfit for dense correspondence tasks, unless a large amount of supervision is used. In this work, we propose a deep network, termed AnchorNet, that produces image representations that are well-suited for semantic matching. It relies on a set of filters whose response is geometrically consistent across different object instances, even in the presence of strong intra-class, scale, or viewpoint variations. Trained only with weak image-level labels, the final representation successfully captures information about the object structure and improves results of state-of-the-art semantic matching methods such as the deformable spatial pyramid or the proposal flow methods. We show positive results on the cross-instance matching task where different instances of the same object category are matched as well as on a new cross-category semantic matching task aligning pairs of instances each from a different object class.",http://arxiv.org/pdf/1704.04749v1
,,,,,,3134,Learning Diverse Image Colorization,"Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, David Forsyth",Learning Diverse Image Colorization,"Colorization is an ambiguous problem, with multiple viable colorizations for a single grey-level image. However, previous methods only produce the single most probable colorization. Our goal is to model the diversity intrinsic to the problem of colorization and produce multiple colorizations that display long-scale spatial co-ordination. We learn a low dimensional embedding of color fields using a variational autoencoder (VAE). We construct loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors. Finally, we build a conditional model for the multi-modal distribution between grey-level image and the color field embeddings. Samples from this conditional model result in diverse colorization. We demonstrate that our method obtains better diverse colorizations than a standard conditional variational autoencoder (CVAE) model, as well as a recently proposed conditional generative adversarial network (cGAN).",http://arxiv.org/pdf/1612.01958v2
,,,,,,3867,Awesome Typography: Statistics-Based Text Effects Transfer,"Shuai Yang, Jiaying Liu, Zhouhui Lian, Zongming Guo",Awesome Typography: Statistics-Based Text Effects Transfer,"In this work, we explore the problem of generating fantastic special-effects for the typography. It is quite challenging due to the model diversities to illustrate varied text effects for different characters. To address this issue, our key idea is to exploit the analytics on the high regularity of the spatial distribution for text effects to guide the synthesis process. Specifically, we characterize the stylized patches by their normalized positions and the optimal scales to depict their style elements. Our method first estimates these two features and derives their correlation statistically. They are then converted into soft constraints for texture transfer to accomplish adaptive multi-scale texture synthesis and to make style element distribution uniform. It allows our algorithm to produce artistic typography that fits for both local texture patterns and the global spatial distribution in the example. Experimental results demonstrate the superiority of our method for various text effects over conventional style transfer methods. In addition, we validate the effectiveness of our algorithm with extensive artistic typography library generation.",http://arxiv.org/pdf/1611.09026v2
,,,,,Machine Learning,71,Unsupervised Video Summarization With Adversarial LSTM Networks,"Behrooz Mahasseni, Michael Lam, Sinisa Todorovic",,,
,,,,,,221,Deep TEN: Texture Encoding Network,"Hang Zhang, Jia Xue, Kristin Dana",Deep TEN: Texture Encoding Network,"We propose a Deep Texture Encoding Network (Deep-TEN) with a novel Encoding Layer integrated on top of convolutional layers, which ports the entire dictionary learning and encoding pipeline into a single model. Current methods build from distinct components, using standard encoders with separate off-the-shelf features such as SIFT descriptors or pre-trained CNN features for material recognition. Our new approach provides an end-to-end learning framework, where the inherent visual vocabularies are learned directly from the loss function. The features, dictionaries and the encoding representation for the classifier are all learned simultaneously. The representation is orderless and therefore is particularly useful for material and texture recognition. The Encoding Layer generalizes robust residual encoders such as VLAD and Fisher Vectors, and has the property of discarding domain specific information which makes the learned convolutional features easier to transfer. Additionally, joint training using multiple datasets of varied sizes and class labels is supported resulting in increased recognition performance. The experimental results show superior performance as compared to state-of-the-art methods using gold-standard databases such as MINC-2500, Flickr Material Database, KTH-TIPS-2b, and two recent databases 4D-Light-Field-Material and GTOS. The source code for the complete system are publicly available.",http://arxiv.org/pdf/1612.02844v1
,,,,,,365,Order-Preserving Wasserstein Distance for Sequence Matching,"Bing Su, Gang Hua",,,
,,,,,,700,Attend in Groups: A Weakly-Supervised Deep Learning Framework for Learning From Web Data,"Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, Ian Reid",Attend in groups: a weakly-supervised deep learning framework for learning from web data,"Large-scale datasets have driven the rapid development of deep neural networks for visual recognition. However, annotating a massive dataset is expensive and time-consuming. Web images and their labels are, in comparison, much easier to obtain, but direct training on such automatically harvested images can lead to unsatisfactory performance, because the noisy labels of Web images adversely affect the learned recognition models. To address this drawback we propose an end-to-end weakly-supervised deep learning framework which is robust to the label noise in Web images. The proposed framework relies on two unified strategies -- random grouping and attention -- to effectively reduce the negative impact of noisy web image annotations. Specifically, random grouping stacks multiple images into a single training instance and thus increases the labeling accuracy at the instance level. Attention, on the other hand, suppresses the noisy signals from both incorrectly labeled images and less discriminative image regions. By conducting intensive experiments on two challenging datasets, including a newly collected fine-grained dataset with Web images of different car models, the superior performance of the proposed methods over competitive baselines is clearly demonstrated.",http://arxiv.org/pdf/1611.09960v1
,,,,,,1128,Hierarchical Multimodal Metric Learning for Multimodal Classification,"Heng Zhang, Vishal M. Patel, Rama Chellappa",,,
,,,,,,1227,Efficient Linear Programming for Dense CRFs,"Thalaiyasingam Ajanthan, Alban Desmaison, Rudy Bunel, Mathieu Salzmann, Philip H. S. Torr, M. Pawan Kumar",Efficient Linear Programming for Dense CRFs,"The fully connected conditional random field (CRF) with Gaussian pairwise potentials has proven popular and effective for multi-class semantic segmentation. While the energy of a dense CRF can be minimized accurately using a linear programming (LP) relaxation, the state-of-the-art algorithm is too slow to be useful in practice. To alleviate this deficiency, we introduce an efficient LP minimization algorithm for dense CRFs. To this end, we develop a proximal minimization framework, where the dual of each proximal problem is optimized via block coordinate descent. We show that each block of variables can be efficiently optimized. Specifically, for one block, the problem decomposes into significantly smaller subproblems, each of which is defined over a single pixel. For the other block, the problem is optimized via conditional gradient descent. This has two advantages: 1) the conditional gradient can be computed in a time linear in the number of pixels and labels; and 2) the optimal step size can be computed analytically. Our experiments on standard datasets provide compelling evidence that our approach outperforms all existing baselines including the previous LP based approach for dense CRFs.",http://arxiv.org/pdf/1611.09718v2
,,,,,,1355,Variational Autoencoded Regression: High Dimensional Regression of Visual Data on Complex Manifold,"YoungJoon Yoo, Sangdoo Yun, Hyung Jin Chang, Yiannis Demiris, Jin Young Choi",,,
,,,,,,3380,Learning Random-Walk Label Propagation for Weakly-Supervised Semantic Segmentation,"Paul Vernaza, Manmohan Chandraker",,,
,,,,,,3384,Adversarial Discriminative Domain Adaptation,"Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell",Adversarial Discriminative Domain Adaptation,"Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.",http://arxiv.org/pdf/1702.05464v1
,,,,,,3854,Low-Rank-Sparse Subspace Representation for Robust Regression,"Yongqiang Zhang, Daming Shi, Junbin Gao, Dansong Cheng",Robust and Efficient Subspace Segmentation via Least Squares Regression,"This paper studies the subspace segmentation problem which aims to segment data drawn from a union of multiple linear subspaces. Recent works by using sparse representation, low rank representation and their extensions attract much attention. If the subspaces from which the data drawn are independent or orthogonal, they are able to obtain a block diagonal affinity matrix, which usually leads to a correct segmentation. The main differences among them are their objective functions. We theoretically show that if the objective function satisfies some conditions, and the data are sufficiently drawn from independent subspaces, the obtained affinity matrix is always block diagonal. Furthermore, the data sampling can be insufficient if the subspaces are orthogonal. Some existing methods are all special cases. Then we present the Least Squares Regression (LSR) method for subspace segmentation. It takes advantage of data correlation, which is common in real data. LSR encourages a grouping effect which tends to group highly correlated data together. Experimental results on the Hopkins 155 database and Extended Yale Database B show that our method significantly outperforms state-of-the-art methods. Beyond segmentation accuracy, all experiments demonstrate that LSR is much more efficient.",http://arxiv.org/pdf/1404.6736v1
,,,,,Object Recognition & Scene Understanding,353,Generating the Future With Adversarial Transformers,"Carl Vondrick, Antonio Torralba",Unsupervised Learning of Visual Structure using Predictive Generative Networks,"The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.",http://arxiv.org/pdf/1511.06380v2
,,,,,,547,Semantic Amodal Segmentation,"Yan Zhu, Yuandong Tian, Dimitris Metaxas, Piotr DollÃ¡r",Semantic Amodal Segmentation,"Common visual recognition tasks such as classification, object detection, and semantic segmentation are rapidly reaching maturity, and given the recent rate of progress, it is not unreasonable to conjecture that techniques for many of these problems will approach human levels of performance in the next few years. In this paper we look to the future: what is the next frontier in visual recognition?   We offer one possible answer to this question. We propose a detailed image annotation that captures information beyond the visible pixels and requires complex reasoning about full scene structure. Specifically, we create an amodal segmentation of each image: the full extent of each region is marked, not just the visible pixels. Annotators outline and name all salient regions in the image and specify a partial depth order. The result is a rich scene structure, including visible and occluded portions of each region, figure-ground edge information, semantic labels, and object overlap.   We create two datasets for semantic amodal segmentation. First, we label 500 images in the BSDS dataset with multiple annotators per image, allowing us to study the statistics of human annotations. We show that the proposed full scene annotation is surprisingly consistent between annotators, including for regions and edges. Second, we annotate 5000 images from COCO. This larger dataset allows us to explore a number of algorithmic ideas for amodal segmentation and depth ordering. We introduce novel metrics for these tasks, and along with our strong baselines, define concrete new challenges for the community.",http://arxiv.org/pdf/1509.01329v2
,,,,,,741,Learning a Deep Embedding Model for Zero-Shot Learning,"Li Zhang, Tao Xiang, Shaogang Gong",Learning a Deep Embedding Model for Zero-Shot Learning,"Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models.",http://arxiv.org/pdf/1611.05088v3
,,,,,,757,BIND: Binary Integrated Net Descriptors for Texture-Less Object Recognition,"Jacob Chan, Jimmy Addison Lee, Qian Kemao",,,
,,,,,,902,Growing a Brain: Fine-Tuning by Increasing Model Capacity,"Yu-Xiong Wang, Deva Ramanan, Martial Hebert",,,
,,,,,,946,A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection,"Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta",A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection,"How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy -- collect large-scale datasets which have object instances under different conditions. The hope is that the final classifier can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they hardly happen; yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difficult for the object detector to classify. In our framework both the original detector and adversary are learned in a joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline. We also release the code for this paper.",http://arxiv.org/pdf/1704.03414v1
,,,,,,1036,Multiple Instance Detection Network With Online Instance Classifier Refinement,"Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu",Multiple Instance Detection Network with Online Instance Classifier Refinement,"Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.",http://arxiv.org/pdf/1704.00138v1
,,,,,,1064,Kernel Pooling for Convolutional Neural Networks,"Yin Cui, Feng Zhou, Jiang Wang, Xiao Liu, Yuanqing Lin, Serge Belongie",A guide to convolution arithmetic for deep learning,"We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.",http://arxiv.org/pdf/1603.07285v1
,,,,,,1110,Learning Cross-Modal Embeddings for Cooking Recipes and Food Images,"Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, Antonio Torralba",,,
,,,,,,1896,"Zero-Shot Learning - the Good, the Bad and the Ugly","Yongqin Xian, Bernt Schiele, Zeynep Akata","Zero-Shot Learning - The Good, the Bad and the Ugly","Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.",http://arxiv.org/pdf/1703.04394v1
,,,,,,2150,DeepNav: Learning to Navigate Large Cities,"Samarth Brahmbhatt, James Hays",DeepNav: Learning to Navigate Large Cities,"We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for navigating large cities using locally visible street-view images. The DeepNav agent learns to reach its destination quickly by making the correct navigation decisions at intersections. We collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads. This dataset contains 10 city graphs and more than 1 million street-view images. We propose 3 supervised learning approaches for the navigation task and show how A* search in the city graph can be used to generate supervision for the learning. Our annotation process is fully automated using publicly available mapping services and requires no human input. We evaluate the proposed DeepNav models on 4 held-out cities for navigating to 5 different types of destinations. Our algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR)[19].",http://arxiv.org/pdf/1701.09135v2
,,,,,,2289,Scene Graph Generation by Iterative Message Passing,"Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei",Scene Graph Generation by Iterative Message Passing,"Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods for generating scene graphs using Visual Genome dataset and inferring support relations with NYU Depth v2 dataset.",http://arxiv.org/pdf/1701.02426v2
,,,,,,2334,Visual Translation Embedding Network for Visual Relation Detection,"Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua",Visual Translation Embedding Network for Visual Relation Detection,"Visual relations, such as ""person ride bike"" and ""bike next to car"", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate $\approx$ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors.",http://arxiv.org/pdf/1702.08319v1
,,,,,,2780,Unsupervised Part Learning for Visual Recognition,"Ronan Sicre, Yannis Avrithis, Ewa Kijak, FrÃ©dÃ©ric Jurie",Unsupervised Learning of Visual Representations using Videos,"Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.",http://arxiv.org/pdf/1505.00687v2
,,,,,,3329,Comprehension-Guided Referring Expressions,"Ruotian Luo, Gregory Shakhnarovich",Comprehension-guided referring expressions,"We consider generation and comprehension of natural language referring expression for objects in an image. Unlike generic ""image captioning"" which lacks natural standard evaluation criteria, quality of a referring expression may be measured by the receiver's ability to correctly infer which object is being described. Following this intuition, we propose two approaches to utilize models trained for comprehension task to generate better expressions. First, we use a comprehension module trained on human-generated expressions, as a ""critic"" of referring expression generator. The comprehension module serves as a differentiable proxy of human evaluation, providing training signal to the generation module. Second, we use the comprehension module in a generate-and-rerank pipeline, which chooses from candidate expressions generated by a model according to their performance on the comprehension task. We show that both approaches lead to improved referring expression generation on multiple benchmark datasets.",http://arxiv.org/pdf/1701.03439v1
,,,,,,3426,Top-Down Visual Saliency Guided by Captions,"Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko",Top-down Visual Saliency Guided by Captions,"Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at visionlearninggroup.github.io/caption-guided-saliency/.",http://arxiv.org/pdf/1612.07360v2
,,,,,Theory,2208,Grassmannian Manifold Optimization Assisted Sparse Spectral Clustering,"Qiong Wang, Junbin Gao, Hong Li",,,
,,,,,Video Analytics,146,Video Propagation Networks,"Varun Jampani, Raghudeep Gadde, Peter V. Gehler",Video Propagation Networks,"We propose a technique that propagates information forward through video data. The method is conceptually simple and can be applied to tasks that require the propagation of structured information, such as semantic labels, based on video content. We propose a 'Video Propagation Network' that processes video frames in an adaptive manner. The model is applied online: it propagates information forward without the need to access future frames. In particular we combine two components, a temporal bilateral network for dense and video adaptive filtering, followed by a spatial network to refine features and increased flexibility. We present experiments on video object segmentation and semantic video segmentation and show increased performance comparing to the best previous task-specific methods, while having favorable runtime. Additionally we demonstrate our approach on an example regression task of color propagation in a grayscale video.",http://arxiv.org/pdf/1612.05478v3
,,,,,,327,ActionVLAD: Learning Spatio-Temporal Aggregation for Action Classification,"Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, Bryan Russell",ActionVLAD: Learning spatio-temporal aggregation for action classification,"In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal feature aggregation. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13% relative) as well as out-performs other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.",http://arxiv.org/pdf/1704.02895v1
,,,,,,543,SCC: Semantic Context Cascade for Efficient Action Detection,"Fabian Caba Heilbron, Wayner Barrios, Victor Escorcia, Bernard Ghanem",,,
,,,,,,601,Hierarchical Boundary-Aware Neural Encoder for Video Captioning,"Lorenzo Baraldi, Costantino Grana, Rita Cucchiara",Hierarchical Boundary-Aware Neural Encoder for Video Captioning,"The use of Recurrent Neural Networks for video captioning has recently gained a lot of attention, since they can be used both to encode the input video and to generate the corresponding description. In this paper, we present a recurrent video encoding scheme which can discover and leverage the hierarchical structure of the video. Unlike the classical encoder-decoder approach, in which a video is encoded continuously by a recurrent layer, we propose a novel LSTM cell, which can identify discontinuity points between frames or segments and modify the temporal connections of the encoding layer accordingly. We evaluate our approach on three large-scale datasets: the Montreal Video Annotation dataset, the MPII Movie Description dataset and the Microsoft Video Description Corpus. Experiments show that our approach can discover appropriate hierarchical representations of input videos and improve the state of the art results on movie description datasets.",http://arxiv.org/pdf/1611.09312v3
,,,,,,887,HOPE: Hierarchical Object Prototype Encoding for Efficient Object Instance Search in Videos,"Tan Yu, Yuwei Wu, Junsong Yuan",,,
,,,,,,1146,Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos,"Ionut Cosmin Duta, Bogdan Ionescu, Kiyoharu Aizawa, Nicu Sebe",,,
,,,,,,1357,Temporal Action Localization by Structured Maximal Sums,"Zehuan Yuan, Jonathan C. Stroud, Tong Lu, Jia Deng",Temporal Action Localization by Structured Maximal Sums,"We address the problem of temporal action localization in videos. We pose action localization as a structured prediction over arbitrary-length temporal windows, where each window is scored as the sum of frame-wise classification scores. Additionally, our model classifies the start, middle, and end of each action as separate components, allowing our system to explicitly model each action's temporal evolution and take advantage of informative temporal dependencies present in this structure. In this framework, we localize actions by searching for the structured maximal sum, a problem for which we develop a novel, provably-efficient algorithmic solution. The frame-wise classification scores are computed using features from a deep Convolutional Neural Network (CNN), which are trained end-to-end to directly optimize for a novel structured objective. We evaluate our system on the THUMOS 14 action detection benchmark and achieve competitive performance.",http://arxiv.org/pdf/1704.04671v1
,,,,,,1805,Predicting Salient Face in Multiple-Face Videos,"Yufan Liu, Songyang Zhang, Mai Xu, Xuming He",,,
"Sunday, July 23, 2017",1300â€“1430,Kamehameha III,13,Spotlight 2-2A,Object Recognition & Scene Understanding 1,6,Graph-Structured Representations for Visual Question Answering,"Damien Teney, Lingqiao Liu, Anton van den Hengel",Graph-Structured Representations for Visual Question Answering,"This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the ""abstract scenes"" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of ""balanced"" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.",http://arxiv.org/pdf/1609.05600v2
,,,,,,133,Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning,"Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher",Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning,"Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as ""the"" and ""of"". Other words that may seem visual can often be predicted reliably just from the language model e.g., ""sign"" after ""behind a red stop"" or ""phone"" following ""talking on a cell"". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",http://arxiv.org/pdf/1612.01887v2
,,,,,,780,Learned Contextual Feature Reweighting for Image Geo-Localization,"Hyo Jin Kim, Enrique Dunn, Jan-Michael Frahm",,,
,,,,,,1165,"End-To-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering","Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim","End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering","We propose a high-level concept word detector that can be integrated with any video-to-language models. It takes a video as input and generates a list of concept words as useful semantic priors for language generation models. The proposed word detector has two important properties. First, it does not require any external knowledge sources for training. Second, the proposed word detector is trainable in an end-to-end manner jointly with any video-to-language models. To maximize the values of detected words, we also develop a semantic attention mechanism that selectively focuses on the detected concept words and fuse them with the word encoding and decoding in the language model. In order to demonstrate that the proposed approach indeed improves the performance of multiple video-to-language tasks, we participate in four tasks of LSMDC 2016. Our approach achieves the best accuracies in three of them, including fill-in-the-blank, multiple-choice test, and movie retrieval. We also attain comparable performance for the other task, movie description.",http://arxiv.org/pdf/1610.02947v2
,,,,,,1194,Deep Cross-Modal Hashing,"Qing-Yuan Jiang, Wu-Jun Li",Correlation Hashing Network for Efficient Cross-Modal Retrieval,"Hashing is widely applied to approximate nearest neighbor search for large-scale multimodal retrieval with storage and computation efficiency. Cross-modal hashing improves the quality of hash coding by exploiting semantic correlations across different modalities. Existing cross-modal hashing methods first transform data into low-dimensional feature vectors, and then generate binary codes by another separate quantization step. However, suboptimal hash codes may be generated since the quantization error is not explicitly minimized and the feature representation is not jointly optimized with the binary codes. This paper presents a Correlation Hashing Network (CHN) approach to cross-modal hashing, which jointly learns good data representation tailored to hash coding and formally controls the quantization error. The proposed CHN is a hybrid deep architecture that constitutes a convolutional neural network for learning good image representations, a multilayer perception for learning good text representations, two hashing layers for generating compact binary codes, and a structured max-margin loss that integrates all things together to enable learning similarity-preserving and high-quality hash codes. Extensive empirical study shows that CHN yields state of the art cross-modal retrieval performance on standard benchmarks.",http://arxiv.org/pdf/1602.06697v2
,,,,,,2326,Unambiguous Text Localization and Retrieval for Cluttered Scenes,"Xuejian Rong, Chucai Yi, Yingli Tian",,,
,,,,,,2812,Bayesian Supervised Hashing,"Zihao Hu, Junxuan Chen, Hongtao Lu, Tongzhen Zhang",,,
,,,,,,3562,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,"Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy",Speed/accuracy trade-offs for modern convolutional object detectors,"The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as ""meta-architectures"" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.",http://arxiv.org/pdf/1611.10012v3
,,,,Oral 2-2A,,1142,Detecting Visual Relationships With Deep Relational Networks,"Bo Dai, Yuqi Zhang, Dahua Lin",Detecting Visual Relationships with Deep Relational Networks,"Relationships among objects play a crucial role in image understanding. Despite the great success of deep learning techniques in recognizing individual objects, reasoning about the relationships among objects remains a challenging task. Previous methods often treat this as a classification problem, considering each type of relationship (e.g. ""ride"") or each distinct visual phrase (e.g. ""person-ride-horse"") as a category. Such approaches are faced with significant difficulties caused by the high diversity of visual appearance for each kind of relationships or the large number of distinct visual phrases. We propose an integrated framework to tackle this problem. At the heart of this framework is the Deep Relational Network, a novel formulation designed specifically for exploiting the statistical dependencies between objects and their relationships. On two large datasets, the proposed method achieves substantial improvement over state-of-the-art.",http://arxiv.org/pdf/1704.03114v2
,,,,,,1691,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,"Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe",Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,"Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.",http://arxiv.org/pdf/1611.08323v2
,,,,,,2955,Network Dissection: Quantifying Interpretability of Deep Visual Representations,"David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba",Network Dissection: Quantifying Interpretability of Deep Visual Representations,"We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.",http://arxiv.org/pdf/1704.05796v1
,,,,,,3856,AGA: Attribute-Guided Augmentation,"Mandar Dixit, Roland Kwitt, Marc Niethammer, Nuno Vasconcelos",AGA: Attribute Guided Augmentation,"We consider the problem of data augmentation, i.e., generating artificial samples to extend a given corpus of training data. Specifically, we propose attributed-guided augmentation (AGA) which learns a mapping that allows to synthesize data such that an attribute of a synthesized sample is at a desired value or strength. This is particularly interesting in situations where little data with no attribute annotation is available for learning, but we have access to a large external corpus of heavily annotated samples. While prior works primarily augment in the space of images, we propose to perform augmentation in feature space instead. We implement our approach as a deep encoder-decoder architecture that learns the synthesis function in an end-to-end manner. We demonstrate the utility of our approach on the problems of (1) one-shot object recognition in a transfer-learning setting where we have no prior knowledge of the new classes, as well as (2) object-based one-shot scene recognition. As external data, we leverage 3D depth and pose information from the SUN RGB-D dataset. Our experiments show that attribute-guided augmentation of high-level CNN features considerably improves one-shot recognition performance on both problems.",http://arxiv.org/pdf/1612.02559v1
"Sunday, July 23, 2017",1300â€“1430,KalÄÅkaua Ballroom A-B,14,Spotlight 2-2B,Analyzing Humans 2,120,A Hierarchical Approach for Generating Descriptive Image Paragraphs,"Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei",A Hierarchical Approach for Generating Descriptive Image Paragraphs,"Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.",http://arxiv.org/pdf/1611.06607v2
,,,,,,505,Person Re-Identification in the Wild,"Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, Qi Tian",Person Re-identification in the Wild,"We present a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification accuracy and assessing the effectiveness of different detectors for re-identification. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-identification through two simple yet effective improvements: a discriminatively trained ID-discriminative Embedding (IDE) in the person subspace using convolutional neural network (CNN) features and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-identification.",http://arxiv.org/pdf/1604.02531v2
,,,,,,915,Scalable Person Re-Identification on Supervised Smoothed Manifold,"Song Bai, Xiang Bai, Qi Tian",Scalable Person Re-identification on Supervised Smoothed Manifold,"Most existing person re-identification algorithms either extract robust visual features or learn discriminative metrics for person images. However, the underlying manifold which those images reside on is rarely investigated. That raises a problem that the learned metric is not smooth with respect to the local geometry structure of the data manifold.   In this paper, we study person re-identification with manifold-based affinity learning, which did not receive enough attention from this area. An unconventional manifold-preserving algorithm is proposed, which can 1) make the best use of supervision from training data, whose label information is given as pairwise constraints; 2) scale up to large repositories with low on-line time complexity; and 3) be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost the identification accuracies. Extensive experimental results on five popular person re-identification benchmarks consistently demonstrate the effectiveness of our method. Especially, on the largest CUHK03 and Market-1501, our method outperforms the state-of-the-art alternatives by a large margin with high efficiency, which is more appropriate for practical applications.",http://arxiv.org/pdf/1703.08359v1
,,,,,,945,Binge Watching: Scaling Affordance Learning From Sitcoms,"Xiaolong Wang, Rohit Girdhar, Abhinav Gupta",,,
,,,,,,1262,Joint Detection and Identification Feature Learning for Person Search,"Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, Xiaogang Wang",Joint Detection and Identification Feature Learning for Person Search,"Existing person re-identification benchmarks and methods mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be searched from a gallery of whole scene images. To close the gap, we propose a new deep learning framework for person search. Instead of breaking it down into two separate tasks---pedestrian detection and person re-identification, we jointly handle both aspects in a single convolutional neural network. An Online Instance Matching (OIM) loss function is proposed to train the network effectively, which is scalable to datasets with numerous identities. To validate our approach, we collect and annotate a large-scale benchmark dataset for person search. It contains 18,184 images, 8,432 identities, and 96,143 pedestrian bounding boxes. Experiments show that our framework outperforms other separate approaches, and the proposed OIM loss function converges much faster and better than the conventional Softmax loss.",http://arxiv.org/pdf/1604.01850v3
,,,,,,1362,Synthesizing Normalized Faces From Facial Identity Features,"Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, William T. Freeman",Synthesizing Normalized Faces from Facial Identity Features,"We present a method for synthesizing a frontal, neutral-expression image of a person's face given an input face photograph. This is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network. Unlike previous approaches, our encoding feature vector is largely invariant to lighting, pose, and facial expression. Exploiting this invariance, we train our decoder network using only frontal, neutral-expression photographs. Since these photographs are well aligned, we can decompose them into a sparse set of landmark points and aligned texture maps. The decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation. The resulting images can be used for a number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating a 3-D avatar.",http://arxiv.org/pdf/1701.04851v3
,,,,,,2458,Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network,"Ji Lin, Liangliang Ren, Jiwen Lu, Jianjiang Feng, Jie Zhou",,,
,,,,,,3276,Level Playing Field for Million Scale Face Recognition,"Aaron Nech, Ira Kemelmacher-Shlizerman",Level Playing Field for Million Scale Face Recognition,"Face recognition has the perception of a solved problem, however when tested at the million-scale exhibits dramatic variation in accuracies across the different algorithms. Are the algorithms very different? Is access to good/big training data their secret weapon? Where should face recognition improve? To address those questions, we created a benchmark, MF2, that requires all algorithms to be trained on same data, and tested at the million scale. MF2 is a public large-scale set with 672K identities and 4.7M photos created with the goal to level playing field for large scale face recognition. We contrast our results with findings from the other two large-scale benchmarks MegaFace Challenge and MS-Celebs-1M where groups were allowed to train on any private/public/big/small set. Some key discoveries: 1) algorithms, trained on MF2, were able to achieve state of the art and comparable results to algorithms trained on massive private sets, 2) some outperformed themselves once trained on MF2, 3) invariance to aging suffers from low accuracies as in MegaFace, identifying the need for larger age variations possibly within identities or adjustment of algorithms in future testings.",http://arxiv.org/pdf/1705.00393v1
,,,,Oral 2-2B,,1734,Re-Sign: Re-Aligned End-To-End Sequence Modelling With Deep Recurrent CNN-HMMs,"Oscar Koller, Sepehr Zargaran, Hermann Ney",,,
,,,,,,1744,Social Scene Understanding: End-To-End Multi-Person Action Localization and Collective Activity Recognition,"Timur Bagautdinov, Alexandre Alahi, FranÃ§ois Fleuret, Pascal Fua, Silvio Savarese",Social Scene Understanding: End-to-End Multi-Person Action Localization and Collective Activity Recognition,"We present a unified framework for understanding human social behaviors in raw image sequences. Our model jointly detects multiple individuals, infers their social actions, and estimates the collective actions with a single feed-forward pass through a neural network. We propose a single architecture that does not rely on external detection algorithms but rather is trained end-to-end to generate dense proposal maps that are refined via a novel inference scheme. The temporal consistency is handled via a person-level matching Recurrent Neural Network. The complete model takes as input a sequence of frames and outputs detections along with the estimates of individual actions and collective activities. We demonstrate state-of-the-art performance of our algorithm on multiple publicly available benchmarks.",http://arxiv.org/pdf/1611.09078v1
,,,,,,2626,Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly,"Hao Jiang, Kristen Grauman",Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly,"Today's person detection methods work best when people are in common upright poses and appear reasonably well spaced out in the image. However, in many real images, that's not what people do. People often appear quite close to each other, e.g., with limbs linked or heads touching, and their poses are often not pedestrian-like. We propose an approach to detangle people in multi-person images. We formulate the task as a region assembly problem. Starting from a large set of overlapping regions from body part semantic segmentation and generic object proposals, our optimization approach reassembles those pieces together into multiple person instances. It enforces that the composed body part regions of each person instance obey constraints on relative sizes, mutual spatial relationships, foreground coverage, and exclusive label assignments when overlapping. Since optimal region assembly is a challenging combinatorial problem, we present a Lagrangian relaxation method to accelerate the lower bound estimation, thereby enabling a fast branch and bound solution for the global optimum. As output, our method produces a pixel-level map indicating both 1) the body part labels (arm, leg, torso, and head), and 2) which parts belong to which individual person. Our results on three challenging datasets show our method is robust to clutter, occlusion, and complex poses. It outperforms a variety of competing methods, including existing detector CRF methods and region CNN approaches. In addition, we demonstrate its impact on a proxemics recognition task, which demands a precise representation of ""whose body part is where"" in crowded images.",http://arxiv.org/pdf/1604.03880v1
,,,,,,2873,Lip Reading Sentences in the Wild,"Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman",Lip Reading Sentences in the Wild,"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos.   Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.   The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is available.",http://arxiv.org/pdf/1611.05358v2
"Sunday, July 23, 2017",1300â€“1430,KalÄÅkaua Ballroom C,15,Spotlight 2-2C,Applications,728,Deep Matching Prior Network: Toward Tighter Multi-Oriented Text Detection,"Yuliang Liu, Lianwen Jin",Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection,"Detecting incidental scene text is a challenging task because of multi-orientation, perspective distortion, and variation of text size, color and scale. Retrospective research has only focused on using rectangular bounding box or horizontal sliding window to localize text, which may result in redundant background noise, unnecessary overlap or even information loss. To address these issues, we propose a new Convolutional Neural Networks (CNNs) based method, named Deep Matching Prior Network (DMPNet), to detect text with tighter quadrangle. First, we use quadrilateral sliding windows in several specific intermediate convolutional layers to roughly recall the text with higher overlapping area and then a shared Monte-Carlo method is proposed for fast and accurate computing of the polygonal areas. After that, we designed a sequential protocol for relative regression which can exactly predict text with compact quadrangle. Moreover, a auxiliary smooth Ln loss is also proposed for further regressing the position of text, which has better overall performance than L2 loss and smooth L1 loss in terms of robustness and stability. The effectiveness of our approach is evaluated on a public word-level, multi-oriented scene text database, ICDAR 2015 Robust Reading Competition Challenge 4 ""Incidental scene text localization"". The performance of our method is evaluated by using F-measure and found to be 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%.",http://arxiv.org/pdf/1703.01425v1
,,,,,,766,ChestX-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases,"Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M. Summers",,,
,,,,,,911,Attentional Push: A Deep Convolutional Network for Augmenting Image Salience With Shared Attention Modeling in Social Scenes,"Siavash Gorji, James J. Clark",,,
,,,,,,919,Detecting Oriented Text in Natural Images by Linking Segments,"Baoguang Shi, Xiang Bai, Serge Belongie",Detecting Oriented Text in Natural Images by Linking Segments,"Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.",http://arxiv.org/pdf/1703.06520v3
,,,,,,967,Learning Video Object Segmentation From Static Images,"Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, Alexander Sorkine-Hornung",Learning Video Object Segmentation from Static Images,"Inspired by recent advances of deep learning in instance segmentation and object tracking, we introduce video object segmentation problem as a concept of guided instance segmentation. Our model proceeds on a per-frame basis, guided by the output of the previous frame towards the object of interest in the next frame. We demonstrate that highly accurate object segmentation in videos can be enabled by using a convnet trained with static images only. The key ingredient of our approach is a combination of offline and online learning strategies, where the former serves to produce a refined mask from the previous frame estimate and the latter allows to capture the appearance of the specific object instance. Our method can handle different types of input annotations: bounding boxes and segments, as well as incorporate multiple annotated frames, making the system suitable for diverse applications. We obtain competitive results on three different datasets, independently from the type of input annotation.",http://arxiv.org/pdf/1612.02646v1
,,,,,,1579,Seeing Invisible Poses: Estimating 3D Body Pose From Egocentric Video,"Hao Jiang, Kristen Grauman",Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video,"Understanding the camera wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the camera--the wearer's body pose. Prior work focuses on estimating the pose of hands and arms when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of daily life activities. We propose to infer the ""invisible pose"" of a person behind the egocentric camera. Given a single video, our efficient learning-based approach returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scene--which changes predictably as a function of body pose--as well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel energy minimization scheme to infer the pose sequence. It uses soft predictions of the poses per time instant together with a non-parametric model of human pose dynamics over longer windows. Our method outperforms an array of possible alternatives, including deep learning approaches for direct pose regression from images.",http://arxiv.org/pdf/1603.07763v1
,,,,,,1822,Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space,"Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski",,,
,,,,,,3493,A Joint Speaker-Listener-Reinforcer Model for Referring Expressions,"Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg",A Joint Speaker-Listener-Reinforcer Model for Referring Expressions,"Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer",http://arxiv.org/pdf/1612.09542v2
,,,,Oral 2-2C,,788,End-To-End Learning of Driving Models From Large-Scale Video Datasets,"Huazhe Xu, Yang Gao, Fisher Yu, Trevor Darrell",End-to-end Learning of Driving Models from Large-scale Video Datasets,"Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or a simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an end-to-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm.",http://arxiv.org/pdf/1612.01079v1
,,,,,,1775,Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks,"Mengmi Zhang, Keng Teck Ma, Joo Hwee Lim, Qi Zhao, Jiashi Feng",,,
,,,,,,2871,MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network,"Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang",,,
"Sunday, July 23, 2017",1430â€“1630,Kamehameha I,16,Poster 2-2,3D Computer Vision,68,Surface Motion Capture Transfer With Gaussian Process Regression,"Adnane Boukhayma, Jean-SÃ©bastien Franco, Edmond Boyer",,,
,,,,,,326,Visual-Inertial-Semantic Scene Representation for 3D Object Detection,"Jingming Dong, Xiaohan Fei, Stefano Soatto",,,
,,,,,,1646,Template-Based Monocular 3D Recovery of Elastic Shapes Using Lagrangian Multipliers,"Nazim Haouchine, Stephane Cotin",,,
,,,,,,1892,Learning Category-Specific 3D Shape Models From Weakly Labeled 2D Images,"Dingwen Zhang, Junwei Han, Yang Yang, Dong Huang",,,
,,,,,,2017,Simultaneous Geometric and Radiometric Calibration of a Projector-Camera Pair,"Marjan Shahpaski, Luis Ricardo Sapaico, Gaspard Chevassus, Sabine SÃ_sstrunk",,,
,,,,,,2027,A Clever Elimination Strategy for Efficient Minimal Solvers,"Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, Tomas Pajdla",A clever elimination strategy for efficient minimal solvers,"We present a new insight into the systematic generation of minimal solvers in computer vision, which leads to smaller and faster solvers. Many minimal problem formulations are coupled sets of linear and polynomial equations where image measurements enter the linear equations only. We show that it is useful to solve such systems by first eliminating all the unknowns that do not appear in the linear equations and then extending solutions to the rest of unknowns. This can be generalized to fully non-linear systems by linearization via lifting. We demonstrate that this approach leads to more efficient solvers in three problems of partially calibrated relative camera pose computation with unknown focal length and/or radial distortion. Our approach also generates new interesting constraints on the fundamental matrices of partially calibrated cameras, which were not known before.",http://arxiv.org/pdf/1703.05289v1
,,,,,,2084,Learning Barycentric Representations of 3D Shapes for Sketch-Based 3D Shape Retrieval,"Jin Xie, Guoxian Dai, Fan Zhu, Yi Fang",,,
,,,,,,2860,Geodesic Distance Descriptors,"Gil Shamai, Ron Kimmel",Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in the Homography Space,"With the aim to improve the performance of feature matching, we present an unsupervised approach to fuse various local descriptors in the space of homographies. Inspired by the observation that the homographies of correct feature correspondences vary smoothly along the spatial domain, our approach stands on the unsupervised nature of feature matching, and can select a good descriptor for matching each feature point. Specifically, the homography space serves as the common domain, in which a correspondence obtained by any descriptor is considered as a point, for integrating various heterogeneous descriptors. Both geometric coherence and spatial continuity among correspondences are considered via computing their geodesic distances in the space. In this way, mutual verification across different descriptors is allowed, and correct correspondences will be highlighted with a high degree of consistency (i.e., short geodesic distances here). It follows that one-class SVM can be applied to identifying these correct correspondences, and boosts the performance of feature matching. The proposed approach is comprehensively compared with the state-of-the-art approaches, and evaluated on four benchmarks of image matching. The promising results manifest its effectiveness.",http://arxiv.org/pdf/1412.4196v1
,,,,,Analyzing Humans in Images,158,Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks,"Hongsong Wang, Liang Wang",Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks,"Recently, skeleton based action recognition gains more popularity due to cost-effective depth sensors coupled with real-time skeleton estimation algorithms. Traditional approaches based on handcrafted features are limited to represent the complexity of motion patterns. Recent methods that use Recurrent Neural Networks (RNN) to handle raw skeletons only focus on the contextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons. In this paper, we propose a novel two-stream RNN architecture to model both temporal dynamics and spatial configurations for skeleton based action recognition. We explore two different structures for the temporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed according to human body kinematics. We also propose two effective methods to model the spatial structure by converting the spatial graph into a sequence of joints. To improve generalization of our model, we further exploit 3D transformation based data augmentation techniques including rotation and scaling transformation to transform the 3D coordinates of skeletons during training. Experiments on 3D action recognition benchmark datasets show that our method brings a considerable improvement for a variety of actions, i.e., generic actions, interaction activities and gestures.",http://arxiv.org/pdf/1704.02581v2
,,,,,,174,Forecasting Human Dynamics From Static Images,"Yu-Wei Chao, Jimei Yang, Brian Price, Scott Cohen, Jia Deng",Forecasting Human Dynamics from Static Images,"This paper presents the first study on forecasting human dynamics from static images. The problem is to input a single RGB image and generate a sequence of upcoming human body poses in 3D. To address the problem, we propose the 3D Pose Forecasting Network (3D-PFNet). Our 3D-PFNet integrates recent advances on single-image human pose estimation and sequence prediction, and converts the 2D predictions into 3D space. We train our 3D-PFNet using a three-step training strategy to leverage a diverse source of training data, including image and video based human pose datasets and 3D motion capture (MoCap) data. We demonstrate competitive performance of our 3D-PFNet on 2D pose forecasting and 3D pose recovery through quantitative and qualitative results.",http://arxiv.org/pdf/1704.03432v1
,,,,,,477,Re-Ranking Person Re-Identification With k-Reciprocal Encoding,"Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li",,,
,,,,,,550,Deep Sequential Context Networks for Action Prediction,"Yu Kong, Zhiqiang Tao, Yun Fu",,,
,,,,,,600,Global Context-Aware Attention LSTM Networks for 3D Action Recognition,"Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, Alex C. Kot",,,
,,,,,,903,Dynamic Attention-Controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-Set Sample Weighting,"Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber, Xiao-Jun Wu",Dynamic Attention-controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-set Sample Weighting,"We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting for attention-controlled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art.",http://arxiv.org/pdf/1611.05396v2
,,,,,,1236,A Deep Regression Architecture With Two-Stage Re-Initialization for High Performance Facial Landmark Detection,"Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, Xi Zhou",,,
,,,,,,1309,Multiple People Tracking by Lifted Multicut and Person Re-Identification,"Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, Bernt Schiele",,,
,,,,,,2022,Towards Accurate Multi-Person Pose Estimation in the Wild,"George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Kevin Murphy",Towards Accurate Multi-person Pose Estimation in the Wild,"We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages.   In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring.   Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5% absolute improvement compared to the previous best performing method on the same dataset.",http://arxiv.org/pdf/1701.01779v2
,,,,,Applications,28,Towards a Quality Metric for Dense Light Fields,"Vamsi Kiran Adhikarla, Marek Vinkler, Denis Sumin, RafaÅ‚ K. Mantiuk, Karol Myszkowski, Hans-Peter Seidel, Piotr Didyk",Towards a quality metric for dense light fields,"Light fields become a popular representation of three dimensional scenes, and there is interest in their processing, resampling, and compression. As those operations often result in loss of quality, there is a need to quantify it. In this work, we collect a new dataset of dense reference and distorted light fields as well as the corresponding quality scores which are scaled in perceptual units. The scores were acquired in a subjective experiment using an interactive light-field viewing setup. The dataset contains typical artifacts that occur in light-field processing chain due to light-field reconstruction, multi-view compression, and limitations of automultiscopic displays. We test a number of existing objective quality metrics to determine how well they can predict the quality of light fields. We find that the existing image quality metrics provide good measures of light-field quality, but require dense reference light- fields for optimal performance. For more complex tasks of comparing two distorted light fields, their performance drops significantly, which reveals the need for new, light-field-specific metrics.",http://arxiv.org/pdf/1704.07576v1
,,,,,,1592,Controlling Perceptual Factors in Neural Style Transfer,"Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann, Eli Shechtman",Controlling Perceptual Factors in Neural Style Transfer,"Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.",http://arxiv.org/pdf/1611.07865v2
,,,,,Biomedical Image/Video Analysis,2842,Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation,"Kuan-Lun Tseng, Yen-Liang Lin, Winston Hsu, Chung-Yang Huang",Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation,"Deep learning models such as convolutional neural net- work have been widely used in 3D biomedical segmentation and achieve state-of-the-art performance. However, most of them often adapt a single modality or stack multiple modalities as different input channels. To better leverage the multi- modalities, we propose a deep encoder-decoder structure with cross-modality convolution layers to incorporate different modalities of MRI data. In addition, we exploit convolutional LSTM to model a sequence of 2D slices, and jointly learn the multi-modalities and convolutional LSTM in an end-to-end manner. To avoid converging to the certain labels, we adopt a re-weighting scheme and two-phase training to handle the label imbalance. Experimental results on BRATS-2015 show that our method outperforms state-of-the-art biomedical segmentation approaches.",http://arxiv.org/pdf/1704.07754v1
,,,,,,2881,LSTM Self-Supervision for Detailed Behavior Analysis,"Biagio Brattoli, Uta BÃ_chler, Anna-Sophia Wahl, Martin E. Schwab, BjÃ¶rn Ommer",,,
,,,,,Computational Photography,2082,A Wide-Field-Of-View Monocentric Light Field Camera,"Donald G. Dansereau, Glenn Schuster, Joseph Ford, Gordon Wetzstein",,,
,,,,,Image Motion & Tracking,765,S2F: Slow-To-Fast Interpolator Flow,"Yanchao Yang, Stefano Soatto",,,
,,,,,,798,CLKN: Cascaded Lucas-Kanade Networks for Image Alignment,"Che-Han Chang, Chun-Nan Chou, Edward Y. Chang",,,
,,,,,,2386,Multi-Object Tracking With Quadruplet Convolutional Neural Networks,"Jeany Son, Mooyeol Baek, Minsu Cho, Bohyung Han",,,
,,,,,Low- & Mid-Level Vision,55,Learning to Detect Salient Objects With Image-Level Supervision,"Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, Xiang Ruan",Salient Object Detection: A Discriminative Regional Feature Integration Approach,"Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score. Saliency scores across multiple levels are finally fused to produce the saliency map. The contributions lie in two-fold. One is that we propose a discriminate regional feature integration approach for salient object detection. Compared with existing heuristic models, our proposed method is able to automatically integrate high-dimensional regional saliency features and choose discriminative ones. The other is that by investigating standard generic region properties as well as two widely studied concepts for salient object detection, i.e., regional contrast and backgroundness, our approach significantly outperforms state-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate that our method runs as fast as most existing algorithms.",http://arxiv.org/pdf/1410.5926v1
,,,,,,846,From Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur,"Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton van den Hengel, Qinfeng Shi",From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur,"Removing pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but the extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach is thus that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.",http://arxiv.org/pdf/1612.02583v1
,,,,,,1175,Co-Occurrence Filter,"Roy J. Jevnisek, Shai Avidan",Co-occurrence Filter,"Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on the Bilateral Filter (BF) but instead of using a Gaussian on the range values to preserve edges it relies on a co-occurrence matrix. Pixel values that co-occur frequently in the image (i.e., inside textured regions) will have a high weight in the co-occurrence matrix. This, in turn, means that such pixel pairs will be averaged and hence smoothed, regardless of their intensity differences. On the other hand, pixel values that rarely co-occur (i.e., across texture boundaries) will have a low weight in the co-occurrence matrix. As a result, they will not be averaged and the boundary between them will be preserved. The CoF therefore extends the BF to deal with boundaries, not just edges. It learns co-occurrences directly from the image. We can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image, or a different image. We give the definition of the filter, discuss how to use it with color images and show several use cases.",http://arxiv.org/pdf/1703.04111v1
,,,,,,1298,Fractal Dimension Invariant Filtering and Its CNN-Based Implementation,"Hongteng Xu, Junchi Yan, Nils Persson, Weiyao Lin, Hongyuan Zha",Fractal Dimension Invariant Filtering and Its CNN-based Implementation,"Fractal analysis has been widely used in computer vision, especially in texture image processing and texture analysis. The key concept of fractal-based image model is the fractal dimension, which is invariant to bi-Lipschitz transformation of image, and thus capable of representing intrinsic structural information of image robustly. However, the invariance of fractal dimension generally does not hold after filtering, which limits the application of fractal-based image model. In this paper, we propose a novel fractal dimension invariant filtering (FDIF) method, extending the invariance of fractal dimension to filtering operations. Utilizing the notion of local self-similarity, we first develop a local fractal model for images. By adding a nonlinear post-processing step behind anisotropic filter banks, we demonstrate that the proposed filtering method is capable of preserving the local invariance of the fractal dimension of image. Meanwhile, we show that the FDIF method can be re-instantiated approximately via a CNN-based architecture, where the convolution layer extracts anisotropic structure of image and the nonlinear layer enhances the structure via preserving local fractal dimension of image. The proposed filtering method provides us with a novel geometric interpretation of CNN-based image model. Focusing on a challenging image processing task --- detecting complicated curves from the texture-like images, the proposed method obtains superior results to the state-of-art approaches.",http://arxiv.org/pdf/1603.06036v3
,,,,,,1300,Noise-Blind Image Deblurring,"Meiguang Jin, Stefan Roth, Paolo Favaro",A Regularization Approach to Blind Deblurring and Denoising of QR Barcodes,"QR bar codes are prototypical images for which part of the image is a priori known (required patterns). Open source bar code readers, such as ZBar, are readily available. We exploit both these facts to provide and assess purely regularization-based methods for blind deblurring of QR bar codes in the presence of noise.",http://arxiv.org/pdf/1410.6333v3
,,,,,,1387,Simultaneous Visual Data Completion and Denoising Based on Tensor Rank and Total Variation Minimization and Its Primal-Dual Splitting Algorithm,"Tatsuya Yokota, Hidekata Hontani",,,
,,,,,,2146,HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors,"Vassileios Balntas, Karel Lenc, Andrea Vedaldi, Krystian Mikolajczyk",HPatches: A benchmark and evaluation of handcrafted and learned local descriptors,"In this paper, we propose a novel benchmark for evaluating local image descriptors. We demonstrate that the existing datasets and evaluation protocols do not specify unambiguously all aspects of evaluation, leading to ambiguities and inconsistencies in results reported in the literature. Furthermore, these datasets are nearly saturated due to the recent improvements in local descriptors obtained by learning them from large annotated datasets. Therefore, we introduce a new large dataset suitable for training and testing modern descriptors, together with strictly defined evaluation protocols in several tasks such as matching, retrieval and classification. This allows for more realistic, and thus more reliable comparisons in different application scenarios. We evaluate the performance of several state-of-the-art descriptors and analyse their properties. We show that a simple normalisation of traditional hand-crafted descriptors can boost their performance to the level of deep learning based descriptors within a realistic benchmarks evaluation.",http://arxiv.org/pdf/1704.05939v1
,,,,,,2245,Hyperspectral Image Super-Resolution via Non-Local Sparse Tensor Factorization,"Renwei Dian, Leyuan Fang, Shutao Li",,,
,,,,,,2302,Reflection Removal Using Low-Rank Matrix Completion,"Byeong-Ju Han, Jae-Young Sim",Removing Clouds and Recovering Ground Observations in Satellite Image Sequences via Temporally Contiguous Robust Matrix Completion,"We consider the problem of removing and replacing clouds in satellite image sequences, which has a wide range of applications in remote sensing. Our approach first detects and removes the cloud-contaminated part of the image sequences. It then recovers the missing scenes from the clean parts using the proposed ""TECROMAC"" (TEmporally Contiguous RObust MAtrix Completion) objective. The objective function balances temporal smoothness with a low rank solution while staying close to the original observations. The matrix whose the rows are pixels and columnsare days corresponding to the image, has low-rank because the pixels reflect land-types such as vegetation, roads and lakes and there are relatively few variations as a result. We provide efficient optimization algorithms for TECROMAC, so we can exploit images containing millions of pixels. Empirical results on real satellite image sequences, as well as simulated data, demonstrate that our approach is able to recover underlying images from heavily cloud-contaminated observations.",http://arxiv.org/pdf/1604.03915v1
,,,,,,2725,Object Co-Skeletonization With Co-Segmentation,"Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, Junsong Yuan",,,
,,,,,Machine Learning,129,Mining Object Parts From CNNs via Active Question-Answering,"Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu",Mining Object Parts from CNNs via Active Question-Answering,"Given a convolutional neural network (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in the pre-trained CNN, which are related to the target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on the pre-trained CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1/6-1/3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods.",http://arxiv.org/pdf/1704.03173v1
,,,,,,224,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,"Xingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin",PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,"A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks are met with diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. A benchmark on the ILSVRC 2012 validation set demonstrates substantial improvements over the state-of-the-art. Compared to Inception-ResNet-v2, it reduces the top-5 error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.",http://arxiv.org/pdf/1611.05725v1
,,,,,,402,The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions,"Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel",The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions,"One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from {image,question,answer} tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. We propose here instead a more general and scalable approach which exploits the fact that very good methods to achieve these operations already exist, and thus do not need to be trained. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases.",http://arxiv.org/pdf/1612.05386v1
,,,,,,417,Joint Discriminative Bayesian Dictionary and Classifier Learning,"Naveed Akhtar, Ajmal Mian, Fatih Porikli",,,
,,,,,,673,Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection,"Nikolay Savinov, Akihito Seki, Ä_ubor LadickÃ_, Torsten Sattler, Marc Pollefeys",,,
,,,,,,816,Outlier-Robust Tensor PCA,"Pan Zhou, Jiashi Feng",,,
,,,,,,888,Learning Adaptive Receptive Fields for Deep Image Parsing Network,"Zhen Wei, Yao Sun, Jinqiao Wang, Hanjiang Lai, Si Liu",,,
,,,,,,1489,Learning an Invariant Hilbert Space for Domain Adaptation,"Samitha Herath, Mehrtash Harandi, Fatih Porikli",Learning an Invariant Hilbert Space for Domain Adaptation,"This paper introduces a learning scheme to construct a Hilbert space (i.e., a vector space along its inner product) to address both unsupervised and semi-supervised domain adaptation problems. This is achieved by learning projections from each domain to a latent space along the Mahalanobis metric of the latent space to simultaneously minimizing a notion of domain variance while maximizing a measure of discriminatory power. In particular, we make use of the Riemannian optimization techniques to match statistical properties (e.g., first and second order statistics) between samples projected into the latent space from different domains. Upon availability of class labels, we further deem samples sharing the same label to form more compact clusters while pulling away samples coming from different classes.We extensively evaluate and contrast our proposal against state-of-the-art methods for the task of visual domain adaptation using both handcrafted and deep-net features. Our experiments show that even with a simple nearest neighbor classifier, the proposed method can outperform several state-of-the-art methods benefitting from more involved classification schemes.",http://arxiv.org/pdf/1611.08350v2
,,,,,,1607,Fixed-Point Factorized Networks,"Peisong Wang, Jian Cheng",Fixed-point Factorized Networks,"In recent years, Deep Neural Networks (DNNs) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision, speech recognition and Natural Language Processing. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) on pre-trained models to reduce the computational complexity as well as the storage requirement of networks. Extensive experiments on large-scale ImageNet classification task show the effectiveness of our proposed method.",http://arxiv.org/pdf/1611.01972v1
,,,,,,1663,Discriminative Optimization: Theory and Applications to Point Cloud Registration,"Jayakorn Vongkulbhisal, Fernando De la Torre, JoÃ£o P. Costeira",,,
,,,,,,1727,Online Asymmetric Similarity Learning for Cross-Modal Retrieval,"Yiling Wu, Shuhui Wang, Qingming Huang",,,
,,,,,,1766,Improving Training of Deep Neural Networks via Singular Value Bounding,"Kui Jia, Dacheng Tao, Shenghua Gao, Xiangmin Xu",Improving training of deep neural networks via Singular Value Bounding,"Deep learning methods achieve great success recently on many computer vision problems, with image classification and object detection as the prominent examples. In spite of these practical successes, optimization of deep networks remains an active topic in deep learning research. In this work, we focus on investigation of the network solution properties that can potentially lead to good performance. Our research is inspired by theoretical and empirical results that use orthogonal matrices to initialize networks, but we are interested in investigating how orthogonal weight matrices perform when network training converges. To this end, we propose to constrain the solutions of weight matrices in the orthogonal feasible set during the whole process of network training, and achieve this by a simple yet effective method called Singular Value Bounding (SVB). In SVB, all singular values of each weight matrix are simply bounded in a narrow band around the value of 1. Based on the same motivation, we also propose Bounded Batch Normalization (BBN), which improves Batch Normalization by removing its potential risk of ill-conditioned layer transform. We present both theoretical and empirical results to justify our proposed methods. Experiments on benchmark image classification datasets show the efficacy of our proposed SVB and BBN. In particular, we achieve the state-of-the-art results of 3.06% error rate on CIFAR10 and 16.90% on CIFAR100, using off-the-shelf network architectures (Wide ResNets). Our preliminary results on ImageNet also show the promise in large-scale learning.",http://arxiv.org/pdf/1611.06013v3
,,,,,,2051,S3Pool: Pooling With Stochastic Spatial Sampling,"Shuangfei Zhai, Hui Wu, Abhishek Kumar, Yu Cheng, Yongxi Lu, Zhongfei Zhang, Rogerio Feris",S3Pool: Pooling with Stochastic Spatial Sampling,"Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two-step procedure: first, a pooling window (e.g., $2\times 2$) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for \emph{learning} (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models. Experimental code is available at https://github.com/Shuangfei/s3pool.",http://arxiv.org/pdf/1611.05138v1
,,,,,,2173,Sports Field Localization via Deep Structured Models,"Namdar Homayounfar, Sanja Fidler, Raquel Urtasun",,,
,,,,,,2255,Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation,"Binghui Chen, Weihong Deng, Junping Du",,,
,,,,,,2451,Switching Convolutional Neural Network for Crowd Counting,"Deepak Babu Sam, Shiv Surya, R. Venkatesh Babu",,,
,,,,,,2575,Network Sketching: Exploiting Binary Structure in Deep CNNs,"Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen",Network Sketching: Exploiting Binary Structure in Deep CNNs,"Convolutional neural networks (CNNs) with deep architectures have substantially advanced the state-of-the-art in computer vision tasks. However, deep networks are typically resource-intensive and thus difficult to be deployed on mobile devices. Recently, CNNs with binary weights have shown compelling efficiency to the community, whereas the accuracy of such models is usually unsatisfactory in practice. In this paper, we introduce network sketching as a novel technique of pursuing binary-weight CNNs, targeting at more faithful inference and better trade-off for practical applications. Our basic idea is to exploit binary structure directly in pre-trained filter banks and produce binary-weight models via tensor expansion. The whole process can be treated as a coarse-to-fine model approximation, akin to the pencil drawing steps of outlining and shading. To further speedup the generated models, namely the sketches, we also propose an associative implementation of binary tensor convolutions. Experimental results demonstrate that a proper sketch of AlexNet (or ResNet) outperforms the existing binary-weight models by large margins on the ImageNet large scale classification task, while the committed memory for network parameters only exceeds a little.",http://arxiv.org/pdf/1706.02021v1
,,,,,,2849,Multi-Task Clustering of Human Actions by Sharing Information,"Xiaoqiang Yan, Shizhe Hu, Yangdong Ye",,,
,,,,,,2951,Soft-Margin Mixture of Regressions,"Dong Huang, Longfei Han, Fernando De la Torre",Bayesian Hierarchical Mixtures of Experts,"The Hierarchical Mixture of Experts (HME) is a well-known tree-based model for regression and classification, based on soft probabilistic splits. In its original formulation it was trained by maximum likelihood, and is therefore prone to over-fitting. Furthermore the maximum likelihood framework offers no natural metric for optimizing the complexity and structure of the tree. Previous attempts to provide a Bayesian treatment of the HME model have relied either on ad-hoc local Gaussian approximations or have dealt with related models representing the joint distribution of both input and output variables. In this paper we describe a fully Bayesian treatment of the HME model based on variational inference. By combining local and global variational methods we obtain a rigourous lower bound on the marginal probability of the data under the model. This bound is optimized during the training phase, and its resulting value can be used for model order selection. We present results using this approach for a data set describing robot arm kinematics.",http://arxiv.org/pdf/1212.2447v1
,,,,,,3053,Multigrid Neural Architectures,"Tsung-Wei Ke, Michael Maire, Stella X. Yu",Multigrid Neural Architectures,"We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of grids. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail.   Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient. Multigrid is independent of other architectural choices; we show synergy in combination with residual connections. Multigrid yields dramatic improvement on a synthetic semantic segmentation dataset. Most strikingly, relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid is a more powerful alternative to existing CNN designs on a flat grid.",http://arxiv.org/pdf/1611.07661v2
,,,,,,3092,High-Resolution Image Inpainting Using Multi-Scale Neural Patch Synthesis,"Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, Hao Li",High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis,"Recent advances in deep learning have shown exciting promise in filling large holes in natural images with semantically plausible and context aware details, impacting fundamental image manipulation tasks such as object removal. While these learning-based methods are significantly more effective in capturing high-level features than prior techniques, they can only handle very low-resolution inputs due to memory limitations and difficulty in training. Even for slightly larger images, the inpainted regions would appear blurry and unpleasant boundaries become visible. We propose a multi-scale neural patch synthesis approach based on joint optimization of image content and texture constraints, which not only preserves contextual structures but also produces high-frequency details by matching and adapting patches with the most similar mid-layer feature correlations of a deep classification network. We evaluate our method on the ImageNet and Paris Streetview datasets and achieved state-of-the-art inpainting accuracy. We show our approach produces sharper and more coherent results than prior methods, especially for high-resolution images.",http://arxiv.org/pdf/1611.09969v2
,,,,,,3107,Deep Quantization: Encoding Convolutional Activations With Deep Generative Model,"Zhaofan Qiu, Ting Yao, Tao Mei",Deep Quantization: Encoding Convolutional Activations with Deep Generative Model,"Deep convolutional neural networks (CNNs) have proven highly effective for visual recognition, where learning a universal representation from activations of convolutional layer plays a fundamental problem. In this paper, we present Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep architecture that quantizes the local activations of convolutional layer in a deep generative model, by training them in an end-to-end manner. To incorporate FV encoding strategy into deep generative models, we introduce Variational Auto-Encoder model, which steers a variational inference and learning in a neural network which can be straightforwardly optimized using standard stochastic gradient method. Different from the FV characterized by conventional generative models (e.g., Gaussian Mixture Model) which parsimoniously fit a discrete mixture model to data distribution, the proposed FV-VAE is more flexible to represent the natural property of data for better generalization. Extensive experiments are conducted on three public datasets, i.e., UCF101, ActivityNet, and CUB-200-2011 in the context of video action recognition and fine-grained image classification, respectively. Superior results are reported when compared to state-of-the-art representations. Most remarkably, our proposed FV-VAE achieves to-date the best published accuracy of 94.2% on UCF101.",http://arxiv.org/pdf/1611.09502v1
,,,,,,3112,DOPE: Distributed Optimization for Pairwise Energies,"Jose Dolz, Ismail Ben Ayed, Christian Desrosiers",DOPE: Distributed Optimization for Pairwise Energies,"We formulate an Alternating Direction Method of Mul-tipliers (ADMM) that systematically distributes the computations of any technique for optimizing pairwise functions, including non-submodular potentials. Such discrete functions are very useful in segmentation and a breadth of other vision problems. Our method decomposes the problem into a large set of small sub-problems, each involving a sub-region of the image domain, which can be solved in parallel. We achieve consistency between the sub-problems through a novel constraint that can be used for a large class of pair-wise functions. We give an iterative numerical solution that alternates between solving the sub-problems and updating consistency variables, until convergence. We report comprehensive experiments, which demonstrate the benefit of our general distributed solution in the case of the popular serial algorithm of Boykov and Kolmogorov (BK algorithm) and, also, in the context of non-submodular functions.",http://arxiv.org/pdf/1704.03116v1
,,,,,,3198,Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis,"Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky",Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis,"The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage.",http://arxiv.org/pdf/1701.02096v1
,,,,,Object Recognition & Scene Understanding,100,Polyhedral Conic Classifiers for Visual Object Detection and Classification,"Hakan Cevikalp, Bill Triggs",,,
,,,,,,258,Incremental Kernel Null Space Discriminant Analysis for Novelty Detection,"Juncheng Liu, Zhouhui Lian, Yi Wang, Jianguo Xiao",,,
,,,,,,283,Predicting Ground-Level Scene Layout From Aerial Imagery,"Menghua Zhai, Zachary Bessinger, Scott Workman, Nathan Jacobs",Predicting Ground-Level Scene Layout from Aerial Imagery,"We introduce a novel strategy for learning to extract semantically meaningful features from aerial imagery. Instead of manually labeling the aerial imagery, we propose to predict (noisy) semantic features automatically extracted from co-located ground imagery. Our network architecture takes an aerial image as input, extracts features using a convolutional neural network, and then applies an adaptive transformation to map these features into the ground-level perspective. We use an end-to-end learning approach to minimize the difference between the semantic segmentation extracted directly from the ground image and the semantic segmentation predicted solely based on the aerial image. We show that a model learned using this strategy, with no additional training, is already capable of rough semantic labeling of aerial imagery. Furthermore, we demonstrate that by finetuning this model we can achieve more accurate semantic segmentation than two baseline initialization strategies. We use our network to address the task of estimating the geolocation and geoorientation of a ground image. Finally, we show how features extracted from an aerial image can be used to hallucinate a plausible ground-level panorama.",http://arxiv.org/pdf/1612.02709v1
,,,,,,863,Deep Feature Flow for Video Recognition,"Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei",Deep Feature Flow for Video Recognition,"Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition.",http://arxiv.org/pdf/1611.07715v2
,,,,,,1010,Object-Aware Dense Semantic Correspondence,"Fan Yang, Xin Li, Hong Cheng, Jianping Li, Leiting Chen",Learning Spatiotemporal-Aware Representation for POI Recommendation,"The wide spread of location-based social networks brings about a huge volume of user check-in data, which facilitates the recommendation of points of interest (POIs). Recent advances on distributed representation shed light on learning low dimensional dense vectors to alleviate the data sparsity problem. Current studies on representation learning for POI recommendation embed both users and POIs in a common latent space, and users' preference is inferred based on the distance/similarity between a user and a POI. Such an approach is not in accordance with the semantics of users and POIs as they are inherently different objects. In this paper, we present a novel spatiotemporal aware (STA) representation, which models the spatial and temporal information as \emph{a relationship connecting users and POIs}. Our model generalizes the recent advances in knowledge graph embedding. The basic idea is that the embedding of a $<$time, location$>$ pair corresponds to a translation from embeddings of users to POIs. Since the POI embedding should be close to the user embedding plus the relationship vector, the recommendation can be performed by selecting the top-\emph{k} POIs similar to the translated POI, which are all of the same type of objects. We conduct extensive experiments on two real-world datasets. The results demonstrate that our STA model achieves the state-of-the-art performance in terms of high recommendation accuracy, robustness to data sparsity and effectiveness in handling cold start problem.",http://arxiv.org/pdf/1704.08853v1
,,,,,,1050,Semantic Regularisation for Recurrent Image Annotation,"Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun",Semantic Regularisation for Recurrent Image Annotation,"The ""CNN-RNN"" design pattern is increasingly widely applied in a variety of image annotation tasks including multi-label classification and captioning. Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This leaves the RNN overstretched with two jobs: predicting the visual concepts and modelling their correlations for generating structured annotation output. Importantly this makes the end-to-end training of the CNN and RNN slow and ineffective due to the difficulty of back propagating gradients through the RNN to train the CNN. We propose a simple modification to the design pattern that makes learning more effective and efficient. Specifically, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN. Regularising the interface can partially or completely decouple the learning problems, allowing each to be more effectively trained and jointly training much more efficient. Extensive experiments show that state-of-the art performance is achieved on multi-label classification as well as image captioning.",http://arxiv.org/pdf/1611.05490v1
,,,,,,1633,Video2Shop: Exact Matching Clothes in Videos to Online Shopping Images,"Zhi-Qi Cheng, Xiao Wu, Yang Liu, Xian-Sheng Hua",,,
,,,,,,1925,Fast-At: Fast Automatic Thumbnail Generation Using Deep Neural Networks,"Seyed A. Esmaeili, Bharat Singh, Larry S. Davis",Fast-AT: Fast Automatic Thumbnail Generation using Deep Neural Networks,"Fast-AT is an automatic thumbnail generation system based on deep neural networks. It is a fully-convolutional deep neural network, which learns specific filters for thumbnails of different sizes and aspect ratios. During inference, the appropriate filter is selected depending on the dimensions of the target thumbnail. Unlike most previous work, Fast-AT does not utilize saliency but addresses the problem directly. In addition, it eliminates the need to conduct region search on the saliency map. The model generalizes to thumbnails of different sizes including those with extreme aspect ratios and can generate thumbnails in real time. A data set of more than 70,000 thumbnail annotations was collected to train Fast-AT. We show competitive results in comparison to existing techniques.",http://arxiv.org/pdf/1612.04811v2
,,,,,,1955,Multi-Level Attention Networks for Visual Question Answering,"Dongfei Yu, Jianlong Fu, Tao Mei, Yong Rui",,,
,,,,,,2052,Generating Descriptions With Grounded and Co-Referenced People,"Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon Oh, Bernt Schiele",Generating Descriptions with Grounded and Co-Referenced People,"Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters.",http://arxiv.org/pdf/1704.01518v1
,,,,,,2960,Straight to Shapes: Real-Time Detection of Encoded Shapes,"Saumya Jetley, Michael Sapienza, Stuart Golodetz, Philip H. S. Torr",Straight to Shapes: Real-time Detection of Encoded Shapes,"Current object detection approaches predict bounding boxes, but these provide little instance-specific information beyond location, scale and aspect ratio. In this work, we propose to directly regress to objects' shapes in addition to their bounding boxes and categories. It is crucial to find an appropriate shape representation that is compact and decodable, and in which objects can be compared for higher-order concepts such as view similarity, pose variation and occlusion. To achieve this, we use a denoising convolutional auto-encoder to establish an embedding space, and place the decoder after a fast end-to-end network trained to regress directly to the encoded shape vectors. This yields what to the best of our knowledge is the first real-time shape prediction network, running at ~35 FPS on a high-end desktop. With higher-order shape reasoning well-integrated into the network pipeline, the network shows the useful practical quality of generalising to unseen categories that are similar to the ones in the training set, something that most existing approaches fail to handle.",http://arxiv.org/pdf/1611.07932v1
,,,,,,3012,Simultaneous Feature Aggregating and Hashing for Large-Scale Image Search,"Thanh-Toan Do, Dang-Khoa Le Tan, Trung T. Pham, Ngai-Man Cheung",Simultaneous Feature Aggregating and Hashing for Large-scale Image Search,"In most state-of-the-art hashing-based visual search systems, local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work, the aggregating and the hashing processes are designed independently. In this paper, we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically, our joint optimization produces aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition, we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.",http://arxiv.org/pdf/1704.00860v1
,,,,,,3202,Improving Facial Attribute Prediction Using Semantic Segmentation,"Mahdi M. Kalayeh, Boqing Gong, Mubarak Shah",Improving Facial Attribute Prediction using Semantic Segmentation,"Attributes are semantically meaningful characteristics whose applicability widely crosses category boundaries. They are particularly important in describing and recognizing concepts where no explicit training example is given, \textit{e.g., zero-shot learning}. Additionally, since attributes are human describable, they can be used for efficient human-computer interaction. In this paper, we propose to employ semantic segmentation to improve facial attribute prediction. The core idea lies in the fact that many facial attributes describe local properties. In other words, the probability of an attribute to appear in a face image is far from being uniform in the spatial domain. We build our facial attribute prediction model jointly with a deep semantic segmentation network. This harnesses the localization cues learned by the semantic segmentation to guide the attention of the attribute prediction to the regions where different attributes naturally show up. As a result of this approach, in addition to recognition, we are able to localize the attributes, despite merely having access to image level labels (weak supervision) during training. We evaluate our proposed method on CelebA and LFWA datasets and achieve superior results to the prior arts. Furthermore, we show that in the reverse problem, semantic face parsing improves when facial attributes are available. That reaffirms the need to jointly model these two interconnected tasks.",http://arxiv.org/pdf/1704.08740v1
,,,,,Video Analytics,2252,Learning Cross-Modal Deep Representations for Robust Pedestrian Detection,"Dan Xu, Wanli Ouyang, Elisa Ricci, Xiaogang Wang, Nicu Sebe",Learning Cross-Modal Deep Representations for Robust Pedestrian Detection,"This paper presents a novel method for detecting pedestrians under adverse illumination conditions. Our approach relies on a novel cross-modality learning framework and it is based on two main phases. First, given a multimodal dataset, a deep convolutional network is employed to learn a non-linear mapping, modeling the relations between RGB and thermal data. Then, the learned feature representations are transferred to a second deep network, which receives as input an RGB image and outputs the detection results. In this way, features which are both discriminative and robust to bad illumination conditions are learned. Importantly, at test time, only the second pipeline is considered and no thermal data are required. Our extensive evaluation demonstrates that the proposed approach outperforms the state-of- the-art on the challenging KAIST multispectral pedestrian dataset and it is competitive with previous methods on the popular Caltech dataset.",http://arxiv.org/pdf/1704.02431v1
,,,,,,2321,Spatio-Temporal Self-Organizing Map Deep Network for Dynamic Object Detection From Videos,"Yang Du, Chunfeng Yuan, Bing Li, Weiming Hu, Stephen Maybank",,,
,,,,,,2332,CERN: Confidence-Energy Recurrent Network for Group Activity Recognition,"Tianmin Shu, Sinisa Todorovic, Song-Chun Zhu",CERN: Confidence-Energy Recurrent Network for Group Activity Recognition,"This work is about recognizing human activities occurring in videos at distinct semantic levels, including individual actions, interactions, and group activities. The recognition is realized using a two-level hierarchy of Long Short-Term Memory (LSTM) networks, forming a feed-forward deep architecture, which can be trained end-to-end. In comparison with existing architectures of LSTMs, we make two key contributions giving the name to our approach as Confidence-Energy Recurrent Network -- CERN. First, instead of using the common softmax layer for prediction, we specify a novel energy layer (EL) for estimating the energy of our predictions. Second, rather than finding the common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that the EL additionally computes the p-values of the solutions, and in this way estimates the most confident energy minimum. The evaluation on the Collective Activity and Volleyball datasets demonstrates: (i) advantages of our two contributions relative to the common softmax and energy-minimization formulations and (ii) a superior performance relative to the state-of-the-art approaches.",http://arxiv.org/pdf/1704.03058v1
,,,,,,2551,Understanding Traffic Density From Large-Scale Web Camera Data,"Shanghang Zhang, Guanhang Wu, JoÃ£o P. Costeira, JosÃ© M. F. Moura",Understanding Traffic Density from Large-Scale Web Camera Data,"Understanding traffic density from large-scale web camera (webcam) videos is a challenging problem because such videos have low spatial and temporal resolution, high occlusion and large perspective. To deeply understand traffic density, we explore both deep learning based and optimization based methods. To avoid individual vehicle detection and tracking, both methods map the image into vehicle density map, one based on rank constrained regression and the other one based on fully convolution networks (FCN). The regression based method learns different weights for different blocks in the image to increase freedom degrees of weights and embed perspective information. The FCN based method jointly estimates vehicle density map and vehicle count with a residual learning framework to perform end-to-end dense prediction, allowing arbitrary image resolution, and adapting to different vehicle scales and perspectives. We analyze and compare both methods, and get insights from optimization based method to improve deep model. Since existing datasets do not cover all the challenges in our work, we collected and labelled a large-scale traffic video dataset, containing 60 million frames from 212 webcams. Both methods are extensively evaluated and compared on different counting tasks and datasets. FCN based method significantly reduces the mean absolute error from 10.99 to 5.31 on the public dataset TRANCOS compared with the state-of-the-art baseline.",http://arxiv.org/pdf/1703.05868v2
,,,,,,3324,Collaborative Summarization of Topic-Related Videos,"Rameswar Panda, Amit K. Roy-Chowdhury",Collaborative Summarization of Topic-Related Videos,"Large collections of videos are grouped into clusters by a topic keyword, such as Eiffel Tower or Surfing, with many important visual concepts repeating across them. Such a topically close set of videos have mutual influence on each other, which could be used to summarize one of them by exploiting information from others in the set. We build on this intuition to develop a novel approach to extract a summary that simultaneously captures both important particularities arising in the given video, as well as, generalities identified from the set of videos. The topic-related videos provide visual context to identify the important parts of the video being summarized. We achieve this by developing a collaborative sparse optimization method which can be efficiently solved by a half-quadratic minimization algorithm. Our work builds upon the idea of collaborative techniques from information retrieval and natural language processing, which typically use the attributes of other similar objects to predict the attribute of a given object. Experiments on two challenging and diverse datasets well demonstrate the efficacy of our approach over state-of-the-art methods.",http://arxiv.org/pdf/1706.03114v1
"Monday, July 24, 2017",0830â€“1000,Kamehameha III,17,Spotlight 3-1A,Machine Learning 3,19,Local Binary Convolutional Neural Networks,"Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides",Local Binary Convolutional Neural Networks,"We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN). The design principles of LBC are motivated by local binary patterns (LBP). The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer. The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer. Furthermore, due to lower model complexity and sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer. We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer. Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), reach state-of-the-art performance on a range of visual datasets (MNIST, SVHN, CIFAR-10, and a subset of ImageNet) while enjoying significant computational savings.",http://arxiv.org/pdf/1608.06049v1
,,,,,,508,Deep Self-Taught Learning for Weakly Supervised Object Localization,"Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, Wei Liu",Deep Self-Taught Learning for Weakly Supervised Object Localization,"Most existing weakly supervised localization (WSL) approaches learn detectors by finding positive bounding boxes based on features learned with image-level supervision. However, those features do not contain spatial location related information and usually provide poor-quality positive samples for training a detector. To overcome this issue, we propose a deep self-taught learning approach, which makes the detector learn the object-level features reliable for acquiring tight positive samples and afterwards re-train itself based on them. Consequently, the detector progressively improves its detection ability and localizes more informative positive samples. To implement such self-taught learning, we propose a seed sample acquisition method via image-to-object transferring and dense subgraph discovery to find reliable positive samples for initializing the detector. An online supportive sample harvesting scheme is further proposed to dynamically select the most confident tight positive samples and train the detector in a mutual boosting way. To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process. Extensive experiments on PASCAL 2007 and 2012 show that our approach outperforms the state-of-the-arts, strongly validating its effectiveness.",http://arxiv.org/pdf/1704.05188v2
,,,,,,634,Multi-Modal Mean-Fields via Cardinality-Based Clamping,"Pierre BaquÃ©, FranÃ§ois Fleuret, Pascal Fua",Multi-Modal Mean-Fields via Cardinality-Based Clamping,"Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields.",http://arxiv.org/pdf/1611.07941v1
,,,,,,1132,Probabilistic Temporal Subspace Clustering,"Behnam Gholami, Vladimir Pavlovic","Dynamical functional prediction and classification, with application to traffic flow prediction","Motivated by the need for accurate traffic flow prediction in transportation management, we propose a functional data method to analyze traffic flow patterns and predict future traffic flow. In this study we approach the problem by sampling traffic flow trajectories from a mixture of stochastic processes. The proposed functional mixture prediction approach combines functional prediction with probabilistic functional classification to take distinct traffic flow patterns into account. The probabilistic classification procedure, which incorporates functional clustering and discrimination, hinges on subspace projection. The proposed methods not only assist in predicting traffic flow trajectories, but also identify distinct patterns in daily traffic flow of typical temporal trends and variabilities. The proposed methodology is widely applicable in analysis and prediction of longitudinally recorded functional data.",http://arxiv.org/pdf/1301.2399v1
,,,,,,1258,Provable Self-Representation Based Outlier Detection in a Union of Subspaces,"Chong You, Daniel P. Robinson, RenÃ© Vidal",Provable Self-Representation Based Outlier Detection in a Union of Subspaces,"Many computer vision tasks involve processing large amounts of data contaminated by outliers, which need to be detected and rejected. While outlier detection methods based on robust statistics have existed for decades, only recently have methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection when the inliers lie in one or more low-dimensional subspaces. This paper proposes a new outlier detection method that combines tools from sparse representation with random walks on a graph. By exploiting the property that data points can be expressed as sparse linear combinations of each other, we obtain an asymmetric affinity matrix among data points, which we use to construct a weighted directed graph. By defining a suitable Markov Chain from this graph, we establish a connection between inliers/outliers and essential/inessential states of the Markov chain, which allows us to detect outliers by using random walks. We provide a theoretical analysis that justifies the correctness of our method under geometric and connectivity assumptions. Experimental results on image databases demonstrate its superiority with respect to state-of-the-art sparse and low-rank outlier detection methods.",http://arxiv.org/pdf/1704.03925v1
,,,,,,1728,Latent Multi-View Subspace Clustering,"Changqing Zhang, Qinghua Hu, Huazhu Fu, Pengfei Zhu, Xiaochun Cao",Low-rank Multi-view Clustering in Third-Order Tensor Space,"The plenty information from multiple views data as well as the complementary information among different views are usually beneficial to various tasks, e.g., clustering, classification, de-noising. Multi-view subspace clustering is based on the fact that the multi-view data are generated from a latent subspace. To recover the underlying subspace structure, the success of the sparse and/or low-rank subspace clustering has been witnessed recently. Despite some state-of-the-art subspace clustering approaches can numerically handle multi-view data, by simultaneously exploring all possible pairwise correlation within views, the high order statistics is often disregarded which can only be captured by simultaneously utilizing all views. As a consequence, the clustering performance for multi-view data is compromised. To address this issue, in this paper, a novel multi-view clustering method is proposed by using \textit{t-product} in third-order tensor space. Based on the circular convolution operation, multi-view data can be effectively represented by a \textit{t-linear} combination with sparse and low-rank penalty using ""self-expressiveness"". Our extensive experimental results on facial, object, digits image and text data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of many criteria.",http://arxiv.org/pdf/1608.08336v2
,,,,,,2231,Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks,"Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, C. Lee Giles",Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Network,"We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.",http://arxiv.org/pdf/1706.02337v1
,,,,,,2503,Age Progression/Regression by Conditional Adversarial Autoencoder,"Zhifei Zhang, Yang Song, Hairong Qi",Age Progression/Regression by Conditional Adversarial Autoencoder,"""If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5?"" The answer is probably a ""No."" Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth.",http://arxiv.org/pdf/1702.08423v2
,,,,Oral 3-1A,,106,Compact Matrix Factorization With Dependent Subspaces,"Viktor Larsson, Carl Olsson",,,
,,,,,,630,FFTLasso: Large-Scale LASSO in the Fourier Domain,"Adel Bibi, Hani Itani, Bernard Ghanem",,,
,,,,,,2020,On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution,"Yuqian Zhang, Yenson Lau, Han-wen Kuo, Sky Cheung, Abhay Pasupathy, John Wright",,,
,,,,,,3586,Global Optimality in Neural Network Training,"Benjamin D. Haeffele, RenÃ© Vidal",Evolutionary Artificial Neural Network Based on Chemical Reaction Optimization,"Evolutionary algorithms (EAs) are very popular tools to design and evolve artificial neural networks (ANNs), especially to train them. These methods have advantages over the conventional backpropagation (BP) method because of their low computational requirement when searching in a large solution space. In this paper, we employ Chemical Reaction Optimization (CRO), a newly developed global optimization method, to replace BP in training neural networks. CRO is a population-based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction. Simulation results show that CRO outperforms many EA strategies commonly used to train neural networks.",http://arxiv.org/pdf/1502.00193v1
"Monday, July 24, 2017",0830â€“1000,KalÄÅkaua Ballroom,18,Spotlight 3-1B,Object Recognition & Scene Understanding 2,1686,What Is and What Is Not a Salient Object? Learning Salient Object Detector by Ensembling Linear Exemplar Regressors,"Changqun Xia, Jia Li, Xiaowu Chen, Anlin Zheng, Yu Zhang",,,
,,,,,,279,Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection,"Xiaodan Liang, Lisa Lee, Eric P. Xing",Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection,"Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.",http://arxiv.org/pdf/1703.03054v1
,,,,,,384,Modeling Relationships in Referential Expressions With Compositional Modular Networks,"Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, Kate Saenko",Modeling Relationships in Referential Expressions with Compositional Modular Networks,"People often refer to entities in an image in terms of their relationships with other entities. For example, ""the black cat sitting under the table"" refers to both a ""black cat"" entity and its relationship with another ""table"" entity. Understanding these relationships is essential for interpreting and grounding such natural language expressions. Most prior work focuses on either grounding entire referential expressions holistically to one region, or localizing relationships based on a fixed set of categories. In this paper we instead present a modular deep architecture capable of analyzing referential expressions into their component parts, identifying entities and relationships mentioned in the input expression and grounding them all in the scene. We call this approach Compositional Modular Networks (CMNs): a novel architecture that learns linguistic analysis and visual inference end-to-end. Our approach is built around two types of neural modules that inspect local regions and pairwise interactions between regions. We evaluate CMNs on multiple referential expression datasets, outperforming state-of-the-art approaches on all tasks.",http://arxiv.org/pdf/1611.09978v1
,,,,,,386,Counting Everyday Objects in Everyday Scenes,"Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R. Selvaraju, Dhruv Batra, Devi Parikh",Counting Everyday Objects in Everyday Scenes,"We are interested in counting the number of instances of object classes in natural, everyday images. Previous counting approaches tackle the problem in restricted domains such as counting pedestrians in surveillance videos. Counts can also be estimated from outputs of other vision tasks like object detection. In this work, we build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. Our approach is inspired by the phenomenon of subitizing - the ability of humans to make quick assessments of counts given a perceptual signal, for small count values. Given a natural scene, we employ a divide and conquer strategy while incorporating context across the scene to adapt the subitizing idea to counting. Our approach offers consistent improvements over numerous baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets. Subsequently, we study how counting can be used to improve object detection. We then show a proof of concept application of our counting methods to the task of Visual Question Answering, by studying the `how many?' questions in the VQA and COCO-QA datasets.",http://arxiv.org/pdf/1604.03505v3
,,,,,,864,Fully Convolutional Instance-Aware Semantic Segmentation,"Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei",Fully Convolutional Instance-aware Semantic Segmentation,"We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released at \url{https://github.com/daijifeng001/TA-FCN}.",http://arxiv.org/pdf/1611.07709v2
,,,,,,1173,Semantic Autoencoder for Zero-Shot Learning,"Elyor Kodirov, Tao Xiang, Shaogang Gong",Semantic Autoencoder for Zero-Shot Learning,"Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g.~attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g.~attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.",http://arxiv.org/pdf/1704.08345v1
,,,,,,1191,CityPersons: A Diverse Dataset for Pedestrian Detection,"Shanshan Zhang, Rodrigo Benenson, Bernt Schiele",CityPersons: A Diverse Dataset for Pedestrian Detection,"Convnets have enabled significant progress in pedestrian detection recently, but there are still open questions regarding suitable architectures and training data. We revisit CNN design and point out key adaptations, enabling plain FasterRCNN to obtain state-of-the-art results on the Caltech dataset.   To achieve further improvement from more and better data, we introduce CityPersons, a new set of person annotations on top of the Cityscapes dataset. The diversity of CityPersons allows us for the first time to train one single CNN model that generalizes well over multiple benchmarks. Moreover, with additional training with CityPersons, we obtain top results using FasterRCNN on Caltech, improving especially for more difficult cases (heavy occlusion and small scale) and providing higher localization quality.",http://arxiv.org/pdf/1702.05693v1
,,,,,,2327,GuessWhat?! Visual Object Discovery Through Multi-Modal Dialogue,"Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, Aaron Courville",,,
,,,,Oral 3-1B,,1813,Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition,"Jianlong Fu, Heliang Zheng, Tao Mei",,,
,,,,,,2187,Annotating Object Instances With a Polygon-RNN,"LluÃ_s CastrejÃ_n, Kaustav Kundu, Raquel Urtasun, Sanja Fidler",Annotating Object Instances with a Polygon-RNN,"We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We further show generalization capabilities of our approach to unseen datasets.",http://arxiv.org/pdf/1704.05548v1
,,,,,,2370,Connecting Look and Feel: Associating the Visual and Tactile Properties of Physical Materials,"Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, Edward Adelson",Connecting Look and Feel: Associating the visual and tactile properties of physical materials,"For machines to interact with the physical world, they must understand the physical properties of objects and materials they encounter. We use fabrics as an example of a deformable material with a rich set of mechanical properties. A thin flexible fabric, when draped, tends to look different from a heavy stiff fabric. It also feels different when touched. Using a collection of 118 fabric sample, we captured color and depth images of draped fabrics along with tactile data from a high resolution touch sensor. We then sought to associate the information from vision and touch by jointly training CNNs across the three modalities. Through the CNN, each input, regardless of the modality, generates an embedding vector that records the fabric's physical property. By comparing the embeddings, our system is able to look at a fabric image and predict how it will feel, and vice versa. We also show that a system jointly trained on vision and touch data can outperform a similar system trained only on visual data when tested purely with visual inputs.",http://arxiv.org/pdf/1704.03822v1
,,,,,,3126,Deep Learning Human Mind for Automated Visual Classification,"Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, Mubarak Shah",Deep Learning Human Mind for Automated Visual Classification,"What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories. Afterwards, we train a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus effectively allowing machines to employ human brain-based features for automated visual classification. We use a 32-channel EEG to record brain activity of seven subjects while looking at images of 40 ImageNet object classes. The proposed RNN based approach for discriminating object classes using brain signals reaches an average accuracy of about 40%, which outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models, both on ImageNet and CalTech 101, thus demonstrating its classification and generalization capabilities. This gives us a real hope that, indeed, human mind can be read and transferred to machines.",http://arxiv.org/pdf/1609.00344v1
"Monday, July 24, 2017",1000â€“1200,Kamehameha I,19,Poster 3-1,3D Computer Vision,259,Self-Calibration-Based Approach to Critical Motion Sequences of Rolling-Shutter Structure From Motion,"Eisuke Ito, Takayuki Okatani",Self-calibration-based Approach to Critical Motion Sequences of Rolling-shutter Structure from Motion,"In this paper we consider critical motion sequences (CMSs) of rolling-shutter (RS) SfM. Employing an RS camera model with linearized pure rotation, we show that the RS distortion can be approximately expressed by two internal parameters of an ""imaginary"" camera plus one-parameter nonlinear transformation similar to lens distortion. We then reformulate the problem as self-calibration of the imaginary camera, in which its skew and aspect ratio are unknown and varying in the image sequence. In the formulation, we derive a general representation of CMSs. We also show that our method can explain the CMS that was recently reported in the literature, and then present a new remedy to deal with the degeneracy. Our theoretical results agree well with experimental results; it explains degeneracies observed when we employ naive bundle adjustment, and how they are resolved by our method.",http://arxiv.org/pdf/1611.05476v1
,,,,,,311,Semi-Calibrated Near Field Photometric Stereo,"Fotios Logothetis, Roberto Mecca, Roberto Cipolla",,,
,,,,,,885,Semantic Multi-View Stereo: Jointly Estimating Objects and Voxels,"Ali Osman Ulusoy, Michael J. Black, Andreas Geiger",,,
,,,,,,894,Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps,"Matteo Poggi, Stefano Mattoccia",,,
,,,,,,1021,The Misty Three Point Algorithm for Relative Pose,"Tobias PalmÃ©r, Kalle Ã…strÃ¶m, Jan-Michael Frahm",,,
,,,,,,1095,The Surfacing of Multiview 3D Drawings via Lofting and Occlusion Reasoning,"Anil Usumezbas, Ricardo Fabbri, Benjamin B. Kimia",,,
,,,,,,1226,A New Representation of Skeleton Sequences for 3D Action Recognition,"Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, Farid Boussaid",A New Representation of Skeleton Sequences for 3D Action Recognition,"This paper presents a new method for 3D action recognition with skeleton sequences (i.e., 3D trajectories of human skeleton joints). The proposed method first transforms each skeleton sequence into three clips each consisting of several frames for spatial temporal feature learning using deep neural networks. Each clip is generated from one channel of the cylindrical coordinates of the skeleton sequence. Each frame of the generated clips represents the temporal information of the entire skeleton sequence, and incorporates one particular spatial relationship between the joints. The entire clips include multiple frames with different spatial relationships, which provide useful spatial structural information of the human skeleton. We propose to use deep convolutional neural networks to learn long-term temporal information of the skeleton sequence from the frames of the generated clips, and then use a Multi-Task Learning Network (MTLN) to jointly process all frames of the generated clips in parallel to incorporate spatial structural information for action recognition. Experimental results clearly show the effectiveness of the proposed new representation and feature learning method for 3D action recognition.",http://arxiv.org/pdf/1703.03492v3
,,,,,,1249,A General Framework for Curve and Surface Comparison and Registration With Oriented Varifolds,"IrÃ¨ne Kaltenmark, Benjamin Charlier, Nicolas Charon",,,
,,,,,,1265,Learning to Align Semantic Segmentation and 2.5D Maps for Geolocalization,"Anil Armagan, Martin Hirzer, Peter M. Roth, Vincent Lepetit",,,
,,,,,,1824,A Generative Model for Depth-Based Robust 3D Facial Pose Tracking,"Lu Sheng, Jianfei Cai, Tat-Jen Cham, Vladimir Pavlovic, King Ngi Ngan",,,
,,,,,,2989,Fast 3D Reconstruction of Faces With Glasses,"Fabio Maninchedda, Martin R. Oswald, Marc Pollefeys",,,
,,,,,,3442,An Efficient Algebraic Solution to the Perspective-Three-Point Problem,"Tong Ke, Stergios I. Roumeliotis",An Efficient Algebraic Solution to the Perspective-Three-Point Problem,"In this work, we present an algebraic solution to the classical perspective-3-point (P3P) problem for determining the position and attitude of a camera from observations of three known reference points. In contrast to previous approaches, we first directly determine the camera's attitude by employing the corresponding geometric constraints to formulate a system of trigonometric equations. This is then efficiently solved, following an algebraic approach, to determine the unknown rotation matrix and subsequently the camera's position. As compared to recent alternatives, our method avoids computing unnecessary (and potentially numerically unstable) intermediate results, and thus achieves higher numerical accuracy and robustness at a lower computational cost. These benefits are validated through extensive Monte-Carlo simulations for both nominal and close-to-singular geometric configurations.",http://arxiv.org/pdf/1701.08237v1
,,,,,Analyzing Humans in Images,40,Learning From Synthetic Humans,"GÃ_l Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, Cordelia Schmid",Learning from Synthetic Humans,"Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data.",http://arxiv.org/pdf/1701.01370v2
,,,,,,249,Forecasting Interactive Dynamics of Pedestrians With Fictitious Play,"Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani",Forecasting Interactive Dynamics of Pedestrians with Fictitious Play,"We develop predictive models of pedestrian dynamics by encoding the coupled nature of multi-pedestrian interaction using game theory, and deep learning-based visual analysis to estimate person-specific behavior parameters. Building predictive models for multi-pedestrian interactions however, is very challenging due to two reasons: (1) the dynamics of interaction are complex interdependent processes, where the predicted behavior of one pedestrian can affect the actions taken by others and (2) dynamics are variable depending on an individuals physical characteristics (e.g., an older person may walk slowly while the younger person may walk faster). To address these challenges, we (1) utilize concepts from game theory to model the interdependent decision making process of multiple pedestrians and (2) use visual classifiers to learn a mapping from pedestrian appearance to behavior parameters. We evaluate our proposed model on several public multiple pedestrian interaction video datasets. Results show that our strategic planning model explains human interactions 25% better when compared to state-of-the-art methods.",http://arxiv.org/pdf/1604.01431v3
,,,,,,394,Hand Keypoint Detection in Single Images Using Multiview Bootstrapping,"Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh",Hand Keypoint Detection in Single Images using Multiview Bootstrapping,"We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.",http://arxiv.org/pdf/1704.07809v1
,,,,,,740,PoseTrack: Joint Multi-Person Pose Estimation and Tracking,"Umar Iqbal, Anton Milan, Juergen Gall",PoseTrack: Joint Multi-Person Pose Estimation and Tracking,"In this work, we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos. Existing methods for multi-person pose estimation in images cannot be applied directly to this problem, since it also requires to solve the problem of person association over time in addition to the pose estimation for each person. We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation. To this end, we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person. The proposed approach implicitly handles occlusion and truncation of persons. Since the problem has not been addressed quantitatively in the literature, we introduce a challenging ""Multi-Person PoseTrack"" dataset, and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale, size, location or the number of persons. Finally, we evaluate the proposed approach and several baseline methods on our new dataset.",http://arxiv.org/pdf/1611.07727v3
,,,,,,809,Expecting the Unexpected: Training Detectors for Unusual Pedestrians With Adversarial Imposters,"Shiyu Huang, Deva Ramanan",Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters,"As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail"" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To allow for large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right ""priors"" or parameters for synthesis: we would like realistic data with poses and object configurations that mimic true Precarious Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem the Adversarial Imposters. We demonstrate that this simple pipeline allows one to synthesize realistic training data by making use of rendering/animation engines within a GAN framework. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Adversarial Imposters can also be used for ""in-the-tail"" validation at test-time, a notoriously difficult challenge for real-world deployment.",http://arxiv.org/pdf/1703.06283v2
,,,,,,1057,On Human Motion Prediction Using Recurrent Neural Networks,"Julieta Martinez, Michael J. Black, Javier Romero",On human motion prediction using recurrent neural networks,"Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.",http://arxiv.org/pdf/1705.02445v1
,,,,,,1280,Learning and Refining of Privileged Information-Based RNNs for Action Recognition From Depth Sequences,"Zhiyuan Shi, Tae-Kyun Kim",Learning and Refining of Privileged Information-based RNNs for Action Recognition from Depth Sequences,"Existing RNN-based approaches for action recognition from depth sequences require either skeleton joints or hand-crafted depth features as inputs. An end-to-end manner, mapping from raw depth maps to action classes, is non-trivial to design due to the fact that: 1) single channel map lacks texture thus weakens the discriminative power; 2) relatively small set of depth training data. To address these challenges, we propose to learn an RNN driven by privileged information (PI) in three-steps: An encoder is pre-trained to learn a joint embedding of depth appearance and PI (i.e. skeleton joints). The learned embedding layers are then tuned in the learning step, aiming to optimize the network by exploiting PI in a form of multi-task loss. However, exploiting PI as a secondary task provides little help to improve the performance of a primary task (i.e. classification) due to the gap between them. Finally, a bridging matrix is defined to connect two tasks by discovering latent PI in the refining step. Our PI-based classification loss maintains a consistency between latent PI and predicted distribution. The latent PI and network are iteratively estimated and updated in an expectation-maximization procedure. The proposed learning process provides greater discriminative power to model subtle depth difference, while helping avoid overfitting the scarcer training data. Our experiments show significant performance gains over state-of-the-art methods on three public benchmark datasets and our newly collected Blanket dataset.",http://arxiv.org/pdf/1703.09625v3
,,,,,,2493,Quality Aware Network for Set to Set Recognition,"Yu Liu, Junjie Yan, Wanli Ouyang",Quality Aware Network for Set to Set Recognition,"This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at https://github.com/sciencefans/Quality-Aware-Network.",http://arxiv.org/pdf/1704.03373v1
,,,,,,2644,Unite the People: Closing the Loop Between 3D and 2D Human Representations,"Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, Peter V. Gehler",Unite the People: Closing the Loop Between 3D and 2D Human Representations,"3D models provide a common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits ""in-the- wild"". However, depending on the level of detail, it can be hard to impossible to acquire labeled data for training 2D estimators on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits for multiple human pose datasets. Human annotators solely sort good and bad fits. This procedure leads to an initial dataset, UP-3D, with rich annotations. With a comprehensive set of experiments, we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail: our models predict 31 segments and 91 landmark locations on the body. Using the 91 landmark pose estimator, we present state-of-the art results for 3D human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure. We show that UP-3D can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. The data, code and models are available for research purposes.",http://arxiv.org/pdf/1701.02468v2
,,,,,,2789,Deep Multitask Architecture for Integrated 2D and 3D Human Sensing,"Alin-Ionut Popa, Mihai Zanfir, Cristian Sminchisescu",Deep Multitask Architecture for Integrated 2D and 3D Human Sensing,"We propose a deep multitask architecture for \emph{fully automatic 2d and 3d human sensing} (DMHS), including \emph{recognition and reconstruction}, in \emph{monocular images}. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data.",http://arxiv.org/pdf/1701.08985v1
,,,,,,2790,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset","JoÃ£o Carreira, Andrew Zisserman",,,
,,,,,Applications,2125,Identifying First-Person Camera Wearers in Third-Person Videos,"Chenyou Fan, Jangwon Lee, Mingze Xu, Krishna Kumar Singh, Yong Jae Lee, David J. Crandall, Michael S. Ryoo",Identifying First-person Camera Wearers in Third-person Videos,"We consider scenarios in which we wish to perform joint scene understanding, object tracking, activity recognition, and other tasks in environments in which multiple people are wearing body-worn cameras while a third-person static camera also captures the scene. To do this, we need to establish person-level correspondences across first- and third-person videos, which is challenging because the camera wearer is not visible from his/her own egocentric video, preventing the use of direct feature matching. In this paper, we propose a new semi-Siamese Convolutional Neural Network architecture to address this novel challenge. We formulate the problem as learning a joint embedding space for first- and third-person videos that considers both spatial- and motion-domain cues. A new triplet loss function is designed to minimize the distance between correct first- and third-person matches while maximizing the distance between incorrect ones. This end-to-end approach performs significantly better than several baselines, in part by learning the first- and third-person features optimized for matching jointly with the distance measure itself.",http://arxiv.org/pdf/1704.06340v1
,,,,,None of the above,1599,Learning to Rank Retargeted Images,"Yang Chen, Yong-Jin Liu, Yu-Kun Lai",Describing Human Aesthetic Perception by Deeply-learned Attributes from Flickr,"Many aesthetic models in computer vision suffer from two shortcomings: 1) the low descriptiveness and interpretability of those hand-crafted aesthetic criteria (i.e., nonindicative of region-level aesthetics), and 2) the difficulty of engineering aesthetic features adaptively and automatically toward different image sets. To remedy these problems, we develop a deep architecture to learn aesthetically-relevant visual attributes from Flickr1, which are localized by multiple textual attributes in a weakly-supervised setting. More specifically, using a bag-ofwords (BoW) representation of the frequent Flickr image tags, a sparsity-constrained subspace algorithm discovers a compact set of textual attributes (e.g., landscape and sunset) for each image. Then, a weakly-supervised learning algorithm projects the textual attributes at image-level to the highly-responsive image patches at pixel-level. These patches indicate where humans look at appealing regions with respect to each textual attribute, which are employed to learn the visual attributes. Psychological and anatomical studies have shown that humans perceive visual concepts hierarchically. Hence, we normalize these patches and feed them into a five-layer convolutional neural network (CNN) to mimick the hierarchy of human perceiving the visual attributes. We apply the learned deep features on image retargeting, aesthetics ranking, and retrieval. Both subjective and objective experimental results thoroughly demonstrate the competitiveness of our approach.",http://arxiv.org/pdf/1605.07699v1
,,,,,Biomedical Image/Video Analysis,2782,Parsing Images of Overlapping Organisms With Deep Singling-Out Networks,"Victor Yurchenko, Victor Lempitsky",Parsing Images of Overlapping Organisms with Deep Singling-Out Networks,"This work is motivated by the mostly unsolved task of parsing biological images with multiple overlapping articulated model organisms (such as worms or larvae). We present a general approach that separates the two main challenges associated with such data, individual object shape estimation and object groups disentangling. At the core of the approach is a deep feed-forward singling-out network (SON) that is trained to map each local patch to a vectorial descriptor that is sensitive to the characteristics (e.g. shape) of a central object, while being invariant to the variability of all other surrounding elements. Given a SON, a local image patch can be matched to a gallery of isolated elements using their SON-descriptors, thus producing a hypothesis about the shape of the central element in that patch. The image-level optimization based on integer programming can then pick a subset of the hypotheses to explain (parse) the whole image and disentangle groups of organisms.   While sharing many similarities with existing ""analysis-by-synthesis"" approaches, our method avoids the need for stochastic search in the high-dimensional configuration space and numerous rendering operations at test-time. We show that our approach can parse microscopy images of three popular model organisms (the C.Elegans roundworms, the Drosophila larvae, and the E.Coli bacteria) even under significant crowding and overlaps between organisms. We speculate that the overall approach is applicable to a wider class of image parsing problems concerned with crowded articulated objects, for which rendering training images is possible.",http://arxiv.org/pdf/1612.06017v1
,,,,,,3589,Fine-Tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally,"Zongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael Gotway, Jianming Liang",,,
,,,,,Computational Photography,999,Depth From Defocus in the Wild,"Huixuan Tang, Scott Cohen, Brian Price, Stephen Schiller, Kiriakos N. Kutulakos",,,
,,,,,,3231,Matting and Depth Recovery of Thin Structures Using a Focal Stack,"Chao Liu, Srinivasa G. Narasimhan, Artur W. Dubrawski",,,
,,,,,Image Motion & Tracking,155,Robust Interpolation of Correspondences for Large Displacement Optical Flow,"Yinlin Hu, Yunsong Li, Rui Song",EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow,"We propose a novel approach for optical flow estimation , targeted at large displacements with significant oc-clusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries -- two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury.",http://arxiv.org/pdf/1501.02565v2
,,,,,,1608,Large Margin Object Tracking With Circulant Feature Maps,"Mengmeng Wang, Yong Liu, Zeyi Huang",Large Margin Object Tracking with Circulant Feature Maps,"Structured output support vector machine (SVM) based tracking algorithms have shown favorable performance recently. Nonetheless, the time-consuming candidate sampling and complex optimization limit their real-time applications. In this paper, we propose a novel large margin object tracking method which absorbs the strong discriminative ability from structured output SVM and speeds up by the correlation filter algorithm significantly. Secondly, a multimodal target detection technique is proposed to improve the target localization precision and prevent model drift introduced by similar objects or background noise. Thirdly, we exploit the feedback from high-confidence tracking results to avoid the model corruption problem. We implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks (CNNs) based features to validate the strong compatibility of the algorithm. The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per second. The source code and experimental results will be made publicly available.",http://arxiv.org/pdf/1703.05020v2
,,,,,,1725,Minimum Delay Moving Object Detection,"Dong Lao, Ganesh Sundaramoorthi",Quickest Moving Object Detection,"We present a general framework and method for simultaneous detection and segmentation of an object in a video that moves (or comes into view of the camera) at some unknown time in the video. The method is an online approach based on motion segmentation, and it operates under dynamic backgrounds caused by a moving camera or moving nuisances. The goal of the method is to detect and segment the object as soon as it moves. Due to stochastic variability in the video and unreliability of the motion signal, several frames are needed to reliably detect the object. The method is designed to detect and segment with minimum delay subject to a constraint on the false alarm rate. The method is derived as a problem of Quickest Change Detection. Experiments on a dataset show the effectiveness of our method in minimizing detection delay subject to false alarm constraints.",http://arxiv.org/pdf/1605.07369v1
,,,,,,1758,Multi-Task Correlation Particle Filter for Robust Object Tracking,"Tianzhu Zhang, Changsheng Xu, Ming-Hsuan Yang",,,
,,,,,,1996,Attentional Correlation Filter Network for Adaptive Visual Tracking,"Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris, Jin Young Choi",,,
,,,,,,2157,The World of Fast Moving Objects,"Denys Rozumnyi, Jan Kotera, Filip Å roubek, LukÃ¡Å¡ NovotnÃ_, JiÅ™Ã_ Matas",The World of Fast Moving Objects,"The notion of a Fast Moving Object (FMO), i.e. an object that moves over a distance exceeding its size within the exposure time, is introduced. FMOs may, and typically do, rotate with high angular speed. FMOs are very common in sports videos, but are not rare elsewhere. In a single frame, such objects are often barely visible and appear as semi-transparent streaks.   A method for the detection and tracking of FMOs is proposed. The method consists of three distinct algorithms, which form an efficient localization pipeline that operates successfully in a broad range of conditions. We show that it is possible to recover the appearance of the object and its axis of rotation, despite its blurred appearance. The proposed method is evaluated on a new annotated dataset. The results show that existing trackers are inadequate for the problem of FMO localization and a new approach is required. Two applications of localization, temporal super-resolution and highlighting, are presented.",http://arxiv.org/pdf/1611.07889v1
,,,,,,2792,Discriminative Correlation Filter With Channel and Spatial Reliability,"Alan LukeÅ_iÄç, TomÃ¡Å¡ VojÃ_Å™, Luka ÄŒehovin Zajc, JiÅ™Ã_ Matas, Matej Kristan",Discriminative Correlation Filter with Channel and Spatial Reliability,"Short-term tracking is an open and challenging problem for which discriminative correlation filters (DCF) have shown excellent performance. We introduce the channel and spatial reliability concepts to DCF tracking and provide a novel learning algorithm for its efficient and seamless integration in the filter update and the tracking process. The spatial reliability map adjusts the filter support to the part of the object suitable for tracking. This both allows to enlarge the search region and improves tracking of non-rectangular objects. Reliability scores reflect channel-wise quality of the learned filters and are used as feature weighting coefficients in localization. Experimentally, with only two simple standard features, HoGs and Colornames, the novel CSR-DCF method -- DCF with Channel and Spatial Reliability -- achieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF runs in real-time on a CPU.",http://arxiv.org/pdf/1611.08461v1
,,,,,Low- & Mid-Level Vision,411,Learning Deep Binary Descriptor With Multi-Quantization,"Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang Feng, Jie Zhou",,,
,,,,,,1125,One-To-Many Network for Visually Pleasing Compression Artifacts Reduction,"Jun Guo, Hongyang Chao",One-to-Many Network for Visually Pleasing Compression Artifacts Reduction,"We consider the compression artifacts reduction problem, where a compressed image is transformed into an artifact-free image. Recent approaches for this problem typically train a one-to-one mapping using a per-pixel $L_2$ loss between the outputs and the ground-truths. We point out that these approaches used to produce overly smooth results, and PSNR doesn't reflect their real performance. In this paper, we propose a one-to-many network, which measures output quality using a perceptual loss, a naturalness loss, and a JPEG loss. We also avoid grid-like artifacts during deconvolution using a ""shift-and-average"" strategy. Extensive experimental results demonstrate the dramatic visual improvement of our approach over the state of the arts.",http://arxiv.org/pdf/1611.04994v2
,,,,,,1402,Gated Feedback Refinement Network for Dense Image Labeling,"Md Amirul Islam, Mrigank Rochan, Neil D. B. Bruce, Yang Wang",,,
,,,,,,1843,BRISKS: Binary Features for Spherical Images on a Geodesic Grid,"Hao Guan, William A. P. Smith",,,
,,,,,,1936,Superpixels and Polygons Using Simple Non-Iterative Clustering,"Radhakrishna Achanta, Sabine SÃ_sstrunk",,,
,,,,,,2237,Hardware-Efficient Guided Image Filtering for Multi-Label Problem,"Longquan Dai, Mengke Yuan, Zechao Li, Xiaopeng Zhang, Jinhui Tang",,,
,,,,,,2763,Alternating Direction Graph Matching,"D. KhuÃª LÃª-Huu, Nikos Paragios",Matching Connectivity: On the Structure of Graphs with Perfect Matchings,"We introduce the concept of matching connectivity as a notion of connectivity in graph admitting perfect matchings which heavily relies on the structural properties of those matchings. We generalise a result of Robertson, Seymour and Thomas for bipartite graphs with perfect matchings (see [Neil Roberts, Paul D Seymour, and Robin Thomas. Permanents, pfaffian orientations, and even directed curcuits. Annals of Mathematics, 150(2):929-975, 1999]) in order to obtain a concept of alternating paths that turns out to be sufficient for the description of our connectivity parameter. We introduce some basic properties of matching connectivity and prove a Menger-type result for matching n-connected graphs. Furthermore, we show that matching connectivity fills a gap in the investigation of n-extendable graphs and their connectivity properties. To be more precise we show that every n-extendable graph is matching n-connected and for the converse every matching (n+1)-connected graph either is n-extendable, or belongs to a well described class of graphs: the brace h-critical graphs.",http://arxiv.org/pdf/1704.00493v1
,,,,,,3129,Learning Discriminative and Transformation Covariant Local Feature Detectors,"Xu Zhang, Felix X. Yu, Svebor Karaman, Shih-Fu Chang",,,
,,,,,Machine Learning,47,Correlational Gaussian Processes for Cross-Domain Visual Recognition,"Chengjiang Long, Gang Hua",,,
,,,,,,64,DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data,"Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu",DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data,"A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture model's parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of ""inception-score"", a measure which has been found to correlate well with human assessment of generated samples.",http://arxiv.org/pdf/1706.02071v1
,,,,,,586,A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial Problems,"Paul Swoboda, Jan Kuske, Bogdan Savchynskyy",A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial Problems,"We propose a general dual ascent framework for Lagrangean decomposition of combinatorial problems. Although methods of this type have shown their efficiency for a number of problems, so far there was no general algorithm applicable to multiple problem types. In his work, we propose such a general algorithm. It depends on several parameters, which can be used to optimize its performance in each particular setting. We demonstrate efficacy of our method on graph matching and multicut problems, where it outperforms state-of-the-art solvers including those based on subgradient optimization and off-the-shelf linear programming solvers.",http://arxiv.org/pdf/1612.05460v2
,,,,,,160,Oriented Response Networks,"Yanzhao Zhou, Qixiang Ye, Qiang Qiu, Jianbin Jiao",Oriented Response Networks,"Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",http://arxiv.org/pdf/1701.01833v1
,,,,,,527,Missing Modalities Imputation via Cascaded Residual Autoencoder,"Luan Tran, Xiaoming Liu, Jiayu Zhou, Rong Jin",,,
,,,,,,539,Efficient Optimization for Hierarchically-structured Interacting Segments (HINTS),"Hossam Isack, Olga Veksler, Ipek Oguz, Milan Sonka, Yuri Boykov",Efficient optimization for Hierarchically-structured Interacting Segments (HINTS),"We propose an effective optimization algorithm for a general hierarchical segmentation model with geometric interactions between segments. Any given tree can specify a partial order over object labels defining a hierarchy. It is well-established that segment interactions, such as inclusion/exclusion and margin constraints, make the model significantly more discriminant. However, existing optimization methods do not allow full use of such models. Generic -expansion results in weak local minima, while common binary multi-layered formulations lead to non-submodularity, complex high-order potentials, or polar domain unwrapping and shape biases. In practice, applying these methods to arbitrary trees does not work except for simple cases. Our main contribution is an optimization method for the Hierarchically-structured Interacting Segments (HINTS) model with arbitrary trees. Our Path-Moves algorithm is based on multi-label MRF formulation and can be seen as a combination of well-known a-expansion and Ishikawa techniques. We show state-of-the-art biomedical segmentation for many diverse examples of complex trees.",http://arxiv.org/pdf/1703.10530v1
,,,,,,588,A Message Passing Algorithm for the Minimum Cost Multicut Problem,"Paul Swoboda, Bjoern Andres",A Message Passing Algorithm for the Minimum Cost Multicut Problem,"We propose a dual decomposition and linear program relaxation of the NP -hard minimum cost multicut problem. Unlike other polyhedral relaxations of the multicut polytope, it is amenable to efficient optimization by message passing. Like other polyhedral elaxations, it can be tightened efficiently by cutting planes. We define an algorithm that alternates between message passing and efficient separation of cycle- and odd-wheel inequalities. This algorithm is more efficient than state-of-the-art algorithms based on linear programming, including algorithms written in the framework of leading commercial software, as we show in experiments with large instances of the problem from applications in computer vision, biomedical image analysis and data mining.",http://arxiv.org/pdf/1612.05441v2
,,,,,,1027,End-To-End Representation Learning for Correlation Filter Based Tracking,"Jack Valmadre, Luca Bertinetto, JoÃ£o Henriques, Andrea Vedaldi, Philip H. S. Torr",End-to-end representation learning for Correlation Filter based tracking,"The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",http://arxiv.org/pdf/1704.06036v1
,,,,,,1311,Filter Flow Made Practical: Massively Parallel and Lock-Free,"Sathya N. Ravi, Yunyang Xiong, Lopamudra Mukherjee, Vikas Singh",,,
,,,,,,1315,Online Graph Completion: Multivariate Signal Recovery in Computer Vision,"Won Hwa Kim, Mona Jalal, Seongjae Hwang, Sterling C. Johnson, Vikas Singh",,,
,,,,,,1397,Point to Set Similarity Based Deep Feature Learning for Person Re-Identification,"Sanping Zhou, Jinjun Wang, Jiayun Wang, Yihong Gong, Nanning Zheng",,,
,,,,,,1802,Exploiting Saliency for Object Segmentation From Image Level Labels,"Seong Joon Oh, Rodrigo Benenson, Anna Khoreva, Zeynep Akata, Mario Fritz, Bernt Schiele",Exploiting saliency for object segmentation from image level labels,"There have been remarkable improvements in the semantic labelling task in the recent years. However, the state of the art methods rely on large-scale pixel-level annotations. This paper studies the problem of training a pixel-wise semantic labeller network from image-level annotations of the present object classes. Recently, it has been shown that high quality seeds indicating discriminative object regions can be obtained from image-level labels. Without additional information, obtaining the full extent of the object is an inherently ill-posed problem due to co-occurrences. We propose using a saliency model as additional information and hereby exploit prior knowledge on the object extent and image statistics. We show how to combine both information sources in order to recover 80% of the fully supervised performance - which is the new state of the art in weakly supervised training for pixel-wise semantic labelling.",http://arxiv.org/pdf/1701.08261v1
,,,,,,2034,Consensus Maximization With Linear Matrix Inequality Constraints,"Pablo Speciale, Danda Pani Paudel, Martin R. Oswald, Till Kroeger, Luc Van Gool, Marc Pollefeys",,,
,,,,,,2212,Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks,"Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, Thomas Funkhouser",Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks,"Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object edge detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 400K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.",http://arxiv.org/pdf/1612.07429v2
,,,,,,2305,Deep Multimodal Representation Learning From Temporal Data,"Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A. Bernal, Jiebo Luo",Deep Multimodal Representation Learning from Temporal Data,"In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.",http://arxiv.org/pdf/1704.03152v1
,,,,,,2717,All You Need Is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks With Orthonormality and Modulation,"Di Xie, Jiang Xiong, Shiliang Pu",All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation,"Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts.   Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.",http://arxiv.org/pdf/1703.01827v3
,,,,,,3159,Hard Mixtures of Experts for Large Scale Weakly Supervised Vision,"Sam Gross, Marc'Aurelio Ranzato, Arthur Szlam",Hard Mixtures of Experts for Large Scale Weakly Supervised Vision,"Training convolutional networks (CNN's) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice. However, there is still no effective method for training large CNN's that do not fit in the memory of a few GPU cards, or for parallelizing CNN training. In this work we show that a simple hard mixture of experts model can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks. Mixture of experts models are not new (Jacobs et. al. 1991, Collobert et. al. 2003), but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation. We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert. Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model. Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space. We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.",http://arxiv.org/pdf/1704.06363v1
,,,,,,3201,A Reinforcement Learning Approach to the View Planning Problem,"Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, Ser Nam Lim",A Reinforcement Learning Approach to the View Planning Problem,"We present a Reinforcement Learning (RL) solution to the view planning problem (VPP), which generates a sequence of view points that are capable of sensing all accessible area of a given object represented as a 3D model. In doing so, the goal is to minimize the number of view points, making the VPP a class of set covering optimization problem (SCOP). The SCOP is NP-hard, and the inapproximability results tell us that the greedy algorithm provides the best approximation that runs in polynomial time. In order to find a solution that is better than the greedy algorithm, (i) we introduce a novel score function by exploiting the geometry of the 3D model, (ii) we model an intuitive human approach to VPP using this score function, and (iii) we cast VPP as a Markovian Decision Process (MDP), and solve the MDP in RL framework using well-known RL algorithms. In particular, we use SARSA, Watkins-Q and TD with function approximation to solve the MDP. We compare the results of our method with the baseline greedy algorithm in an extensive set of test objects, and show that we can out-perform the baseline in almost all cases.",http://arxiv.org/pdf/1610.06204v2
,,,,,,3361,Zero-Shot Classification With Discriminative Semantic Representation Learning,"Meng Ye, Yuhong Guo",,,
,,,,,Object Recognition & Scene Understanding,191,"Automatic Discovery, Association Estimation and Learning of Semantic Attributes for a Thousand Categories","Ziad Al-Halah, Rainer Stiefelhagen","Automatic Discovery, Association Estimation and Learning of Semantic Attributes for a Thousand Categories","Attribute-based recognition models, due to their impressive performance and their ability to generalize well on novel categories, have been widely adopted for many computer vision applications. However, usually both the attribute vocabulary and the class-attribute associations have to be provided manually by domain experts or large number of annotators. This is very costly and not necessarily optimal regarding recognition performance, and most importantly, it limits the applicability of attribute-based models to large scale data sets. To tackle this problem, we propose an end-to-end unsupervised attribute learning approach. We utilize online text corpora to automatically discover a salient and discriminative vocabulary that correlates well with the human concept of semantic attributes. Moreover, we propose a deep convolutional model to optimize class-attribute associations with a linguistic prior that accounts for noise and missing data in text. In a thorough evaluation on ImageNet, we demonstrate that our model is able to efficiently discover and learn semantic attributes at a large scale. Furthermore, we demonstrate that our model outperforms the state-of-the-art in zero-shot learning on three data sets: ImageNet, Animals with Attributes and aPascal/aYahoo. Finally, we enable attribute-based learning on ImageNet and will share the attributes and associations for future research.",http://arxiv.org/pdf/1704.03607v1
,,,,,,194,Scene Parsing Through ADE20K Dataset,"Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba",Semantic Understanding of Scenes through the ADE20K Dataset,"Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects.",http://arxiv.org/pdf/1608.05442v1
,,,,,,303,Weakly Supervised Cascaded Convolutional Networks,"Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, Luc Van Gool",Weakly Supervised Cascaded Convolutional Networks,"Object detection is a challenging task in visual understanding domain, and even more so if the supervision is to be weak. Recently, few efforts to handle the task without expensive human annotations is established by promising deep neural network. A new architecture of cascaded networks is proposed to learn a convolutional neural network (CNN) under such conditions. We introduce two such architectures, with either two cascade stages or three which are trained in an end-to-end pipeline. The first stage of both architectures extracts best candidate of class specific region proposals by training a fully convolutional network. In the case of the three stage architecture, the middle stage provides object segmentation, using the output of the activation maps of first stage. The final stage of both architectures is a part of a convolutional neural network that performs multiple instance learning on proposals extracted in the previous stage(s). Our experiments on the PASCAL VOC 2007, 2010, 2012 and large scale object datasets, ILSVRC 2013, 2014 datasets show improvements in the areas of weakly-supervised object detection, classification and localization.",http://arxiv.org/pdf/1611.08258v1
,,,,,,530,Discretely Coding Semantic Rank Orders for Supervised Image Hashing,"Li Liu, Ling Shao, Fumin Shen, Mengyang Yu",,,
,,,,,,692,Joint Geometrical and Statistical Alignment for Visual Domain Adaptation,"Jing Zhang, Wanqing Li, Philip Ogunbona",Joint Geometrical and Statistical Alignment for Visual Domain Adaptation,"This paper presents a novel unsupervised domain adaptation method for cross-domain visual recognition. We propose a unified framework that reduces the shift between domains both statistically and geometrically, referred to as Joint Geometrical and Statistical Alignment (JGSA). Specifically, we learn two coupled projections that project the source domain and target domain data into low dimensional subspaces where the geometrical shift and distribution shift are reduced simultaneously. The objective function can be solved efficiently in a closed form. Extensive experiments have verified that the proposed method significantly outperforms several state-of-the-art domain adaptation methods on a synthetic dataset and three different real world cross-domain visual recognition tasks.",http://arxiv.org/pdf/1705.05498v1
,,,,,,705,Weakly Supervised Dense Video Captioning,"Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue",Weakly Supervised Dense Video Captioning,"This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.",http://arxiv.org/pdf/1704.01502v1
,,,,,,707,RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,"Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation,"Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",http://arxiv.org/pdf/1611.06612v3
,,,,,,725,"Semantic Segmentation via Structured Patch Prediction, Context CRF and Guidance CRF","Falong Shen, Rui Gan, Shuicheng Yan, Gang Zeng",,,
,,,,,,729,Person Search With Natural Language Description,"Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, Xiaogang Wang",Person Search with Natural Language Description,"Searching persons in large-scale image databases with the query of natural language description has important applications in video surveillance. Existing methods mainly focused on searching persons with image-based or attribute-based queries, which have major limitations for a practical usage. In this paper, we study the problem of person search with natural language description. Given the textual description of a person, the algorithm of the person search is required to rank all the samples in the person database then retrieve the most relevant sample corresponding to the queried description. Since there is no person dataset or benchmark with textual description available, we collect a large-scale person description dataset with detailed natural language annotations and person samples from various sources, termed as CUHK Person Description Dataset (CUHK-PEDES). A wide range of possible models and baselines have been evaluated and compared on the person search benchmark. An Recurrent Neural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to establish the state-of-the art performance on person search.",http://arxiv.org/pdf/1702.05729v2
,,,,,,1026,Weakly Supervised Affordance Detection,"Johann Sawatzky, Abhilash Srikantha, Juergen Gall",Weakly Supervised Object Boundaries,"State-of-the-art learning based boundary detection methods require extensive training data. Since labelling object boundaries is one of the most expensive types of annotations, there is a need to relax the requirement to carefully annotate images to make both the training more affordable and to extend the amount of training data. In this paper we propose a technique to generate weakly supervised annotations and show that bounding box annotations alone suffice to reach high-quality object boundaries without using any object-specific boundary annotations. With the proposed weak supervision techniques we achieve the top performance on the object boundary detection task, outperforming by a large margin the current fully supervised state-of-the-art methods.",http://arxiv.org/pdf/1511.07803v1
,,,,,,1224,Zero-Shot Recognition Using Dual Visual-Semantic Mapping Paths,"Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, Yueting Zhuang",Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths,"Zero-shot recognition aims to accurately recognize objects of unseen classes by using a shared visual-semantic mapping between the image feature space and the semantic embedding space. This mapping is learned on training data of seen classes and is expected to have transfer ability to unseen classes. In this paper, we tackle this problem by exploiting the intrinsic relationship between the semantic space manifold and the transfer ability of visual-semantic mapping. We formalize their connection and cast zero-shot recognition as a joint optimization problem. Motivated by this, we propose a novel framework for zero-shot recognition, which contains dual visual-semantic mapping paths. Our analysis shows this framework can not only apply prior semantic knowledge to infer underlying semantic manifold in the image feature space, but also generate optimized semantic embedding space, which can enhance the transfer ability of the visual-semantic mapping to unseen classes. The proposed method is evaluated for zero-shot recognition on four benchmark datasets, achieving outstanding results.",http://arxiv.org/pdf/1703.05002v2
,,,,,,1773,Neural Aggregation Network for Video Face Recognition,"Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, Gang Hua",Input Aggregated Network for Face Video Representation,"Recently, deep neural network has shown promising performance in face image recognition. The inputs of most networks are face images, and there is hardly any work reported in literature on network with face videos as input. To sufficiently discover the useful information contained in face videos, we present a novel network architecture called input aggregated network which is able to learn fixed-length representations for variable-length face videos. To accomplish this goal, an aggregation unit is designed to model a face video with various frames as a point on a Riemannian manifold, and the mapping unit aims at mapping the point into high-dimensional space where face videos belonging to the same subject are close-by and others are distant. These two units together with the frame representation unit build an end-to-end learning system which can learn representations of face videos for the specific tasks. Experiments on two public face video datasets demonstrate the effectiveness of the proposed network.",http://arxiv.org/pdf/1603.06655v1
,,,,,,2414,Relationship Proposal Networks,"Ji Zhang, Mohamed Elhoseiny, Scott Cohen, Walter Chang, Ahmed Elgammal",Mining Essential Relationships under Multiplex Networks,"In big data times, massive datasets often carry different relationships among the same group of nodes, analyzing on these heterogeneous relationships may give us a window to peek the essential relationships among nodes. In this paper, first of all we propose a new metric ""similarity rate"" in order to capture the changing rate of similarities between node-pairs though all networks; secondly, we try to use this new metric to uncover essential relationships between node-pairs which essential relationships are often hidden and hard to get. From experiments study of Indonesian Terrorists dataset, this new metric similarity rate function well for giving us a way to uncover essential relationships from lots of appearances.",http://arxiv.org/pdf/1511.09134v1
,,,,,,2518,Learning Object Interactions and Descriptions for Semantic Image Segmentation,"Guangrun Wang, Ping Luo, Liang Lin, Xiaogang Wang",,,
,,,,,,2562,RON: Reverse Connection With Objectness Prior Networks for Object Detection,"Tao Kong, Fuchun Sun, Anbang Yao, Huaping Liu, Ming Lu, Yurong Chen",,,
,,,,,,2570,Weakly-Supervised Visual Grounding of Phrases With Linguistic Structures,"Fanyi Xiao, Leonid Sigal, Yong Jae Lee",Weakly-supervised Visual Grounding of Phrases with Linguistic Structures,"We propose a weakly-supervised approach that takes image-sentence pairs as input and learns to visually ground (i.e., localize) arbitrary linguistic phrases, in the form of spatial attention masks. Specifically, the model is trained with images and their associated image-level captions, without any explicit region-to-phrase correspondence annotations. To this end, we introduce an end-to-end model which learns visual groundings of phrases with two types of carefully designed loss functions. In addition to the standard discriminative loss, which enforces that attended image regions and phrases are consistently encoded, we propose a novel structural loss which makes use of the parse tree structures induced by the sentences. In particular, we ensure complementarity among the attention masks that correspond to sibling noun phrases, and compositionality of attention masks among the children and parent phrases, as defined by the sentence parse tree. We validate the effectiveness of our approach on the Microsoft COCO and Visual Genome datasets.",http://arxiv.org/pdf/1705.01371v1
,,,,,,2967,Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects,"Ting Yao, Yingwei Pan, Yehao Li, Tao Mei",,,
,,,,,,2972,Beyond Instance-Level Image Retrieval: Leveraging Captions to Learn a Global Visual Representation for Semantic Retrieval,"Albert Gordo, Diane Larlus",,,
,,,,,,3088,MuCaLe-Net: Multi Categorical-Level Networks to Generate More Discriminating Features,"Youssef Tamaazousti, HervÃ© Le Borgne, CÃ©line Hudelot",,,
,,,,,,3339,Zero Shot Learning via Multi-Scale Manifold Regularization,"Shay Deutsch, Soheil Kolouri, Kyungnam Kim, Yuri Owechko, Stefano Soatto",,,
,,,,,Theory,1183,Deeply Supervised Salient Object Detection With Short Connections,"Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen Tu, Philip H. S. Torr",Deeply supervised salient object detection with short connections,"Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on salience detection is not obvious. In this paper, we propose a new method for saliency detection by introducing short connections to the skip-layer structures within the HED architecture. Our framework provides rich multi-scale feature maps at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.15 seconds per image), effectiveness, and simplicity over the existing algorithms.",http://arxiv.org/pdf/1611.04849v2
,,,,,,2016,A Matrix Splitting Method for Composite Function Minimization,"Ganzhao Yuan, Wei-Shi Zheng, Bernard Ghanem",A Matrix Splitting Method for Composite Function Minimization,"Composite function minimization captures a wide spectrum of applications in both computer vision and machine learning. It includes bound constrained optimization and cardinality regularized optimization as special cases. This paper proposes and analyzes a new Matrix Splitting Method (MSM) for minimizing composite functions. It can be viewed as a generalization of the classical Gauss-Seidel method and the Successive Over-Relaxation method for solving linear systems in the literature. Incorporating a new Gaussian elimination procedure, the matrix splitting method achieves state-of-the-art performance. For convex problems, we establish the global convergence, convergence rate, and iteration complexity of MSM, while for non-convex problems, we prove its global convergence. Finally, we validate the performance of our matrix splitting method on two particular applications: nonnegative matrix factorization and cardinality regularized sparse coding. Extensive experiments show that our method outperforms existing composite function minimization techniques in term of both efficiency and efficacy.",http://arxiv.org/pdf/1612.02317v1
,,,,,Video Analytics,81,One-Shot Video Object Segmentation,"Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-TaixÃ©, Daniel Cremers, Luc Van Gool",One-Shot Video Object Segmentation,"This paper tackles the task of semi-supervised video object segmentation, i.e., the separation of an object from the background in a video, given the mask of the first frame. We present One-Shot Video Object Segmentation (OSVOS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one-shot). Although all frames are processed independently, the results are temporally coherent and stable. We perform experiments on two annotated video segmentation databases, which show that OSVOS is fast and improves the state of the art by a significant margin (79.8% vs 68.0%).",http://arxiv.org/pdf/1611.05198v4
,,,,,,1506,Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation,"Jiaxin Chen, Yunhong Wang, Jie Qin, Li Liu, Ling Shao",,,
,,,,,,1808,SPFTN: A Self-Paced Fine-Tuning Network for Segmenting Objects in Weakly Labelled Videos,"Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, Junwei Han",,,
"Tuesday, July 25, 2017",0830â€“1000,Kamehameha III,20,Spotlight 4-1A,Machine Learning 4,1881,Hidden Layers in Perceptual Learning,"Gad Cohen, Daphna Weinshall",,,
,,,,,,398,Few-Shot Object Recognition From Machine-Labeled Web Images,"Zhongwen Xu, Linchao Zhu, Yi Yang",Few-Shot Object Recognition from Machine-Labeled Web Images,"With the tremendous advances of Convolutional Neural Networks (ConvNets) on object recognition, we can now obtain reliable enough machine-labeled annotations easily by predictions from off-the-shelf ConvNets. In this work, we present an abstraction memory based framework for few-shot learning, building upon machine-labeled image annotations. Our method takes some large-scale machine-annotated datasets (e.g., OpenImages) as an external memory bank. In the external memory bank, the information is stored in the memory slots with the form of key-value, where image feature is regarded as key and label embedding serves as value. When queried by the few-shot examples, our model selects visually similar data from the external memory bank, and writes the useful information obtained from related external data into another memory bank, i.e., abstraction memory. Long Short-Term Memory (LSTM) controllers and attention mechanisms are utilized to guarantee the data written to the abstraction memory is correlated to the query example. The abstraction memory concentrates information from the external memory bank, so that it makes the few-shot recognition effective. In the experiments, we firstly confirm that our model can learn to conduct few-shot object recognition on clean human-labeled data from ImageNet dataset. Then, we demonstrate that with our model, machine-labeled image annotations are very effective and abundant resources to perform object recognition on novel categories. Experimental results show that our proposed model with machine-labeled annotations achieves great performance, only with a gap of 1% between of the one with human-labeled annotations.",http://arxiv.org/pdf/1612.06152v1
,,,,,,1413,Hallucinating Very Low-Resolution Unaligned and Noisy Face Images by Transformative Discriminative Autoencoders,"Xin Yu, Fatih Porikli",,,
,,,,,,2059,Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension,"Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, Hannaneh Hajishirzi",,,
,,,,,,2074,Deep Hashing Network for Unsupervised Domain Adaptation,"Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan",,,
,,,,,,2384,Generalized Deep Image to Image Regression,"Venkataraman Santhanam, Vlad I. Morariu, Larry S. Davis",Half-CNN: A General Framework for Whole-Image Regression,"The Convolutional Neural Network (CNN) has achieved great success in image classification. The classification model can also be utilized at image or patch level for many other applications, such as object detection and segmentation. In this paper, we propose a whole-image CNN regression model, by removing the full connection layer and training the network with continuous feature maps. This is a generic regression framework that fits many applications. We demonstrate this method through two tasks: simultaneous face detection & segmentation, and scene saliency prediction. The result is comparable with other models in the respective fields, using only a small scale network. Since the regression model is trained on corresponding image / feature map pairs, there are no requirements on uniform input size as opposed to the classification model. Our framework avoids classifier design, a process that may introduce too much manual intervention in model development. Yet, it is highly correlated to the classification network and offers some in-deep review of CNN structures.",http://arxiv.org/pdf/1412.6885v1
,,,,,,2557,Deep Learning With Low Precision by Half-Wave Gaussian Quantization,"Zhaowei Cai, Xiaodong He, Jian Sun, Nuno Vasconcelos",Deep Learning with Low Precision by Half-wave Gaussian Quantization,"The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.",http://arxiv.org/pdf/1702.00953v1
,,,,,,2922,Creativity: Generating Diverse Questions Using Variational Autoencoders,"Unnat Jain, Ziyu Zhang, Alexander G. Schwing",Creativity: Generating Diverse Questions using Variational Autoencoders,"Generating diverse questions for given images is an important task for computational education, entertainment and AI assistants. Different from many conventional prediction techniques is the need for algorithms to generate a diverse set of plausible questions, which we refer to as ""creativity"". In this paper we propose a creative algorithm for visual question generation which combines the advantages of variational autoencoders with long short-term memory networks. We demonstrate that our framework is able to generate a large set of varying questions given a single input image.",http://arxiv.org/pdf/1704.03493v1
,,,,Oral 4-1A,,2123,Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs,"Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele RodolÃ , Jan Svoboda, Michael M. Bronstein",Geometric deep learning on graphs and manifolds using mixture model CNNs,"Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.",http://arxiv.org/pdf/1611.08402v3
,,,,,,2230,Full Resolution Image Compression With Recurrent Neural Networks,"George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell",Full Resolution Image Compression with Recurrent Neural Networks,"This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study ""one-shot"" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.",http://arxiv.org/pdf/1608.05148v1
,,,,,,2340,Neural Face Editing With Intrinsic Image Disentangling,"Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, Dimitris Samaras",Neural Face Editing with Intrinsic Image Disentangling,"Traditional face editing methods often require a number of sophisticated and task specific algorithms to be applied one after the other --- a process that is tedious, fragile, and computationally intensive. In this paper, we propose an end-to-end generative adversarial network that infers a face-specific disentangled representation of intrinsic face properties, including shape (i.e. normals), albedo, and lighting, and an alpha matte. We show that this network can be trained on ""in-the-wild"" images by incorporating an in-network physically-based image formation module and appropriate loss functions. Our disentangling latent representation allows for semantically relevant edits, where one aspect of facial appearance can be manipulated while keeping orthogonal properties fixed, and we demonstrate its use for a number of facial editing applications.",http://arxiv.org/pdf/1704.04131v1
,,,,,,2694,"Ubernet: Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory",Iasonas Kokkinos,,,
"Tuesday, July 25, 2017",0830â€“1000,KalÄÅkaua Ballroom,21,Spotlight 4-1B,Analyzing Humans with 3D Vision,25,3D Face Morphable Models â€œIn-The-Wildâ€ù,"James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis, Stefanos Zafeiriou",,,
,,,,,,517,KillingFusion: Non-Rigid 3D Reconstruction Without Correspondences,"Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, Slobodan Ilic",,,
,,,,,,1708,"Detailed, Accurate, Human Shape Estimation From Clothed 3D Scan Sequences","Chao Zhang, Sergi Pujades, Michael J. Black, Gerard Pons-Moll","Detailed, accurate, human shape estimation from clothed 3D scan sequences","We address the problem of estimating human pose and body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited body models produce smooth shapes lacking personalized details. We contribute a new approach to recover a personalized shape of the person. The estimated shape deviates from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available BUFF, a new 4D dataset that enables quantitative evaluation (http://buff.is.tue.mpg.de). Our method outperforms the state of the art in both pose estimation and shape estimation, qualitatively and quantitatively.",http://arxiv.org/pdf/1703.04454v2
,,,,,,1940,POSEidon: Face-From-Depth for Driver Pose Estimation,"Guido Borghi, Marco Venturelli, Roberto Vezzani, Rita Cucchiara",POSEidon: Face-from-Depth for Driver Pose Estimation,"Fast and accurate upper-body and head pose estimation is a key task for automatic monitoring of driver attention, a challenging context characterized by severe illumination changes, occlusions and extreme poses. In this work, we present a new deep learning framework for head localization and pose estimation on depth images. The core of the proposal is a regression neural network, called POSEidon, which is composed of three independent convolutional nets followed by a fusion layer, specially conceived for understanding the pose by depth. In addition, to recover the intrinsic value of face appearance for understanding head position and orientation, we propose a new Face-from-Depth approach for learning image faces from depth. Results in face reconstruction are qualitatively impressive. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Results show that our method overcomes all recent state-of-art works, running in real time at more than 30 frames per second.",http://arxiv.org/pdf/1611.10195v2
,,,,,,2001,Human Shape From Silhouettes Using Generative HKS Descriptors and Cross-Modal Neural Networks,"Endri Dibra, Himanshu Jain, Cengiz Ã–ztireli, Remo Ziegler, Markus Gross",,,
,,,,,,2699,Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace,"Weilong Peng, Zhiyong Feng, Chao Xu, Yong Su",,,
,,,,,,2825,3D Menagerie: Modeling the 3D Shape and Pose of Animals,"Silvia Zuffi, Angjoo Kanazawa, David W. Jacobs, Michael J. Black",3D Menagerie: Modeling the 3D shape and pose of animals,"There has been significant work on learning realistic, articulated, 3D models of the human body. In contrast, there are few such models of animals, despite many applications. The main challenge is that animals are much less cooperative than humans. The best human body models are learned from thousands of 3D scans of people in specific poses, which is infeasible with live animals. Consequently, we learn our model from a small set of 3D scans of toy figurines in arbitrary poses. We employ a novel part-based shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and refine the registrations and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the registration to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and fit to data. We demonstrate generalization by fitting it to images of real animals including species not seen in training.",http://arxiv.org/pdf/1611.07700v2
,,,,,,739,iCaRL: Incremental Classifier and Representation Learning,"Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph H. Lampert",iCaRL: Incremental Classifier and Representation Learning,"A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.",http://arxiv.org/pdf/1611.07725v2
,,,,Oral 4-1B,,263,Recurrent 3D Pose Sequence Machines,"Mude Lin, Liang Lin, Xiaodan Liang, Keze Wang, Hui Cheng",,,
,,,,,,454,Learning Detailed Face Reconstruction From a Single Image,"Elad Richardson, Matan Sela, Roy Or-El, Ron Kimmel",Learning Detailed Face Reconstruction from a Single Image,"Reconstructing the detailed geometric structure of a face from a given image is a key to many computer vision and graphics applications, such as motion capture and reenactment. The reconstruction task is challenging as human faces vary extensively when considering expressions, poses, textures, and intrinsic geometries. While many approaches tackle this complexity by using additional data to reconstruct the face of a single subject, extracting facial surface from a single image remains a difficult problem. As a result, single-image based methods can usually provide only a rough estimate of the facial geometry. In contrast, we propose to leverage the power of convolutional neural networks to produce a highly detailed face reconstruction from a single image. For this purpose, we introduce an end-to-end CNN framework which derives the shape in a coarse-to-fine fashion. The proposed architecture is composed of two main blocks, a network that recovers the coarse facial geometry (CoarseNet), followed by a CNN that refines the facial features of that geometry (FineNet). The proposed networks are connected by a novel layer which renders a depth image given a mesh in 3D. Unlike object recognition and detection problems, there are no suitable datasets for training CNNs to perform face geometry reconstruction. Therefore, our training regime begins with a supervised phase, based on synthetic images, followed by an unsupervised phase that uses only unconstrained facial images. The accuracy and robustness of the proposed model is demonstrated by both qualitative and quantitative evaluation tests.",http://arxiv.org/pdf/1611.05053v2
,,,,,,1719,Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos,"Jie Song, Limin Wang, Luc Van Gool, Otmar Hilliges",Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos,"Deep ConvNets have been shown to be effective for the task of human pose estimation from single images. However, several challenging issues arise in the video-based case such as self-occlusion, motion blur, and uncommon poses with few or no examples in training data sets. Temporal information can provide additional cues about the location of body joints and help to alleviate these issues. In this paper, we propose a deep structured model to estimate a sequence of human poses in unconstrained videos. This model can be efficiently trained in an end-to-end manner and is capable of representing appearance of body joints and their spatio-temporal relationships simultaneously. Domain knowledge about the human body is explicitly incorporated into the network providing effective priors to regularize the skeletal structure and to enforce temporal consistency. The proposed end-to-end architecture is evaluated on two widely used benchmarks (Penn Action dataset and JHMDB dataset) for video-based pose estimation. Our approach significantly outperforms the existing state-of-the-art methods.",http://arxiv.org/pdf/1703.10898v1
,,,,,,2752,Dynamic FAUST: Registering Human Bodies in Motion,"Federica Bogo, Javier Romero, Gerard Pons-Moll, Michael J. Black",,,
"Tuesday, July 25, 2017",1000â€“1200,Kamehameha I,22,Poster 4-1,3D Computer Vision,142,Semantically Coherent Co-Segmentation and Reconstruction of Dynamic Scenes,"Armin Mustafa, Adrian Hilton",,,
,,,,,,2003,On the Two-View Geometry of Unsynchronized Cameras,"Cenek Albl, Zuzana Kukelova, Andrew Fitzgibbon, Jan Heller, Matej Smid, Tomas Pajdla",On the Two-View Geometry of Unsynchronized Cameras,"We present new methods for simultaneously estimating camera geometry and time shift from video sequences from multiple unsynchronized cameras. Algorithms for simultaneous computation of a fundamental matrix or a homography with unknown time shift between images are developed. Our methods use minimal correspondence sets (eight for fundamental matrix and four and a half for homography) and therefore are suitable for robust estimation using RANSAC. Furthermore, we present an iterative algorithm that extends the applicability on sequences which are significantly unsynchronized, finding the correct time shift up to several seconds. We evaluated the methods on synthetic and wide range of real world datasets and the results show a broad applicability to the problem of camera synchronization.",http://arxiv.org/pdf/1704.06843v1
,,,,,,2007,Using Locally Corresponding CAD Models for Dense 3D Reconstructions From a Single Image,"Chen Kong, Chen-Hsuan Lin, Simon Lucey",,,
,,,,,,2042,Convex Global 3D Registration With Lagrangian Duality,"Jesus Briales, Javier Gonzalez-Jimenez",,,
,,,,,,2080,DeMoN: Depth and Motion Network for Learning Monocular Stereo,"Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox",DeMoN: Depth and Motion Network for Learning Monocular Stereo,"In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.",http://arxiv.org/pdf/1612.02401v2
,,,,,,3303,3D Bounding Box Estimation Using Deep Learning and Geometry,"Arsalan Mousavian, Dragomir Anguelov, John Flynn, Jana KoÅ¡eckÃ¡",3D Bounding Box Estimation Using Deep Learning and Geometry,"We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset.",http://arxiv.org/pdf/1612.00496v2
,,,,,,3852,A Dataset for Benchmarking Image-Based Localization,"Xun Sun, Yuanfan Xie, Pei Luo, Liang Wang",'Part'ly first among equals: Semantic part-based benchmarking for state-of-the-art object recognition systems,"An examination of object recognition challenge leaderboards (ILSVRC, PASCAL-VOC) reveals that the top-performing classifiers typically exhibit small differences amongst themselves in terms of error rate/mAP. To better differentiate the top performers, additional criteria are required. Moreover, the (test) images, on which the performance scores are based, predominantly contain fully visible objects. Therefore, `harder' test images, mimicking the challenging conditions (e.g. occlusion) in which humans routinely recognize objects, need to be utilized for benchmarking. To address the concerns mentioned above, we make two contributions. First, we systematically vary the level of local object-part content, global detail and spatial context in images from PASCAL VOC 2010 to create a new benchmarking dataset dubbed PPSS-12. Second, we propose an object-part based benchmarking procedure which quantifies classifiers' robustness to a range of visibility and contextual settings. The benchmarking procedure relies on a semantic similarity measure that naturally addresses potential semantic granularity differences between the category labels in training and test datasets, thus eliminating manual mapping. We use our procedure on the PPSS-12 dataset to benchmark top-performing classifiers trained on the ILSVRC-2012 dataset. Our results show that the proposed benchmarking procedure enables additional differentiation among state-of-the-art object classifiers in terms of their ability to handle missing content and insufficient object detail. Given this capability for additional differentiation, our approach can potentially supplement existing benchmarking procedures used in object recognition challenge leaderboards.",http://arxiv.org/pdf/1611.07703v2
,,,,,Analyzing Humans in Images,186,Asynchronous Temporal Fields for Action Recognition,"Gunnar A. Sigurdsson, Santosh Divvala, Ali Farhadi, Abhinav Gupta",Asynchronous Temporal Fields for Action Recognition,"Actions are more than just movements and trajectories: we cook to eat and we hold a cup to drink from it. A thorough understanding of videos requires going beyond appearance modeling and necessitates reasoning about the sequence of activities, as well as the higher-level constructs such as intentions. But how do we model and reason about these? We propose a fully-connected temporal CRF model for reasoning over various aspects of activities that includes objects, actions, and intentions, where the potentials are predicted by a deep network. End-to-end training of such structured models is a challenging endeavor: For inference and learning we need to construct mini-batches consisting of whole videos, leading to mini-batches with only a few videos. This causes high-correlation between data points leading to breakdown of the backprop algorithm. To address this challenge, we present an asynchronous variational inference method that allows efficient end-to-end training. Our method achieves a classification mAP of 21.9% on the Charades benchmark, outperforming the state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal localization.",http://arxiv.org/pdf/1612.06371v1
,,,,,,489,Sequential Person Recognition in Photo Albums With a Recurrent Network,"Yao Li, Guosheng Lin, Bohan Zhuang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel",Sequential Person Recognition in Photo Albums with a Recurrent Network,"Recognizing the identities of people in everyday photos is still a very challenging problem for machine vision, due to non-frontal faces, changes in clothing, location, lighting and similar. Recent studies have shown that rich relational information between people in the same photo can help in recognizing their identities. In this work, we propose to model the relational information between people as a sequence prediction task. At the core of our work is a novel recurrent network architecture, in which relational information between instances' labels and appearance are modeled jointly. In addition to relational cues, scene context is incorporated in our sequence prediction model with no additional cost. In this sense, our approach is a unified framework for modeling both contextual cues and visual appearance of person instances. Our model is trained end-to-end with a sequence of annotated instances in a photo as inputs, and a sequence of corresponding labels as targets. We demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly released People In Photo Albums (PIPA) dataset.",http://arxiv.org/pdf/1611.09967v1
,,,,,,678,Multi-Context Attention for Human Pose Estimation,"Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, Xiaogang Wang",Multi-Context Attention for Human Pose Estimation,"In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on the detailed description for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic-consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive fields, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts.",http://arxiv.org/pdf/1702.07432v1
,,,,,,737,3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation From Single Depth Images,"Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann",,,
,,,,,,909,Lifting From the Deep: Convolutional 3D Pose Estimation From a Single Image,"Denis Tome, Chris Russell, Lourdes Agapito",Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image,"We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state- of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.",http://arxiv.org/pdf/1701.00295v3
,,,,,,1254,AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos,"Amlan Kar, Nishant Rai, Karan Sikka, Gaurav Sharma",AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos,"We propose a novel method for temporally pooling frames in a video for the task of human action recognition. The method is motivated by the observation that there are only a small number of frames which, together, contain sufficient information to discriminate an action class present in a video, from the rest. The proposed method learns to pool such discriminative and informative frames, while discarding a majority of the non-informative frames in a single temporal scan of the video. Our algorithm does so by continuously predicting the discriminative importance of each video frame and subsequently pooling them in a deep learning framework. We show the effectiveness of our proposed pooling method on standard benchmarks where it consistently improves on baseline pooling methods, with both RGB and optical flow based Convolutional networks. Further, in combination with complementary video representations, we show results that are competitive with respect to the state-of-the-art results on two challenging and publicly available benchmark datasets.",http://arxiv.org/pdf/1611.08240v3
,,,,,,1259,Deep Structured Learning for Facial Action Unit Intensity Estimation,"Robert Walecki, Ognjen (Oggi) Rudovic, Vladimir Pavlovic, BjÃ¶ern Schuller, Maja Pantic",Deep Structured Learning for Facial Action Unit Intensity Estimation,"We consider the task of automated estimation of facial expression intensity. This involves estimation of multiple output variables (facial action units --- AUs) that are structurally dependent. Their structure arises from statistically induced co-occurrence patterns of AU intensity levels. Modeling this structure is critical for improving the estimation performance; however, this performance is bounded by the quality of the input features extracted from face images. The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for $ordinal$ structure in output variables and their $non$-$linear$ dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm. We demonstrate the effectiveness of our approach on the task of AU intensity estimation on two benchmark datasets. We show that joint learning of the deep features and the target output structure results in significant performance gains compared to existing deep structured models for analysis of facial expressions.",http://arxiv.org/pdf/1704.04481v1
,,,,,,1281,"Simultaneous Facial Landmark Detection, Pose and Deformation Estimation Under Facial Occlusion","Yue Wu, Chao Gou, Qiang Ji",,,
,,,,,,1337,Self-Supervised Video Representation Learning With Odd-One-Out Networks,"Basura Fernando, Hakan Bilen, Efstratios Gavves, Stephen Gould",Self-Supervised Video Representation Learning With Odd-One-Out Networks,"We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called ""odd-one-out learning"". In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition.   On action classification, our method obtains 60.3\% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.",http://arxiv.org/pdf/1611.06646v4
,,,,,,2209,Robust Joint and Individual Variance Explained,"Christos Sagonas, Yannis Panagakis, Alina Leidinger, Stefanos Zafeiriou",Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression,"Echo state network (ESN) is viewed as a temporal non-orthogonal expansion with pseudo-random parameters. Such expansions naturally give rise to regressors of various relevance to a teacher output. We illustrate that often only a certain amount of the generated echo-regressors effectively explain the variance of the teacher output and also that sole local regularization is not able to provide in-depth information concerning the importance of the generated regressors. The importance is therefore determined by a joint calculation of the individual variance contributions and Bayesian relevance using locally regularized orthogonal forward regression (LROFR) algorithm. This information can be advantageously used in a variety of ways for an in-depth analysis of an ESN structure and its state-space parameters in relation to the unknown dynamics of the underlying problem. We present locally regularized linear readout built using LROFR. The readout may have a different dimensionality than an ESN model itself, and besides improving robustness and accuracy of an ESN it relates the echo-regressors to different features of the training data and may determine what type of an additional readout is suitable for a task at hand. Moreover, as flexibility of the linear readout has limitations and might sometimes be insufficient for certain tasks, we also present a radial basis function (RBF) readout built using LROFR. It is a flexible and parsimonious readout with excellent generalization abilities and is a viable alternative to readouts based on a feed-forward neural network (FFNN) or an RBF net built using relevance vector machine (RVM).",http://arxiv.org/pdf/1110.4304v3
,,,,,,2377,Discriminative Covariance Oriented Representation Learning for Face Recognition With Image Sets,"Wen Wang, Ruiping Wang, Shiguang Shan, Xilin Chen",SVM-based Multiview Face Recognition by Generalization of Discriminant Analysis,"Identity verification of authentic persons by their multiview faces is a real valued problem in machine vision. Multiview faces are having difficulties due to non-linear representation in the feature space. This paper illustrates the usability of the generalization of LDA in the form of canonical covariate for face recognition to multiview faces. In the proposed work, the Gabor filter bank is used to extract facial features that characterized by spatial frequency, spatial locality and orientation. Gabor face representation captures substantial amount of variations of the face instances that often occurs due to illumination, pose and facial expression changes. Convolution of Gabor filter bank to face images of rotated profile views produce Gabor faces with high dimensional features vectors. Canonical covariate is then used to Gabor faces to reduce the high dimensional feature spaces into low dimensional subspaces. Finally, support vector machines are trained with canonical sub-spaces that contain reduced set of features and perform recognition task. The proposed system is evaluated with UMIST face database. The experiment results demonstrate the efficiency and robustness of the proposed system with high recognition rates.",http://arxiv.org/pdf/1001.4140v1
,,,,,,3273,3D Human Pose Estimation = 2D Pose Estimation + Matching,"Ching-Hang Chen, Deva Ramanan",,,
,,,,,Applications,2442,Joint Gap Detection and Inpainting of Line Drawings,"Kazuma Sasaki, Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa",,,
,,,,,Biomedical Image/Video Analysis,918,Riemannian Nonlinear Mixed Effects Models: Analyzing Longitudinal Deformations in Neuroimaging,"Hyunwoo J. Kim, Nagesh Adluru, Heemanshu Suri, Baba C. Vemuri, Sterling C. Johnson, Vikas Singh",,,
,,,,,,2661,Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images Using Weakly-Supervised Joint Convolutional Sparse Coding,"Yawen Huang, Ling Shao, Alejandro F. Frangi",Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images using Weakly-Supervised Joint Convolutional Sparse Coding,"Magnetic Resonance Imaging (MRI) offers high-resolution \emph{in vivo} imaging and rich functional and anatomical multimodality tissue contrast. In practice, however, there are challenges associated with considerations of scanning costs, patient comfort, and scanning time that constrain how much data can be acquired in clinical or research studies. In this paper, we explore the possibility of generating high-resolution and multimodal images from low-resolution single-modality imagery. We propose the weakly-supervised joint convolutional sparse coding to simultaneously solve the problems of super-resolution (SR) and cross-modality image synthesis. The learning process requires only a few registered multimodal image pairs as the training set. Additionally, the quality of the joint dictionary learning can be improved using a larger set of unpaired images. To combine unpaired data from different image resolutions/modalities, a hetero-domain image alignment term is proposed. Local image neighborhoods are naturally preserved by operating on the whole image domain (as opposed to image patches) and using joint convolutional sparse coding. The paired images are enhanced in the joint learning process with unpaired data and an additional maximum mean discrepancy term, which minimizes the dissimilarity between their feature distributions. Experiments show that the proposed method outperforms state-of-the-art techniques on both SR reconstruction and simultaneous SR and cross-modality synthesis.",http://arxiv.org/pdf/1705.02596v1
,,,,,Computational Photography,3099,Multiple-Scattering Microphysics Tomography,"Aviad Levis, Yoav Y. Schechner, Anthony B. Davis",,,
,,,,,Image Motion & Tracking,464,Accurate Optical Flow via Direct Cost Volume Processing,"Jia Xu, RenÃ© Ranftl, Vladlen Koltun",Accurate Optical Flow via Direct Cost Volume Processing,"We present an optical flow estimation approach that operates on the full four-dimensional cost volume. This direct approach shares the structural benefits of leading stereo matching pipelines, which are known to yield high accuracy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume can be constructed in a fraction of a second due to its regularity. We then exploit this regularity further by adapting semi-global matching to the four-dimensional setting. This yields a pipeline that achieves significantly higher accuracy than state-of-the-art optical flow methods while being faster than most. Our approach outperforms all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.",http://arxiv.org/pdf/1704.07325v1
,,,,,,2274,Event-Based Visual Inertial Odometry,"Alex Zihao Zhu, Nikolay Atanasov, Kostas Daniilidis","The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM","New vision sensors, such as the Dynamic and Active-pixel Vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called ""events"") and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of ego-motion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e., rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.",http://arxiv.org/pdf/1610.08336v3
,,,,,,2371,Robust Visual Tracking Using Oblique Random Forests,"Le Zhang, Jagannadan Varadarajan, Ponnuthurai Nagaratnam Suganthan, Narendra Ahuja, Pierre Moulin",,,
,,,,,Low- & Mid-Level Vision,192,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,"Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang",Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,"Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.",http://arxiv.org/pdf/1704.03915v1
,,,,,,612,Learning Non-Lambertian Object Intrinsics Across ShapeNet Categories,"Jian Shi, Yue Dong, Hao Su, Stella X. Yu",Learning Non-Lambertian Object Intrinsics across ShapeNet Categories,"We consider the non-Lambertian object intrinsic problem of recovering diffuse albedo, shading, and specular highlights from a single image of an object.   We build a large-scale object intrinsics database based on existing 3D models in the ShapeNet database. Rendered with realistic environment maps, millions of synthetic images of objects and their corresponding albedo, shading, and specular ground-truth images are used to train an encoder-decoder CNN. Once trained, the network can decompose an image into the product of albedo and shading components, along with an additive specular component.   Our CNN delivers accurate and sharp results in this classical inverse problem of computer vision, sharp details attributed to skip layer connections at corresponding resolutions from the encoder to the decoder. Benchmarked on our ShapeNet and MIT intrinsics datasets, our model consistently outperforms the state-of-the-art by a large margin.   We train and test our CNN on different object categories. Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories. Our analysis shows that feature learning at the encoder stage is more crucial for developing a universal representation across categories.   We apply our synthetic data trained model to images and videos downloaded from the internet, and observe robust and realistic intrinsics results. Quality non-Lambertian intrinsics could open up many interesting applications such as image-based albedo and specular editing.",http://arxiv.org/pdf/1612.08510v1
,,,,,,744,MCMLSD: A Dynamic Programming Approach to Line Segment Detection,"Emilio J. AlmazÃ n, Ron Tal, Yiming Qian, James H. Elder",,,
,,,,,,889,Contour-Constrained Superpixels for Image and Video Processing,"Se-Ho Lee, Won-Dong Jang, Chang-Su Kim",,,
,,,,,,1104,Richer Convolutional Features for Edge Detection,"Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, Xiang Bai",Richer Convolutional Features for Edge Detection,"In this paper, we propose an accurate edge detector using richer convolutional features (RCF). Since objects in nature images have various scales and aspect ratios, the automatically learned rich hierarchical representations by CNNs are very critical and effective to detect edges and object boundaries. And the convolutional features gradually become coarser with receptive fields increasing. Based on these observations, our proposed network architecture makes full use of multiscale and multi-level information to perform the image-to-image edge prediction by combining all of the useful convolutional features into a holistic framework. It is the first attempt to adopt such rich convolutional features in computer vision tasks. Using VGG16 network, we achieve \sArt results on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of \textbf{.811} while retaining a fast speed (\textbf{8} FPS). Besides, our fast version of RCF achieves ODS F-measure of \textbf{.806} with \textbf{30} FPS.",http://arxiv.org/pdf/1612.02103v2
,,,,,,1320,Non-Local Color Image Denoising With Convolutional Neural Networks,Stamatios Lefkimmiatis,Non-Local Color Image Denoising with Convolutional Neural Networks,"We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.",http://arxiv.org/pdf/1611.06757v1
,,,,,,1526,Generative Face Completion,"Yijun Li, Sifei Liu, Jimei Yang, Ming-Hsuan Yang",Completely empty pyramids on integer lattices and two-dimensional faces of multidimensional continued fractions,"In this paper we develop an integer-affine classification of three-dimensional multistory completely empty convex marked pyramids. We apply it to obtain the complete lists of compact two-dimensional faces of multidimensional continued fractions lying in planes with integer distances to the origin equal 2, 3, 4 ... The faces are considered up to the action of the group of integer-linear transformations. In conclusion we formulate some actual unsolved problems associated with the generalizations for n-dimensional faces and more complicated face configurations.",http://arxiv.org/pdf/math/0510482v2
,,,,,,1726,Hyper-Laplacian Regularized Unidirectional Low-Rank Tensor Recovery for Multispectral Image Denoising,"Yi Chang, Luxin Yan, Sheng Zhong",,,
,,,,,,1923,Unsupervised Semantic Scene Labeling for Streaming Data,"Maggie Wigness, John G. Rogers III",,,
,,,,,,3102,Why You Should Forget Luminance Conversion and Do Something Better,"Rang M. H. Nguyen, Michael S. Brown",,,
,,,,,,3196,Deep Semantic Feature Matching,"Nikolai Ufer, BjÃ¶rn Ommer",AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching,"Despite significant progress of deep learning in recent years, state-of-the-art semantic matching methods still rely on legacy features such as SIFT or HoG. We argue that the strong invariance properties that are key to the success of recent deep architectures on the classification task make them unfit for dense correspondence tasks, unless a large amount of supervision is used. In this work, we propose a deep network, termed AnchorNet, that produces image representations that are well-suited for semantic matching. It relies on a set of filters whose response is geometrically consistent across different object instances, even in the presence of strong intra-class, scale, or viewpoint variations. Trained only with weak image-level labels, the final representation successfully captures information about the object structure and improves results of state-of-the-art semantic matching methods such as the deformable spatial pyramid or the proposal flow methods. We show positive results on the cross-instance matching task where different instances of the same object category are matched as well as on a new cross-category semantic matching task aligning pairs of instances each from a different object class.",http://arxiv.org/pdf/1704.04749v1
,,,,,Machine Learning,52,Revisiting the Variable Projection Method for Separable Nonlinear Least Squares Problems,"Je Hyeong Hong, Christopher Zach, Andrew Fitzgibbon",,,
,,,,,,183,Efficient Multiple Instance Metric Learning Using Weakly Supervised Data,"Marc T. Law, Yaoliang Yu, Raquel Urtasun, Richard S. Zemel, Eric P. Xing",,,
,,,,,,200,"WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation","Thibaut Durand, Taylor Mordan, Nicolas Thome, Matthieu Cord",,,
,,,,,,385,Image-To-Image Translation With Conditional Adversarial Networks,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros",Image-to-Image Translation with Conditional Adversarial Networks,"We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",http://arxiv.org/pdf/1611.07004v1
,,,,,,449,Deep Roots: Improving CNN Efficiency With Hierarchical Filter Groups,"Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio Criminisi",Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups,"We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters compared to state-of-the-art deep CNNs, without compromising accuracy, by exploiting the sparsity of inter-layer filter dependencies. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less computation, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).",http://arxiv.org/pdf/1605.06489v3
,,,,,,552,Aggregated Residual Transformations for Deep Neural Networks,"Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, Kaiming He",Aggregated Residual Transformations for Deep Neural Networks,"We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call ""cardinality"" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",http://arxiv.org/pdf/1611.05431v2
,,,,,,582,MIML-FCN+: Multi-Instance Multi-Label Learning via Fully Convolutional Networks With Privileged Information,"Hao Yang, Joey Tianyi Zhou, Jianfei Cai, Yew Soon Ong",,,
,,,,,,754,Low-Rank Embedded Ensemble Semantic Dictionary for Zero-Shot Learning,"Zhengming Ding, Ming Shao, Yun Fu",,,
,,,,,,940,Factorized Variational Autoencoders for Modeling Audience Reactions to Movies,"Zhiwei Deng, Rajitha Navarathna, Peter Carr, Stephan Mandt, Yisong Yue, Iain Matthews, Greg Mori",,,
,,,,,,980,Learning Features by Watching Objects Move,"Deepak Pathak, Ross Girshick, Piotr DollÃ¡r, Trevor Darrell, Bharath Hariharan",Learning Features by Watching Objects Move,"This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.",http://arxiv.org/pdf/1612.06370v2
,,,,,,1152,What Can Help Pedestrian Detection?,"Jiayuan Mao, Tete Xiao, Yuning Jiang, Zhimin Cao",,,
,,,,,,1565,DeepPermNet: Visual Permutation Learning,"Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, Stephen Gould",DeepPermNet: Visual Permutation Learning,"We present a principled approach to uncover the structure of visual data by solving a novel deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Unfortunately, permutation matrices are discrete, thereby posing difficulties for gradient-based methods. To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations. Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).",http://arxiv.org/pdf/1704.02729v1
,,,,,,1914,Learning the Multilinear Structure of Visual Data,"Mengjiao Wang, Yannis Panagakis, Patrick Snape, Stefanos Zafeiriou",,,
,,,,,,2032,Adaptive and Move Making Auxiliary Cuts for Binary Pairwise Energies,"Lena Gorelick, Yuri Boykov, Olga Veksler",,,
,,,,,,2417,Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning,"Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze",Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning,"Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or amount of computation, we find that they do not necessarily result in lower energy consumption, and therefore do not serve as a good metric for energy cost estimation.   To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses energy consumption estimation of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements that target realistic battery-powered system setups. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in output feature maps instead of filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is further globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. Finally, we show that pruning the AlexNet with a reduced number of target classes can greatly decrease the number of weights but the energy reduction is limited.   Energy modeling tool and energy-aware pruned models available at http://eyeriss.mit.edu/energy.html",http://arxiv.org/pdf/1611.05128v4
,,,,,,3111,Joint Multi-Person Pose Estimation and Semantic Part Segmentation,"Fangting Xia, Peng Wang, Xianjie Chen, Alan L. Yuille",Deep Multitask Architecture for Integrated 2D and 3D Human Sensing,"We propose a deep multitask architecture for \emph{fully automatic 2d and 3d human sensing} (DMHS), including \emph{recognition and reconstruction}, in \emph{monocular images}. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data.",http://arxiv.org/pdf/1701.08985v1
,,,,,,3299,Deep Feature Interpolation for Image Content Changes,"Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah Snavely, Kavita Bala, Kilian Weinberger",Deep Feature Interpolation for Image Content Changes,"We propose Deep Feature Interpolation (DFI), a new data-driven baseline for automatic high-resolution image transformation. As the name suggests, it relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, DFI can perform high-level semantic transformations like ""make older/younger"", ""make bespectacled"", ""add smile"", among others, surprisingly well - sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as DFI requires no specialized network architecture or even any deep network to be trained for these tasks. DFI therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging in the rise of deep learning.",http://arxiv.org/pdf/1611.05507v2
,,,,,,3609,FASON: First and Second Order Information Fusion Network for Texture Recognition,"Xiyang Dai, Joe Yue-Hei Ng, Larry S. Davis",,,
,,,,,,3875,Lean Crowdsourcing: Combining Humans and Machines in an Online System,"Steve Branson, Grant Van Horn, Pietro Perona",,,
,,,,,Object Recognition & Scene Understanding,156,Supervising Neural Attention Models for Video Captioning by Human Gaze Data,"Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim",,,
,,,,,,203,L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space,"Yurun Tian, Bin Fan, Fuchao Wu",,,
,,,,,,282,Convolutional Random Walk Networks for Semantic Image Segmentation,"Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi",Convolutional Random Walk Networks for Semantic Image Segmentation,"Most current semantic segmentation methods rely on fully convolutional networks (FCNs). However, their use of large receptive fields and many pooling layers cause low spatial resolution inside the deep layers. This leads to predictions with poor localization around the boundaries. Prior work has attempted to address this issue by post-processing predictions with CRFs or MRFs. But such models often fail to capture semantic relationships between objects, which causes spatially disjoint predictions. To overcome these problems, recent methods integrated CRFs or MRFs into an FCN framework. The downside of these new models is that they have much higher complexity than traditional FCNs, which renders training and testing more challenging.   In this work we introduce a simple, yet effective Convolutional Random Walk Network (RWN) that addresses the issues of poor boundary localization and spatially fragmented predictions with very little increase in model complexity. Our proposed RWN jointly optimizes the objectives of pixelwise affinity and semantic segmentation. It combines these two objectives via a novel random walk layer that enforces consistent spatial grouping in the deep layers of the network. Our RWN is implemented using standard convolution and matrix multiplication. This allows an easy integration into existing FCN frameworks and it enables end-to-end training of the whole network via standard back-propagation. Our implementation of RWN requires just $131$ additional parameters compared to the traditional FCNs, and yet it consistently produces an improvement over the FCNs on semantic segmentation and scene labeling.",http://arxiv.org/pdf/1605.07681v3
,,,,,,397,Knowledge Acquisition for Visual Question Answering via Iterative Querying,"Yuke Zhu, Joseph J. Lim, Li Fei-Fei",,,
,,,,,,562,Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search,"Bo Zhao, Jiashi Feng, Xiao Wu, Shuicheng Yan",,,
,,,,,,594,From Zero-Shot Learning to Conventional Supervised Classification: Unseen Visual Data Synthesis,"Yang Long, Li Liu, Ling Shao, Fumin Shen, Guiguang Ding, Jungong Han",From Zero-shot Learning to Conventional Supervised Classification: Unseen Visual Data Synthesis,"Robust object recognition systems usually rely on powerful feature extraction mechanisms from a large number of real images. However, in many realistic applications, collecting sufficient images for ever-growing new classes is unattainable. In this paper, we propose a new Zero-shot learning (ZSL) framework that can synthesise visual features for unseen classes without acquiring real images. Using the proposed Unseen Visual Data Synthesis (UVDS) algorithm, semantic attributes are effectively utilised as an intermediate clue to synthesise unseen visual features at the training stage. Hereafter, ZSL recognition is converted into the conventional supervised problem, i.e. the synthesised visual features can be straightforwardly fed to typical classifiers such as SVM. On four benchmark datasets, we demonstrate the benefit of using synthesised unseen data. Extensive experimental results suggest that our proposed approach significantly improve the state-of-the-art results.",http://arxiv.org/pdf/1705.01782v1
,,,,,,598,Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization?,"Torsten Sattler, Akihiko Torii, Josef Sivic, Marc Pollefeys, Hajime Taira, Masatoshi Okutomi, Tomas Pajdla",,,
,,,,,,876,Asymmetric Feature Maps With Application to Sketch Based Retrieval,"Giorgos Tolias, OndÅ™ej Chum",Asymmetric Feature Maps with Application to Sketch Based Retrieval,"We propose a novel concept of asymmetric feature maps (AFM), which allows to evaluate multiple kernels between a query and database entries without increasing the memory requirements. To demonstrate the advantages of the AFM method, we derive a short vector image representation that, due to asymmetric feature maps, supports efficient scale and translation invariant sketch-based image retrieval. Unlike most of the short-code based retrieval systems, the proposed method provides the query localization in the retrieved image. The efficiency of the search is boosted by approximating a 2D translation search via trigonometric polynomial of scores by 1D projections. The projections are a special case of AFM. An order of magnitude speed-up is achieved compared to traditional trigonometric polynomials. The results are boosted by an image-based average query expansion, exceeding significantly the state of the art on standard benchmarks.",http://arxiv.org/pdf/1704.03946v1
,,,,,,926,Diverse Image Annotation,"Baoyuan Wu, Fan Jia, Wei Liu, Bernard Ghanem",Synthesizing Training Images for Boosting Human 3D Pose Estimation,"Human 3D pose estimation from a single image is a challenging task with numerous applications. Convolutional Neural Networks (CNNs) have recently achieved superior performance on the task of 2D pose estimation from a single image, by training on images with 2D annotations collected by crowd sourcing. This suggests that similar success could be achieved for direct estimation of 3D poses. However, 3D poses are much harder to annotate, and the lack of suitable annotated training images hinders attempts towards end-to-end solutions. To address this issue, we opt to automatically synthesize training images with ground truth pose annotations. Our work is a systematic study along this road. We find that pose space coverage and texture diversity are the key ingredients for the effectiveness of synthetic training data. We present a fully automatic, scalable approach that samples the human pose space for guiding the synthesis procedure and extracts clothing textures from real images. Furthermore, we explore domain adaptation for bridging the gap between our synthetic training images and real testing photos. We demonstrate that CNNs trained with our synthetic images out-perform those trained with real photos on 3D pose estimation tasks.",http://arxiv.org/pdf/1604.02703v6
,,,,,,956,AMC: Attention guided Multi-modal Correlation Learning for Image Search,"Kan Chen, Trung Bui, Chen Fang, Zhaowen Wang, Ram Nevatia",AMC: Attention guided Multi-modal Correlation Learning for Image Search,"Given a user's query, traditional image search systems rank images according to its relevance to a single modality (e.g., image content or surrounding text). Nowadays, an increasing number of images on the Internet are available with associated meta data in rich modalities (e.g., titles, keywords, tags, etc.), which can be exploited for better similarity measure with queries. In this paper, we leverage visual and textual modalities for image search by learning their correlation with input query. According to the intent of query, attention mechanism can be introduced to adaptively balance the importance of different modalities. We propose a novel Attention guided Multi-modal Correlation (AMC) learning method which consists of a jointly learned hierarchy of intra and inter-attention networks. Conditioned on query's intent, intra-attention networks (i.e., visual intra-attention network and language intra-attention network) attend on informative parts within each modality; a multi-modal inter-attention network promotes the importance of the most query-relevant modalities. In experiments, we evaluate AMC models on the search logs from two real world image search engines and show a significant boost on the ranking of user-clicked images in search results. Additionally, we extend AMC models to caption ranking task on COCO dataset and achieve competitive results compared with recent state-of-the-arts.",http://arxiv.org/pdf/1704.00763v1
,,,,,,989,Multi-Attention Network for One Shot Learning,"Peng Wang, Lingqiao Liu, Chunhua Shen, Zi Huang, Anton van den Hengel, Heng Tao Shen",Person Depth ReID: Robust Person Re-identification with Commodity Depth Sensors,"This work targets person re-identification (ReID) from depth sensors such as Kinect. Since depth is invariant to illumination and less sensitive than color to day-by-day appearance changes, a natural question is whether depth is an effective modality for Person ReID, especially in scenarios where individuals wear different colored clothes or over a period of several months. We explore the use of recurrent Deep Neural Networks for learning high-level shape information from low-resolution depth images. In order to tackle the small sample size problem, we introduce regularization and a hard temporal attention unit. The whole model can be trained end to end with a hybrid supervised loss. We carry out a thorough experimental evaluation of the proposed method on three person re-identification datasets, which include side views, views from the top and sequences with varying degree of partial occlusion, pose and viewpoint variations. To that end, we introduce a new dataset with RGB-D and skeleton data. In a scenario where subjects are recorded after three months with new clothes, we demonstrate large performance gains attained using Depth ReID compared to a state-of-the-art Color ReID. Finally, we show further improvements using the temporal attention unit in multi-shot setting.",http://arxiv.org/pdf/1705.09882v1
,,,,,,1001,Fried Binary Embedding for High-Dimensional Visual Features,"Weixiang Hong, Junsong Yuan, Sreyasee Das Bhattacharjee",,,
,,,,,,1054,Pyramid Scene Parsing Network,"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",Pyramid Scene Parsing Network,"Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",http://arxiv.org/pdf/1612.01105v2
,,,,,,1234,Learning Deep Match Kernels for Image-Set Classification,"Haoliang Sun, Xiantong Zhen, Yuanjie Zheng, Gongping Yang, Yilong Yin, Shuo Li",Revisiting IM2GPS in the Deep Learning Era,"Image geolocalization, inferring the geographic location of an image, is a challenging computer vision problem with many potential applications. The recent state-of-the-art approach to this problem is a deep image classification approach in which the world is spatially divided into cells and a deep network is trained to predict the correct cell for a given image. We propose to combine this approach with the original Im2GPS approach in which a query image is matched against a database of geotagged images and the location is inferred from the retrieved set. We estimate the geographic location of a query image by applying kernel density estimation to the locations of its nearest neighbors in the reference database. Interestingly, we find that the best features for our retrieval task are derived from networks trained with classification loss even though we do not use a classification approach at test time. Training with classification loss outperforms several deep feature learning methods (e.g. Siamese networks with contrastive of triplet loss) more typical for retrieval applications. Our simple approach achieves state-of-the-art geolocalization accuracy while also requiring significantly less training data.",http://arxiv.org/pdf/1705.04838v1
,,,,,,1370,Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description,"Xishan Zhang, Ke Gao, Yongdong Zhang, Dongming Zhang, Jintao Li, Qi Tian",,,
,,,,,,1517,Learning Multifunctional Binary Codes for Both Category and Attribute Oriented Retrieval Tasks,"Haomiao Liu, Ruiping Wang, Shiguang Shan, Xilin Chen",,,
,,,,,,2294,"Indoor Scene Parsing With Instance Segmentation, Semantic Labeling and Support Relationship Inference","Wei Zhuo, Mathieu Salzmann, Xuming He, Miaomiao Liu",,,
,,,,,,2351,Episodic CAMN: Contextual Attention-Based Memory Networks With Iterative Feedback for Scene Labeling,"Abrar H. Abdulnabi, Bing Shuai, Stefan Winkler, Gang Wang",,,
,,,,,,2394,Link the Head to the â€œBeakâ€ù: Zero Shot Learning From Noisy Text Description at Part Precision,"Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, Ahmed Elgammal",,,
,,,,,,2402,SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning,"Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, Tat-Seng Chua",SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning,"Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.",http://arxiv.org/pdf/1611.05594v2
,,,,,,2559,Deep Pyramidal Residual Networks,"Dongyoon Han, Jiwhan Kim, Junmo Kim",Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,"Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.",http://arxiv.org/pdf/1704.03915v1
,,,,,,2743,Product Split Trees,"Artem Babenko, Victor Lempitsky","Hyperbolic graphs for free products, and the Gromov boundary of the graph of cyclic splittings","We define analogues of the graphs of free splittings, of cyclic splittings, and of maximally-cyclic splittings of $F_N$ for free products of groups, and show their hyperbolicity. Given a countable group $G$ which splits as $G=G_1\ast\dots\ast G_k\ast F$, where $F$ denotes a finitely generated free group, we identify the Gromov boundary of the graph of relative cyclic splittings with the space of equivalence classes of $\mathcal{Z}$-averse trees in the boundary of the corresponding outer space. A tree is \emph{$\mathcal{Z}$-averse} if it is not compatible with any tree $T'$, that is itself compatible with a relative cyclic splitting. Two $\mathcal{Z}$-averse trees are \emph{equivalent} if they are both compatible with a common tree in the boundary of the corresponding outer space. We give a similar description of the Gromov boundary of the graph of maximally-cyclic splittings.",http://arxiv.org/pdf/1408.0544v3
,,,,,,3177,Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,"Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,"Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.   We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).   We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.   Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.",http://arxiv.org/pdf/1612.00837v3
,,,,,,3417,Commonly Uncommon: Semantic Sparsity in Situation Recognition,"Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, Ali Farhadi",Commonly Uncommon: Semantic Sparsity in Situation Recognition,"Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.",http://arxiv.org/pdf/1612.00901v1
,,,,,,3667,Cross-Modality Binary Code Learning via Fusion Similarity Hashing,"Hong Liu, Rongrong Ji, Yongjian Wu, Feiyue Huang, Baochang Zhang",,,
,,,,,Theory,650,Saliency Revisited: Analysis of Mouse Movements Versus Fixations,"Hamed R. Tavakoli, Fawad Ahmed, Ali Borji, Jorma Laaksonen",Saliency Revisited: Analysis of Mouse Movements versus Fixations,"This paper revisits visual saliency prediction by evaluating the recent advancements in this field such as crowd-sourced mouse tracking-based databases and contextual annotations. We pursue a critical and quantitative approach towards some of the new challenges including the quality of mouse tracking versus eye tracking for model training and evaluation. We extend quantitative evaluation of models in order to incorporate contextual information by proposing an evaluation methodology that allows accounting for contextual factors such as text, faces, and object attributes. The proposed contextual evaluation scheme facilitates detailed analysis of models and helps identify their pros and cons. Through several experiments, we find that (1) mouse tracking data has lower inter-participant visual congruency and higher dispersion, compared to the eye tracking data, (2) mouse tracking data does not totally agree with eye tracking in general and in terms of different contextual regions in specific, and (3) mouse tracking data leads to acceptable results in training current existing models, and (4) mouse tracking data is less reliable for model selection and evaluation. The contextual evaluation also reveals that, among the studied models, there is no single model that performs best on all the tested annotations.",http://arxiv.org/pdf/1705.10546v1
,,,,,,1887,"InterpoNet, a Brain Inspired Neural Network for Optical Flow Dense Interpolation","Shay Zweig, Lior Wolf","InterpoNet, A brain inspired neural network for optical flow dense interpolation","Sparse-to-dense interpolation for optical flow is a fundamental phase in the pipeline of most of the leading optical flow estimation algorithms. The current state-of-the-art method for interpolation, EpicFlow, is a local average method based on an edge aware geodesic distance. We propose a new data-driven sparse-to-dense interpolation algorithm based on a fully convolutional network. We draw inspiration from the filling-in process in the visual cortex and introduce lateral dependencies between neurons and multi-layer supervision into our learning process. We also show the importance of the image contour to the learning process. Our method is robust and outperforms EpicFlow on competitive optical flow benchmarks with several underlying matching algorithms. This leads to state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.",http://arxiv.org/pdf/1611.09803v3
,,,,,Video Analytics,1063,SST: Single-Stream Temporal Action Proposals,"Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, Juan Carlos Niebles",,,
,,,,,,1105,Video Segmentation via Multiple Granularity Analysis,"Rui Yang, Bingbing Ni, Chao Ma, Yi Xu, Xiaokang Yang",,,
,,,,,,1442,Spatio-Temporal Alignment of Non-Overlapping Sequences From Independently Panning Cameras,"Seyed Morteza Safdarnejad, Xiaoming Liu",,,
,,,,,,1750,UntrimmedNets for Weakly Supervised Action Recognition and Detection,"Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool",UntrimmedNets for Weakly Supervised Action Recognition and Detection,"Current action recognition methods heavily rely on trimmed videos for model training. However, it is expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn action recognition models from untrimmed videos without the requirement of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two components are implemented with feed-forward networks, and UntrimmedNet is therefore an end-to-end trainable architecture. We exploit the learned models for action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of those strongly supervised approaches on these two datasets.",http://arxiv.org/pdf/1703.03329v2
"Tuesday, July 25, 2017",1300â€“1430,Kamehameha III,23,Spotlight 4-2A,Object Recognition & Scene Understanding 3,1855,Gaze Embeddings for Zero-Shot Image Classification,"Nour Karessli, Zeynep Akata, Bernt Schiele, Andreas Bulling",Gaze Embeddings for Zero-Shot Image Classification,"Zero-shot image classification using auxiliary information, such as attributes describing discriminative object properties, requires time-consuming annotation by domain experts. We instead propose a method that relies on human gaze as auxiliary information, exploiting that even non-expert users have a natural ability to judge class membership. We present a data collection paradigm that involves a discrimination task to increase the information content obtained from gaze data. Our method extracts discriminative descriptors from the data and learns a compatibility function between image and gaze using three novel gaze embeddings: Gaze Histograms (GH), Gaze Features with Grid (GFG) and Gaze Features with Sequence (GFS). We introduce two new gaze-annotated datasets for fine-grained image classification and show that human gaze data is indeed class discriminative, provides a competitive alternative to expert-annotated attributes, and outperforms other baselines for zero-shot image classification.",http://arxiv.org/pdf/1611.09309v2
,,,,,,90,What's in a Question: Using Visual Questions as a Form of Supervision,"Siddha Ganju, Olga Russakovsky, Abhinav Gupta",What's in a Question: Using Visual Questions as a Form of Supervision,"Collecting fully annotated image datasets is challenging and expensive. Many types of weak supervision have been explored: weak manual annotations, web search results, temporal continuity, ambient sound and others. We focus on one particular unexplored mode: visual questions that are asked about images. The key observation that inspires our work is that the question itself provides useful information about the image (even without the answer being available). For instance, the question ""what is the breed of the dog?"" informs the AI that the animal in the scene is a dog and that there is only one dog present. We make three contributions: (1) providing an extensive qualitative and quantitative analysis of the information contained in human visual questions, (2) proposing two simple but surprisingly effective modifications to the standard visual question answering models that allow them to make use of weak supervision in the form of unanswered questions associated with images and (3) demonstrating that a simple data augmentation strategy inspired by our insights results in a 7.1% improvement on the standard VQA benchmark.",http://arxiv.org/pdf/1704.03895v1
,,,,,,293,Attend to You: Personalized Image Captioning With Context Sequence Memory Networks,"Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim",Attend to You: Personalized Image Captioning with Context Sequence Memory Networks,"We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.",http://arxiv.org/pdf/1704.06485v2
,,,,,,941,Adversarially Tuned Scene Generation,"VSR Veeravasarapu, Constantin Rothkopf, Ramesh Visvanathan",Adversarially Tuned Scene Generation,"Generalization performance of trained computer vision systems that use computer graphics (CG) generated data is not yet effective due to the concept of 'domain-shift' between virtual and real data. Although simulated data augmented with a few real world samples has been shown to mitigate domain shift and improve transferability of trained models, guiding or bootstrapping the virtual data generation with the distributions learnt from target real world domain is desired, especially in the fields where annotating even few real images is laborious (such as semantic labeling, and intrinsic images etc.). In order to address this problem in an unsupervised manner, our work combines recent advances in CG (which aims to generate stochastic scene layouts coupled with large collections of 3D object models) and generative adversarial training (which aims train generative models by measuring discrepancy between generated and real data in terms of their separability in the space of a deep discriminatively-trained classifier). Our method uses iterative estimation of the posterior density of prior distributions for a generative graphical model. This is done within a rejection sampling framework. Initially, we assume uniform distributions as priors on the parameters of a scene described by a generative graphical model. As iterations proceed the prior distributions get updated to distributions that are closer to the (unknown) distributions of target data. We demonstrate the utility of adversarially tuned scene generation on two real-world benchmark datasets (CityScapes and CamVid) for traffic scene semantic labeling with a deep convolutional net (DeepLab). We realized performance improvements by 2.28 and 3.14 points (using the IoU metric) between the DeepLab models trained on simulated sets prepared from the scene generation models before and after tuning to CityScapes and CamVid respectively.",http://arxiv.org/pdf/1701.00405v1
,,,,,,1163,Residual Attention Network for Image Classification,"Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang",Residual Attention Network for Image Classification,"In this work, we propose ""Residual Attention Network"", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.",http://arxiv.org/pdf/1704.06904v1
,,,,,,1176,Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade,"Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang",Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade,"We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.",http://arxiv.org/pdf/1704.01344v1
,,,,,,1840,Learning Non-Maximum Suppression,"Jan Hosang, Rodrigo Benenson, Bernt Schiele",Learning non-maximum suppression,"Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and -- being based on greedy clustering with a fixed distance threshold -- forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.",http://arxiv.org/pdf/1705.02950v2
,,,,,,3408,The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives,"Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal DaumÃ© III, Larry S. Davis",The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives,"Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the ""gutters"" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called ""closure"". While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.",http://arxiv.org/pdf/1611.05118v2
,,,,Oral 4-2A,,581,Object Region Mining With Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach,"Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan",Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach,"We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.",http://arxiv.org/pdf/1703.08448v2
,,,,,,912,Fine-Grained Recognition as HSnet Search for Informative Image Parts,"Michael Lam, Behrooz Mahasseni, Sinisa Todorovic",,,
,,,,,,996,G2DeNet: Global Gaussian Distribution Embedding Network and Its Application to Visual Recognition,"Qilong Wang, Peihua Li, Lei Zhang",,,
,,,,,,3485,"YOLO9000: Better, Faster, Stronger","Joseph Redmon, Ali Farhadi","YOLO9000: Better, Faster, Stronger","We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.",http://arxiv.org/pdf/1612.08242v1
"Tuesday, July 25, 2017",1300â€“1430,KalÄÅkaua Ballroom,24,Spotlight 4-2B,Machine Learning for 3D Vision,704,Multi-View 3D Object Detection Network for Autonomous Driving,"Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia",Multi-View 3D Object Detection Network for Autonomous Driving,"This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.",http://arxiv.org/pdf/1611.07759v2
,,,,,,977,UltraStereo: Efficient Learning-Based Matching for Active Stereo Systems,"Sean Ryan Fanello, Julien Valentin, Christoph Rhemann, Adarsh Kowdle, Vladimir Tankovich, Philip Davidson, Shahram Izadi",,,
,,,,,,2519,Shape Completion Using 3D-Encoder-Predictor CNNs and Shape Synthesis,"Angela Dai, Charles Ruizhongtai Qi, Matthias NieÃŸner",Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis,"We introduce a data-driven approach to complete partial 3D shapes through a combination of volumetric deep neural networks and 3D shape synthesis. From a partially-scanned input shape, our method first infers a low-resolution -- but complete -- output. To this end, we introduce a 3D-Encoder-Predictor Network (3D-EPN) which is composed of 3D convolutional layers. The network is trained to predict and fill in missing data, and operates on an implicit surface representation that encodes both known and unknown space. This allows us to predict global structure in unknown areas at high accuracy. We then correlate these intermediary results with 3D geometry from a shape database at test time. In a final pass, we propose a patch-based 3D shape synthesis method that imposes the 3D geometry from these retrieved shapes as constraints on the coarsely-completed mesh. This synthesis process enables us to reconstruct fine-scale detail and generate high-resolution output while respecting the global mesh structure obtained by the 3D-EPN. Although our 3D-EPN outperforms state-of-the-art completion method, the main contribution in our work lies in the combination of a data-driven shape predictor and analytic 3D shape synthesis. In our results, we show extensive evaluations on a newly-introduced shape completion benchmark for both real-world and synthetic data.",http://arxiv.org/pdf/1612.00101v2
,,,,,,2602,Geometric Loss Functions for Camera Pose Regression With Deep Learning,"Alex Kendall, Roberto Cipolla",Geometric Loss Functions for Camera Pose Regression with Deep Learning,"Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.",http://arxiv.org/pdf/1704.00390v2
,,,,,,2762,CNN-SLAM: Real-Time Dense Monocular SLAM With Learned Depth Prediction,"Keisuke Tateno, Federico Tombari, Iro Laina, Nassir Navab",CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction,"Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.",http://arxiv.org/pdf/1704.03489v1
,,,,,,275,Learning From Noisy Large-Scale Datasets With Minimal Supervision,"Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, Serge Belongie",Learning From Noisy Large-Scale Datasets With Minimal Supervision,"We present an approach to effectively use millions of images with noisy annotations in conjunction with a small subset of cleanly-annotated images to learn powerful image representations. One common approach to combine clean and noisy data is to first pre-train a network using the large noisy dataset and then fine-tune with the clean dataset. We show this approach does not fully leverage the information contained in the clean set. Thus, we demonstrate how to use the clean annotations to reduce the noise in the large dataset before fine-tuning the network using both the clean set and the full set with reduced noise. The approach comprises a multi-task network that jointly learns to clean noisy annotations and to accurately classify images. We evaluate our approach on the recently released Open Images dataset, containing ~9 million images, multiple annotations per image and over 6000 unique classes. For the small clean set of annotations we use a quarter of the validation set with ~40k images. Our results demonstrate that the proposed approach clearly outperforms direct fine-tuning across all major categories of classes in the Open Image dataset. Further, our approach is particularly effective for a large number of classes with wide range of noise in annotations (20-80% false positive annotations).",http://arxiv.org/pdf/1701.01619v2
,,,,,,832,SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation,"Li Yi, Hao Su, Xingwen Guo, Leonidas J. Guibas",SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation,"In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.",http://arxiv.org/pdf/1612.00606v1
,,,,,,2999,Non-Local Deep Features for Salient Object Detection,"Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, Pierre-Marc Jodoin",,,
,,,,Oral 4-2B,,104,Unsupervised Monocular Depth Estimation With Left-Right Consistency,"ClÃ©ment Godard, Oisin Mac Aodha, Gabriel J. Brostow",Unsupervised Monocular Depth Estimation with Left-Right Consistency,"Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.   We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.",http://arxiv.org/pdf/1609.03677v3
,,,,,,687,Unsupervised Learning of Depth and Ego-Motion From Video,"Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe",Unsupervised Learning of Depth and Ego-Motion from Video,"We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings.",http://arxiv.org/pdf/1704.07813v1
,,,,,,1319,OctNet: Learning Deep 3D Representations at High Resolutions,"Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger",OctNet: Learning Deep 3D Representations at High Resolutions,"We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.",http://arxiv.org/pdf/1611.05009v4
,,,,,,1431,3D Shape Segmentation With Projective Convolutional Networks,"Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha Chaudhuri",3D Shape Segmentation with Projective Convolutional Networks,"This paper introduces a deep architecture for segmenting 3D objects into their labeled semantic parts. Our architecture combines image-based Fully Convolutional Networks (FCNs) and surface-based Conditional Random Fields (CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are used for efficient view-based reasoning about 3D object parts. Through a special projection layer, FCN outputs are effectively aggregated across multiple views and scales, then are projected onto the 3D object surfaces. Finally, a surface-based CRF combines the projected outputs with geometric consistency cues to yield coherent segmentations. The whole architecture (multi-view FCNs and CRF) is trained end-to-end. Our approach significantly outperforms the existing state-of-the-art methods in the currently largest segmentation benchmark (ShapeNet). Finally, we demonstrate promising segmentation results on noisy 3D shapes acquired from consumer-grade depth cameras.",http://arxiv.org/pdf/1612.02808v2
"Tuesday, July 25, 2017",1430â€“1630,Kamehameha I,25,Poster 4-2,3D Computer Vision,87,SGM-Nets: Semi-Global Matching With Neural Networks,"Akihito Seki, Marc Pollefeys",,,
,,,,,,455,Stereo-Based 3D Reconstruction of Dynamic Fluid Surfaces by Global Optimization,"Yiming Qian, Minglun Gong, Yee-Hong Yang",,,
,,,,,,647,Fine-To-Coarse Global Registration of RGB-D Scans,"Maciej Halber, Thomas Funkhouser",,,
,,,,,,731,"Analyzing Computer Vision Data - The Good, the Bad and the Ugly","Oliver Zendel, Katrin Honauer, Markus Murschitz, Martin Humenberger, Gustavo FernÃ¡ndez DomÃ_nguez","Zero-Shot Learning - The Good, the Bad and the Ugly","Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.",http://arxiv.org/pdf/1703.04394v1
,,,,,,1244,Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space,"Matthias Vestner, Roee Litman, Emanuele RodolÃ , Alex Bronstein, Daniel Cremers",Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space,"Many algorithms for the computation of correspondences between deformable shapes rely on some variant of nearest neighbor matching in a descriptor space. Such are, for example, various point-wise correspondence recovery algorithms used as a post-processing stage in the functional correspondence framework. Such frequently used techniques implicitly make restrictive assumptions (e.g., near-isometry) on the considered shapes and in practice suffer from lack of accuracy and result in poor surjectivity. We propose an alternative recovery technique capable of guaranteeing a bijective correspondence and producing significantly higher accuracy and smoothness. Unlike other methods our approach does not depend on the assumption that the analyzed shapes are isometric. We derive the proposed method from the statistical framework of kernel density estimation and demonstrate its performance on several challenging deformable 3D shape matching datasets.",http://arxiv.org/pdf/1701.00669v2
,,,,,,1732,Unsupervised Vanishing Point Detection and Camera Calibration From a Single Manhattan Image With Radial Distortion,"Michel Antunes, JoÃ£o P. Barreto, Djamila Aouada, BjÃ¶rn Ottersten",,,
,,,,,,1874,Toroidal Constraints for Two-Point Localization Under High Outlier Ratios,"Federico Camposeco, Torsten Sattler, Andrea Cohen, Andreas Geiger, Marc Pollefeys",,,
,,,,,,2829,4D Light Field Superpixel and Segmentation,"Hao Zhu, Qi Zhang, Qing Wang",,,
,,,,,,3708,Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure Estimation From Single and Multiple Images,"Yuan Gao, Alan L. Yuille",Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure Estimation from Single and Multiple Images,"Many man-made objects have intrinsic symmetries and Manhattan structure. By assuming an orthographic projection model, this paper addresses the estimation of 3D structures and camera projection using symmetry and/or Manhattan structure cues, which occur when the input is single- or multiple-image from the same category, e.g., multiple different cars. Specifically, analysis on the single image case implies that Manhattan alone is sufficient to recover the camera projection, and then the 3D structure can be reconstructed uniquely exploiting symmetry. However, Manhattan structure can be difficult to observe from a single image due to occlusion. To this end, we extend to the multiple-image case which can also exploit symmetry but does not require Manhattan axes. We propose a novel rigid structure from motion method, exploiting symmetry and using multiple images from the same category as input. Experimental results on the Pascal3D+ dataset show that our method significantly outperforms baseline methods.",http://arxiv.org/pdf/1607.07129v3
,,,,,Analyzing Humans in Images,56,Binary Coding for Partial Action Analysis With Limited Observation Ratios,"Jie Qin, Li Liu, Ling Shao, Bingbing Ni, Chen Chen, Fumin Shen, Yunhong Wang",,,
,,,,,,76,SphereFace: Deep Hypersphere Embedding for Face Recognition,"Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song",SphereFace: Deep Hypersphere Embedding for Face Recognition,"This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific $m$ to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks.",http://arxiv.org/pdf/1704.08063v1
,,,,,,170,IRINA: Iris Recognition (Even) in Inaccurately Segmented Data,"Hugo ProenÃ§a, JoÃ£o C. Neves",,,
,,,,,,306,Look Into Person: Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing,"Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, Liang Lin",Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing,"Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark ""Look into Person (LIP)"" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.",http://arxiv.org/pdf/1703.05446v1
,,,,,,685,"Action Unit Detection With Region Adaptation, Multi-Labeling Learning and Optimal Temporal Fusing","Wei Li, Farnaz Abtahi, Zhigang Zhu","Action Unit Detection with Region Adaptation, Multi-labeling Learning and Optimal Temporal Fusing","Action Unit (AU) detection becomes essential for facial analysis. Many proposed approaches face challenging problems in dealing with the alignments of different face regions, in the effective fusion of temporal information, and in training a model for multiple AU labels. To better address these problems, we propose a deep learning framework for AU detection with region of interest (ROI) adaptation, integrated multi-label learning, and optimal LSTM-based temporal fusing. First, ROI cropping nets (ROI Nets) are designed to make sure specifically interested regions of faces are learned independently; each sub-region has a local convolutional neural network (CNN) - an ROI Net, whose convolutional filters will only be trained for the corresponding region. Second, multi-label learning is employed to integrate the outputs of those individual ROI cropping nets, which learns the inter-relationships of various AUs and acquires global features across sub-regions for AU detection. Finally, the optimal selection of multiple LSTM layers to form the best LSTM Net is carried out to best fuse temporal features, in order to make the AU prediction the most accurate. The proposed approach is evaluated on two popular AU detection datasets, BP4D and DISFA, outperforming the state of the art significantly, with an average improvement of around 13% on BP4D and 25% on DISFA, respectively.",http://arxiv.org/pdf/1704.03067v1
,,,,,,1970,See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification,"Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, Tieniu Tan",,,
,,,,,,2430,Joint Intensity and Spatial Metric Learning for Robust Gait Recognition,"Yasushi Makihara, Atsuyuki Suzuki, Daigo Muramatsu, Xiang Li, Yasushi Yagi",,,
,,,,,,2747,Pose-Aware Person Recognition,"Vijay Kumar, Anoop Namboodiri, Manohar Paluri, C. V. Jawahar",Pose-Aware Person Recognition,"Person recognition methods that use multiple body regions have shown significant improvements over traditional face-based recognition. One of the primary challenges in full-body person recognition is the extreme variation in pose and view point. In this work, (i) we present an approach that tackles pose variations utilizing multiple models that are trained on specific poses, and combined using pose-aware weights during testing. (ii) For learning a person representation, we propose a network that jointly optimizes a single loss over multiple body regions. (iii) Finally, we introduce new benchmarks to evaluate person recognition in diverse scenarios and show significant improvements over previously proposed approaches on all the benchmarks including the photo album setting of PIPA.",http://arxiv.org/pdf/1705.10120v1
,,,,,,3026,Not Afraid of the Dark: NIR-VIS Face Recognition via Cross-Spectral Hallucination and Low-Rank Embedding,"JosÃ© Lezama, Qiang Qiu, Guillermo Sapiro",Not Afraid of the Dark: NIR-VIS Face Recognition via Cross-spectral Hallucination and Low-rank Embedding,"Surveillance cameras today often capture NIR (near infrared) images in low-light environments. However, most face datasets accessible for training and verification are only collected in the VIS (visible light) spectrum. It remains a challenging problem to match NIR to VIS face images due to the different light spectrum. Recently, breakthroughs have been made for VIS face recognition by applying deep learning on a huge amount of labeled VIS face samples. The same deep learning approach cannot be simply applied to NIR face recognition for two main reasons: First, much limited NIR face images are available for training compared to the VIS spectrum. Second, face galleries to be matched are mostly available only in the VIS spectrum. In this paper, we propose an approach to extend the deep learning breakthrough for VIS face recognition to the NIR spectrum, without retraining the underlying deep models that see only VIS faces. Our approach consists of two core components, cross-spectral hallucination and low-rank embedding, to optimize respectively input and output of a VIS deep model for cross-spectral face recognition. Cross-spectral hallucination produces VIS faces from NIR images through a deep learning approach. Low-rank embedding restores a low-rank structure for faces deep features across both NIR and VIS spectrum. We observe that it is often equally effective to perform hallucination to input NIR images or low-rank embedding to output deep features for a VIS deep model for cross-spectral recognition. When hallucination and low-rank embedding are deployed together, we observe significant further improvement; we obtain state-of-the-art accuracy on the CASIA NIR-VIS v2.0 benchmark, without the need at all to re-train the recognition system.",http://arxiv.org/pdf/1611.06638v1
,,,,,Applications,694,Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals,"Katsuyuki Nakamura, Serena Yeung, Alexandre Alahi, Li Fei-Fei",,,
,,,,,,1500,Binarized Mode Seeking for Scalable Visual Pattern Discovery,"Wei Zhang, Xiaochun Cao, Rui Wang, Yuanfang Guo, Zhineng Chen",,,
,,,,,,2275,Scribbler: Controlling Deep Image Synthesis With Sketch and Color,"Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, James Hays",Scribbler: Controlling Deep Image Synthesis with Sketch and Color,"Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.",http://arxiv.org/pdf/1612.00835v2
,,,,,Biomedical Image/Video Analysis,130,Multi-Way Multi-Level Kernel Modeling for Neuroimaging Classification,"Lifang He, Chun-Ta Lu, Hao Ding, Shen Wang, Linlin Shen, Philip S. Yu, Ann B. Ragin",,,
,,,,,,3455,WSISA: Making Survival Prediction From Whole Slide Histopathological Images,"Xinliang Zhu, Jiawen Yao, Feiyun Zhu, Junzhou Huang",,,
,,,,,Computational Photography,781,On the Effectiveness of Visible Watermarks,"Tali Dekel, Michael Rubinstein, Ce Liu, William T. Freeman","Invisible Flow Watermarks for Channels with Dependent Substitution, Deletion, and Bursty Insertion Errors","Flow watermarks efficiently link packet flows in a network in order to thwart various attacks such as stepping stones. We study the problem of designing good flow watermarks. Earlier flow watermarking schemes mostly considered substitution errors, neglecting the effects of packet insertions and deletions that commonly happen within a network. More recent schemes consider packet deletions but often at the expense of the watermark visibility. We present an invisible flow watermarking scheme capable of enduring a large number of packet losses and insertions. To maintain invisibility, our scheme uses quantization index modulation (QIM) to embed the watermark into inter-packet delays, as opposed to time intervals including many packets. As the watermark is injected within individual packets, packet losses and insertions may lead to watermark desynchronization and substitution errors. To address this issue, we add a layer of error-correction coding to our scheme. Experimental results on both synthetic and real network traces demonstrate that our scheme is robust to network jitter, packet drops and splits, while remaining invisible to an attacker.",http://arxiv.org/pdf/1302.5734v2
,,,,,,1219,Snapshot Hyperspectral Light Field Imaging,"Zhiwei Xiong, Lizhi Wang, Huiqun Li, Dong Liu, Feng Wu",,,
,,,,,,2322,Semantic Image Inpainting With Deep Generative Models,"Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson, Minh N. Do",Semantic Image Inpainting with Perceptual and Contextual Losses,"In this paper, we propose a novel method for image inpainting based on a Deep Convolutional Generative Adversarial Network (DCGAN). We define a loss function consisting of two parts: (1) a contextual loss that preserves similarity between the input corrupted image and the recovered image, and (2) a perceptual loss that ensures a perceptually realistic output image. Given a corrupted image with missing values, we use back-propagation on this loss to map the corrupted image to a smaller latent space. The mapped vector is then passed through the generative model to predict the missing content. The proposed framework is evaluated on the CelebA and SVHN datasets for two challenging inpainting tasks with random 80% corruption and large blocky corruption. Experiments show that our method can successfully predict semantic information in the missing region and achieve pixel-level photorealism, which is impossible by almost all existing methods.",http://arxiv.org/pdf/1607.07539v2
,,,,,Image Motion & Tracking,1556,Fast Multi-Frame Stereo Scene Flow With Motion Segmentation,"Tatsunori Taniai, Sudipta N. Sinha, Yoichi Sato",,,
,,,,,,1934,Improved Stereo Matching With Constant Highway Networks and Reflective Confidence Learning,"Amit Shaked, Lior Wolf",Improved Stereo Matching with Constant Highway Networks and Reflective Confidence Learning,"We present an improved three-step pipeline for the stereo matching problem and introduce multiple novelties at each stage. We propose a new highway network architecture for computing the matching cost at each possible disparity, based on multilevel weighted residual shortcuts, trained with a hybrid loss that supports multilevel comparison of image patches. A novel post-processing step is then introduced, which employs a second deep convolutional neural network for pooling global information from multiple disparities. This network outputs both the image disparity map, which replaces the conventional ""winner takes all"" strategy, and a confidence in the prediction. The confidence score is achieved by training the network with a new technique that we call the reflective loss. Lastly, the learned confidence is employed in order to better detect outliers in the refinement step. The proposed pipeline achieves state of the art accuracy on the largest and most competitive stereo benchmarks, and the learned confidence is shown to outperform all existing alternatives.",http://arxiv.org/pdf/1701.00165v1
,,,,,,1941,Optical Flow in Mostly Rigid Scenes,"Jonas Wulff, Laura Sevilla-Lara, Michael J. Black",Optical Flow in Mostly Rigid Scenes,"The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.",http://arxiv.org/pdf/1705.01352v1
,,,,,,2038,Optical Flow Requires Multiple Strategies (but Only One Network),"Tal Schuster, Lior Wolf, David Gadot",Optical Flow Requires Multiple Strategies (but only one network),"We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.",http://arxiv.org/pdf/1611.05607v3
,,,,,,3044,ECO: Efficient Convolution Operators for Tracking,"Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg",ECO: Efficient Convolution Operators for Tracking,"In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance.   We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.",http://arxiv.org/pdf/1611.09224v2
,,,,,Low- & Mid-Level Vision,248,Differential Angular Imaging for Material Recognition,"Jia Xue, Hang Zhang, Kristin Dana, Ko Nishino",Differential Angular Imaging for Material Recognition,"Material recognition for real-world outdoor surfaces has become increasingly important for computer vision to support its operation ""in the wild."" Computational surface modeling that underlies material recognition has transitioned from reflectance modeling using in-lab controlled radiometric measurements to image-based representations based on internet-mined images of materials captured in the scene. We propose to take a middle-ground approach for material recognition that takes advantage of both rich radiometric cues and flexible image capture. We realize this by developing a framework for differential angular imaging, where small angular variations in image capture provide an enhanced appearance representation and significant recognition improvement. We build a large-scale material database, Ground Terrain in Outdoor Scenes (GTOS) database, geared towards real use for autonomous agents. The database consists of over 30,000 images covering 40 classes of outdoor ground terrain under varying weather and lighting conditions. We develop a novel approach for material recognition called a Differential Angular Imaging Network (DAIN) to fully leverage this large dataset. With this novel network architecture, we extract characteristics of materials encoded in the angular and spatial gradients of their appearance. Our results show that DAIN achieves recognition performance that surpasses single view or coarsely quantized multiview images. These results demonstrate the effectiveness of differential angular imaging as a means for flexible, in-place material recognition.",http://arxiv.org/pdf/1612.02372v1
,,,,,,287,Fast Fourier Color Constancy,"Jonathan T. Barron, Yun-Ta Tsai",Fast Fourier Color Constancy,"We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm which solves illuminant estimation by reducing it to a spatial localization task on a torus. By operating in the frequency domain, FFCC produces lower error rates than the previous state-of-the-art by 13-20% while being 250-3000 times faster. This unconventional approach introduces challenges regarding aliasing, directional statistics, and preconditioning, which we address. By producing a complete posterior distribution over illuminants instead of a single illuminant estimate, FFCC enables better training techniques, an effective temporal smoothing technique, and richer methods for error analysis. Our implementation of FFCC runs at ~700 frames per second on a mobile device, allowing it to be used as an accurate, real-time, temporally-coherent automatic white balance algorithm.",http://arxiv.org/pdf/1611.07596v2
,,,,,,551,Comparative Evaluation of Hand-Crafted and Learned Local Features,"Johannes L. SchÃ¶nberger, Hans Hardmeier, Torsten Sattler, Marc Pollefeys",Self-Transfer Learning for Fully Weakly Supervised Object Localization,"Recent advances of deep learning have achieved remarkable performances in various challenging computer vision tasks. Especially in object localization, deep convolutional neural networks outperform traditional approaches based on extraction of data/task-driven features instead of hand-crafted features. Although location information of region-of-interests (ROIs) gives good prior for object localization, it requires heavy annotation efforts from human resources. Thus a weakly supervised framework for object localization is introduced. The term ""weakly"" means that this framework only uses image-level labeled datasets to train a network. With the help of transfer learning which adopts weight parameters of a pre-trained network, the weakly supervised learning framework for object localization performs well because the pre-trained network already has well-trained class-specific features. However, those approaches cannot be used for some applications which do not have pre-trained networks or well-localized large scale images. Medical image analysis is a representative among those applications because it is impossible to obtain such pre-trained networks. In this work, we present a ""fully"" weakly supervised framework for object localization (""semi""-weakly is the counterpart which uses pre-trained filters for weakly supervised localization) named as self-transfer learning (STL). It jointly optimizes both classification and localization networks simultaneously. By controlling a supervision level of the localization network, STL helps the localization network focus on correct ROIs without any types of priors. We evaluate the proposed STL framework using two medical image datasets, chest X-rays and mammograms, and achieve signiticantly better localization performance compared to previous weakly supervised approaches.",http://arxiv.org/pdf/1602.01625v1
,,,,,,1464,Learning Fully Convolutional Networks for Iterative Non-Blind Deconvolution,"Jiawei Zhang, Jinshan Pan, Wei-Sheng Lai, Rynson W. H. Lau, Ming-Hsuan Yang",Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution,"In this paper, we propose a fully convolutional networks for iterative non-blind deconvolution We decompose the non-blind deconvolution problem into image denoising and image deconvolution. We train a FCNN to remove noises in the gradient domain and use the learned gradients to guide the image deconvolution step. In contrast to the existing deep neural network based methods, we iteratively deconvolve the blurred images in a multi-stage framework. The proposed method is able to learn an adaptive image prior, which keeps both local (details) and global (structures) information. Both quantitative and qualitative evaluations on benchmark datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of quality and speed.",http://arxiv.org/pdf/1611.06495v1
,,,,,,1605,Image Deblurring via Extreme Channels Prior,"Yanyang Yan, Wenqi Ren, Yuanfang Guo, Rui Wang, Xiaochun Cao",,,
,,,,,,1776,Simultaneous Stereo Video Deblurring and Scene Flow Estimation,"Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli",Simultaneous Stereo Video Deblurring and Scene Flow Estimation,"Videos for outdoor scene often show unpleasant blur effects due to the large relative motion between the camera and the dynamic objects and large depth variations. Existing works typically focus monocular video deblurring. In this paper, we propose a novel approach to deblurring from stereo videos. In particular, we exploit the piece-wise planar assumption about the scene and leverage the scene flow information to deblur the image. Unlike the existing approach [31] which used a pre-computed scene flow, we propose a single framework to jointly estimate the scene flow and deblur the image, where the motion cues from scene flow estimation and blur information could reinforce each other, and produce superior results than the conventional scene flow estimation or stereo deblurring methods. We evaluate our method extensively on two available datasets and achieve significant improvement in flow estimation and removing the blur effect over the state-of-the-art methods.",http://arxiv.org/pdf/1704.03273v1
,,,,,,2058,Deep Photo Style Transfer,"Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala",Deep Photo Style Transfer,"This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.",http://arxiv.org/pdf/1703.07511v3
,,,,,,2668,Generative Attribute Controller With Conditional Filtered Generative Adversarial Networks,"Takuhiro Kaneko, Kaoru Hiramatsu, Kunio Kashino",,,
,,,,,,3729,Fast Haze Removal for Nighttime Image Using Maximum Reflectance Prior,"Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, Chang Wen Chen",,,
,,,,,Machine Learning,132,Low-Rank Bilinear Pooling for Fine-Grained Classification,"Shu Kong, Charless Fowlkes",Low-rank Bilinear Pooling for Fine-Grained Classification,"Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of fine-grained classification tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a low-rank bilinear classifier. The resulting classifier can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned.   To further compress the model, we propose classifier co-decomposition that factorizes the collection of bilinear classifiers into a common factor and compact per-class terms. The co-decomposition idea can be deployed through two convolutional layers and trained in an end-to-end architecture. We suggest a simple yet effective initialization that avoids explicitly first training and factorizing the larger bilinear classifiers. Through extensive experiments, we show that our model achieves state-of-the-art performance on several public datasets for fine-grained classification trained with only category labels. Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model, and three orders smaller than the standard bilinear CNN model.",http://arxiv.org/pdf/1611.05109v2
,,,,,,220,Neural Scene De-Rendering,"Jiajun Wu, Joshua B. Tenenbaum, Pushmeet Kohli",,,
,,,,,,252,Real-Time Neural Style Transfer for Videos,"Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu",Characterizing and Improving Stability in Neural Style Transfer,"Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not re- quire optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.",http://arxiv.org/pdf/1705.02092v1
,,,,,,422,A Graph Regularized Deep Neural Network for Unsupervised Image Representation Learning,"Shijie Yang, Liang Li, Shuhui Wang, Weigang Zhang, Qingming Huang",Image Representation Learning Using Graph Regularized Auto-Encoders,"We consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning. In those learning tasks, the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space. To overcome this problem, the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering.   Inspired by the recent research works on deep neural network and representation learning, in this paper, we introduce the multiple-layer auto-encoder into image representation, we also apply the locally invariant ideal to our image representation with auto-encoders and propose a novel method, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact representation which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure.   Extensive experiments on image clustering show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-word cases.",http://arxiv.org/pdf/1312.0786v2
,,,,,,587,A Study of Lagrangean Decompositions and Dual Ascent Solvers for Graph Matching,"Paul Swoboda, Carsten Rother, Hassan Abu Alhaija, Dagmar KainmÃ_ller, Bogdan Savchynskyy",A Study of Lagrangean Decompositions and Dual Ascent Solvers for Graph Matching,"We study the quadratic assignment problem, in computer vision also known as graph matching. Two leading solvers for this problem optimize the Lagrange decomposition duals with sub-gradient and dual ascent (also known as message passing) updates. We explore s direction further and propose several additional Lagrangean relaxations of the graph matching problem along with corresponding algorithms, which are all based on a common dual ascent framework. Our extensive empirical evaluation gives several theoretical insights and suggests a new state-of-the-art any-time solver for the considered problem. Our improvement over state-of-the-art is particularly visible on a new dataset with large-scale sparse problem instances containing more than 500 graph nodes each.",http://arxiv.org/pdf/1612.05476v2
,,,,,,615,Collaborative Deep Reinforcement Learning for Joint Object Search,"Xiangyu Kong, Bo Xin, Yizhou Wang, Gang Hua",Collaborative Deep Reinforcement Learning for Joint Object Search,"We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable.",http://arxiv.org/pdf/1702.05573v1
,,,,,,778,Loss Max-Pooling for Semantic Image Segmentation,"Samuel Rota BulÃ_, Gerhard Neuhold, Peter Kontschieder",Loss Max-Pooling for Semantic Image Segmentation,"We introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on benchmark datasets. In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach.",http://arxiv.org/pdf/1704.02966v1
,,,,,,784,Deep View Morphing,"Dinghuang Ji, Junghyun Kwon, Max McFarland, Silvio Savarese",Deep View Morphing,"Recently, convolutional neural networks (CNN) have been successfully applied to view synthesis problems. However, such CNN-based methods can suffer from lack of texture details, shape distortions, or high computational complexity. In this paper, we propose a novel CNN architecture for view synthesis called ""Deep View Morphing"" that does not suffer from these issues. To synthesize a middle view of two input images, a rectification network first rectifies the two input images. An encoder-decoder network then generates dense correspondences between the rectified images and blending masks to predict the visibility of pixels of the rectified images in the middle view. A view morphing network finally synthesizes the middle view using the dense correspondences and blending masks. We experimentally show the proposed method significantly outperforms the state-of-the-art CNN-based view synthesis method.",http://arxiv.org/pdf/1703.02168v1
,,,,,,793,Unsupervised Learning of Long-Term Motion Dynamics for Videos,"Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei",Unsupervised Learning of Long-Term Motion Dynamics for Videos,"We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, Depth, and RGB-D videos.",http://arxiv.org/pdf/1701.01821v3
,,,,,,1197,Revisiting Metric Learning for SPD Matrix Based Visual Representation,"Luping Zhou, Lei Wang, Jianjia Zhang, Yinghuan Shi, Yang Gao",,,
,,,,,,1252,Expert Gate: Lifelong Learning With a Network of Experts,"Rahaf Aljundi, Punarjay Chakravarty, Tinne Tuytelaars",Expert Gate: Lifelong Learning with a Network of Experts,"In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.",http://arxiv.org/pdf/1611.06194v2
,,,,,,1684,"A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning","Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim",,,
,,,,,,1823,Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors,"Piotr Koniusz, Yusuf Tas, Fatih Porikli",,,
,,,,,,1998,Deep Mixture of Linear Inverse Regressions Applied to Head-Pose Estimation,"StÃ©phane LathuiliÃ¨re, RÃ©mi Juge, Pablo Mesejo, Rafael MuÃ±oz-Salinas, Radu Horaud",,,
,,,,,,2002,STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling,"Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz",STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling,"We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.",http://arxiv.org/pdf/1604.02388v3
,,,,,,2077,Harmonic Networks: Deep Translation and Rotation Equivariance,"Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow",Harmonic Networks: Deep Translation and Rotation Equivariance,"Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.   H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.",http://arxiv.org/pdf/1612.04642v2
,,,,,,2191,Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer,"Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang",Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer,"Transferring artistic styles onto everyday photographs has become an extremely popular task in both academia and industry. Recently, offline training has replaced on-line iterative optimization, enabling nearly real-time stylization. When those stylization networks are applied directly to high-resolution images, however, the style of localized regions often appears less similar to the desired artistic style. This is because the transfer process fails to capture small, intricate textures and maintain correct texture scales of the artworks. Here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and luminance channels, and performs stylization hierarchically with multiple losses of increasing scales. Compared to state-of-the-art networks, our network can also perform style transfer in nearly real-time by conducting much more sophisticated training offline. By properly handling style and texture cues at multiple scales using several modalities, we can transfer not just large-scale, obvious style cues but also subtle, exquisite ones. That is, our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales.",http://arxiv.org/pdf/1612.01895v2
,,,,,,2202,"Detect, Replace, Refine: Deep Structured Prediction for Pixel Wise Labeling","Spyros Gidaris, Nikos Komodakis","Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise Labeling","Pixel wise image labeling is an interesting and challenging problem with great significance in the computer vision community. In order for a dense labeling algorithm to be able to achieve accurate and precise results, it has to consider the dependencies that exist in the joint space of both the input and the output variables. An implicit approach for modeling those dependencies is by training a deep neural network that, given as input an initial estimate of the output labels and the input image, it will be able to predict a new refined estimate for the labels. In this context, our work is concerned with what is the optimal architecture for performing the label improvement task. We argue that the prior approaches of either directly predicting new label estimates or predicting residual corrections w.r.t. the initial labels with feed-forward deep network architectures are sub-optimal. Instead, we propose a generic architecture that decomposes the label improvement task to three steps: 1) detecting the initial label estimates that are incorrect, 2) replacing the incorrect labels with new ones, and finally 3) refining the renewed labels by predicting residual corrections w.r.t. them. Furthermore, we explore and compare various other alternative architectures that consist of the aforementioned Detection, Replace, and Refine components. We extensively evaluate the examined architectures in the challenging task of dense disparity estimation (stereo matching) and we report both quantitative and qualitative results on three different datasets. Finally, our dense disparity estimation network that implements the proposed generic architecture, achieves state-of-the-art results in the KITTI 2015 test surpassing prior approaches by a significant margin.",http://arxiv.org/pdf/1612.04770v1
,,,,,,2315,Weighted-Entropy-Based Quantization for Deep Neural Networks,"Eunhyeok Park, Junwhan Ahn, Sungjoo Yoo",,,
,,,,,,2506,Residual Expansion Algorithm: Fast and Effective Optimization for Nonconvex Least Squares Problems,"Daiki Ikami, Toshihiko Yamasaki, Kiyoharu Aizawa",Residual Expansion Algorithm: Fast and Effective Optimization for Nonconvex Least Squares Problems,"We propose the residual expansion (RE) algorithm: a global (or near-global) optimization method for nonconvex least squares problems. Unlike most existing nonconvex optimization techniques, the RE algorithm is not based on either stochastic or multi-point searches; therefore, it can achieve fast global optimization. Moreover, the RE algorithm is easy to implement and successful in high-dimensional optimization. The RE algorithm exhibits excellent empirical performance in terms of k-means clustering, point-set registration, optimized product quantization, and blind image deblurring.",http://arxiv.org/pdf/1705.09549v1
,,,,,,3219,Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-In-The-Blank Image Captioning,"Qing Sun, Stefan Lee, Dhruv Batra",Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning,"We develop the first approximate inference algorithm for 1-Best (and M-Best) decoding in bidirectional neural sequence models by extending Beam Search (BS) to reason about both forward and backward time dependencies. Beam Search (BS) is a widely used approximate inference algorithm for decoding sequences from unidirectional neural sequence models. Interestingly, approximate inference in bidirectional models remains an open problem, despite their significant advantage in modeling information from both the past and future. To enable the use of bidirectional models, we present Bidirectional Beam Search (BiBS), an efficient algorithm for approximate bidirectional inference.To evaluate our method and as an interesting problem in its own right, we introduce a novel Fill-in-the-Blank Image Captioning task which requires reasoning about both past and future sentence structure to reconstruct sensible image descriptions. We use this task as well as the Visual Madlibs dataset to demonstrate the effectiveness of our approach, consistently outperforming all baseline methods.",http://arxiv.org/pdf/1705.08759v1
,,,,,,3555,Newton-Type Methods for Inference in Higher-Order Markov Random Fields,"Hariprasad Kannan, Nikos Komodakis, Nikos Paragios",,,
,,,,,,3685,Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation,"Zheng Xu, MÃ¡rio A. T. Figueiredo, Xiaoming Yuan, Christoph Studer, Tom Goldstein",Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation,"Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.",http://arxiv.org/pdf/1704.02712v1
,,,,,Object Recognition & Scene Understanding,490,ViP-CNN: Visual Phrase Guided Convolutional Neural Network,"Yikang Li, Wanli Ouyang, Xiaogang Wang, Xiao'ou Tang",ViP-CNN: Visual Phrase Guided Convolutional Neural Network,"As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers' attention because of its descriptive power and clear structure. It detects the objects and captures their pair-wise interactions with a subject-predicate-object triplet, e.g. person-ride-horse. In this paper, each visual relationship is considered as a phrase with three components. We formulate the visual relationship detection as three inter-connected recognition problems and propose a Visual Phrase guided Convolutional Neural Network (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a Phrase-guided Message Passing Structure (PMPS) to establish the connection among relationship components and help the model consider the three problems jointly. Corresponding non-maximum suppression method and model training strategy are also proposed. Experimental results show that our ViP-CNN outperforms the state-of-art method both in speed and accuracy. We further pretrain ViP-CNN on our cleansed Visual Genome Relationship dataset, which is found to perform better than the pretraining on the ImageNet for this task.",http://arxiv.org/pdf/1702.07191v2
,,,,,,840,Instance-Aware Image and Sentence Matching With Selective Multimodal LSTM,"Yan Huang, Wei Wang, Liang Wang",Instance-aware Image and Sentence Matching with Selective Multimodal LSTM,"Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based on the observation that such a global similarity arises from a complex aggregation of multiple local similarities between pairwise instances of image (objects) and sentence (words), we propose a selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware image and sentence matching. The sm-LSTM includes a multimodal context-modulated attention scheme at each timestep that can selectively attend to a pair of instances of image and sentence, by predicting pairwise instance-aware saliency maps for image and sentence. For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity. By similarly measuring multiple local similarities within a few timesteps, the sm-LSTM sequentially aggregates them with hidden states to obtain a final matching score as the desired global similarity. Extensive experiments show that our model can well match image and sentence with complex content, and achieve the state-of-the-art results on two public benchmark datasets.",http://arxiv.org/pdf/1611.05588v1
,,,,,,882,Kernel Square-Loss Exemplar Machines for Image Retrieval,"Rafael S. Rezende, Joaquin Zepeda, Jean Ponce, Francis Bach, Patrick PÃ©rez",,,
,,,,,,948,Cognitive Mapping and Planning for Visual Navigation,"Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik",Cognitive Mapping and Planning for Visual Navigation,"We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as ""go to a chair"".",http://arxiv.org/pdf/1702.03920v2
,,,,,,1306,"Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation","Anirban Roy, Sinisa Todorovic",,,
,,,,,,1470,Seeing Into Darkness: Scotopic Visual Recognition,"Bo Chen, Pietro Perona",Seeing into Darkness: Scotopic Visual Recognition,"Images are formed by counting how many photons traveling from a given set of directions hit an image sensor during a given time interval. When photons are few and far in between, the concept of `image' breaks down and it is best to consider directly the flow of photons. Computer vision in this regime, which we call `scotopic', is radically different from the classical image-based paradigm in that visual computations (classification, control, search) have to take place while the stream of photons is captured and decisions may be taken as soon as enough information is available. The scotopic regime is important for biomedical imaging, security, astronomy and many other fields. Here we develop a framework that allows a machine to classify objects with as few photons as possible, while maintaining the error rate below an acceptable threshold. A dynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of this framework. We propose and study an algorithm to optimize the tradeoff of a convolutional network directly from lowlight images and evaluate on simulated images from standard datasets. Surprisingly, scotopic systems can achieve comparable classification performance as traditional vision systems while using less than 0.1% of the photons in a conventional image. In addition, we demonstrate that our algorithms work even when the illuminance of the environment is unknown and varying. Last, we outline a spiking neural network coupled with photon-counting sensors as a power-efficient hardware realization of scotopic algorithms.",http://arxiv.org/pdf/1610.00405v1
,,,,,,1676,Deep Co-Occurrence Feature Learning for Visual Object Recognition,"Ya-Fang Shih, Yang-Ming Yeh, Yen-Yu Lin, Ming-Fang Weng, Yi-Chang Lu, Yung-Yu Chuang",Learning to Associate Words and Images Using a Large-scale Graph,"We develop an approach for unsupervised learning of associations between co-occurring perceptual events using a large graph. We applied this approach to successfully solve the image captcha of China's railroad system. The approach is based on the principle of suspicious coincidence. In this particular problem, a user is presented with a deformed picture of a Chinese phrase and eight low-resolution images. They must quickly select the relevant images in order to purchase their train tickets. This problem presents several challenges: (1) the teaching labels for both the Chinese phrases and the images were not available for supervised learning, (2) no pre-trained deep convolutional neural networks are available for recognizing these Chinese phrases or the presented images, and (3) each captcha must be solved within a few seconds. We collected 2.6 million captchas, with 2.6 million deformed Chinese phrases and over 21 million images. From these data, we constructed an association graph, composed of over 6 million vertices, and linked these vertices based on co-occurrence information and feature similarity between pairs of images. We then trained a deep convolutional neural network to learn a projection of the Chinese phrases onto a 230-dimensional latent space. Using label propagation, we computed the likelihood of each of the eight images conditioned on the latent space projection of the deformed phrase for each captcha. The resulting system solved captchas with 77% accuracy in 2 seconds on average. Our work, in answering this practical challenge, illustrates the power of this class of unsupervised association learning techniques, which may be related to the brain's general strategy for associating language stimuli with visual objects on the principle of suspicious coincidence.",http://arxiv.org/pdf/1705.07768v1
,,,,,,1780,An Empirical Evaluation of Visual Question Answering for Novel Objects,"Santhosh K. Ramakrishnan, Ambar Pal, Gaurav Sharma, Anurag Mittal",An Empirical Evaluation of Visual Question Answering for Novel Objects,"We study the problem of answering questions about images in the harder setting, where the test questions and corresponding images contain novel objects, which were not queried about in the training data. Such setting is inevitable in real world-owing to the heavy tailed distribution of the visual categories, there would be some objects which would not be annotated in the train set. We show that the performance of two popular existing methods drop significantly (up to 28%) when evaluated on novel objects cf. known objects. We propose methods which use large existing external corpora of (i) unlabeled text, i.e. books, and (ii) images tagged with classes, to achieve novel object based visual question answering. We do systematic empirical studies, for both an oracle case where the novel objects are known textually, as well as a fully automatic case without any explicit knowledge of the novel objects, but with the minimal assumption that the novel objects are semantically related to the existing objects in training. The proposed methods for novel object based visual question answering are modular and can potentially be used with many visual question answering architectures. We show consistent improvements with the two popular architectures and give qualitative analysis of the cases where the model does well and of those where it fails to bring improvements.",http://arxiv.org/pdf/1704.02516v1
,,,,,,2064,InstanceCut: From Edges to Instances With MultiCut,"Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, Carsten Rother",InstanceCut: from Edges to Instances with MultiCut,"This work addresses the task of instance-aware semantic segmentation. Our key motivation is to design a simple method with a new modelling-paradigm, which therefore has a different trade-off between advantages and disadvantages compared to known approaches. Our approach, we term InstanceCut, represents the problem by two output modalities: (i) an instance-agnostic semantic segmentation and (ii) all instance-boundaries. The former is computed from a standard convolutional neural network for semantic segmentation, and the latter is derived from a new instance-aware edge detection model. To reason globally about the optimal partitioning of an image into instances, we combine these two modalities into a novel MultiCut formulation. We evaluate our approach on the challenging CityScapes dataset. Despite the conceptual simplicity of our approach, we achieve the best result among all published methods, and perform particularly well for rare object classes.",http://arxiv.org/pdf/1611.08272v1
,,,,,,2611,Fine-Grained Image Classification via Combining Vision and Language,"Xiangteng He, Yuxin Peng",Fine-graind Image Classification via Combining Vision and Language,"Fine-grained image classification is a challenging task due to the large intra-class variance and small inter-class variance, aiming at recognizing hundreds of sub-categories belonging to the same basic-level category. Most existing fine-grained image classification methods generally learn part detection models to obtain the semantic parts for better classification accuracy. Despite achieving promising results, these methods mainly have two limitations: (1) not all the parts which obtained through the part detection models are beneficial and indispensable for classification, and (2) fine-grained image classification requires more detailed visual descriptions which could not be provided by the part locations or attribute annotations. For addressing the above two limitations, this paper proposes the two-stream model combining vision and language (CVL) for learning latent semantic representations. The vision stream learns deep representations from the original visual information via deep convolutional neural network. The language stream utilizes the natural language descriptions which could point out the discriminative parts or characteristics for each image, and provides a flexible and compact way of encoding the salient visual aspects for distinguishing sub-categories. Since the two streams are complementary, combining the two streams can further achieves better classification accuracy. Comparing with 12 state-of-the-art methods on the widely used CUB-200-2011 dataset for fine-grained image classification, the experimental results demonstrate our CVL approach achieves the best performance.",http://arxiv.org/pdf/1704.02792v2
,,,,,,2820,Mimicking Very Efficient Network for Object Detection,"Quanquan Li, Shengying Jin, Junjie Yan",,,
,,,,,,2925,Tracking by Natural Language Specification,"Zhenyang Li, Ran Tao, Efstratios Gavves, Cees G. M. Snoek, Arnold W.M. Smeulders",Roadmap Enhanced Improvement to the VSIMM Tracker via a Constrained Stochastic Context Free Grammar,"The aim of syntactic tracking is to classify spatio-temporal patterns of a target's motion using natural language processing models. In this paper, we generalize earlier work by considering a constrained stochastic context free grammar (CSCFG) for modeling patterns confined to a roadmap. The constrained grammar facilitates modeling specific directions and road names in a roadmap. We present a novel particle filtering algorithm that exploits the CSCFG model for estimating the target's patterns. This meta-level algorithm operates in conjunction with a base-level tracking algorithm. Extensive numerical results using simulated ground moving target indicator (GMTI) radar measurements show substantial improvement in target tracking accuracy.",http://arxiv.org/pdf/1611.03466v1
,,,,,,3167,A Dataset and Exploration of Models for Understanding Video Data Through Fill-In-The-Blank Question-Answering,"Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, Christopher Pal",A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering,"While deep convolutional neural networks frequently approach or exceed human-level performance at benchmark tasks involving static images, extending this success to moving images is not straightforward. Having models which can learn to understand video is of interest for many applications, including content recommendation, prediction, summarization, event/object detection and understanding human visual perception, but many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluations for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgement. We suggest avenues for improving video models, and hope that the proposed dataset can be useful for measuring and encouraging progress in this very interesting field.",http://arxiv.org/pdf/1611.07810v2
,,,,,,3375,Learning Detection With Diverse Proposals,"Samaneh Azadi, Jiashi Feng, Trevor Darrell",Learning Detection with Diverse Proposals,"To predict a set of diverse and informative proposals with enriched representations, this paper introduces a differentiable Determinantal Point Process (DPP) layer that is able to augment the object detection architectures. Most modern object detection architectures, such as Faster R-CNN, learn to localize objects by minimizing deviations from the ground-truth but ignore correlation between multiple proposals and object categories. Non-Maximum Suppression (NMS) as a widely used proposal pruning scheme ignores label- and instance-level relations between object candidates resulting in multi-labeled detections. In the multi-class case, NMS selects boxes with the largest prediction scores ignoring the semantic relation between categories of potential election. In contrast, our trainable DPP layer, allowing for Learning Detection with Diverse Proposals (LDDP), considers both label-level contextual information and spatial layout relationships between proposals without increasing the number of parameters of the network, and thus improves location and category specifications of final detected bounding boxes substantially during both training and inference schemes. Furthermore, we show that LDDP keeps it superiority over Faster R-CNN even if the number of proposals generated by LDPP is only ~30% as many as those for Faster R-CNN.",http://arxiv.org/pdf/1704.03533v1
,,,,,,3491,Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition,"Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, Garrison W. Cottrell",Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition,"Recently, there has been a lot of interest in automatically generating descriptions for an image. Most existing language-model based approaches for this task learn to generate an image description word by word in its original word order. However, for humans, it is more natural to locate the objects and their relationships first, and then elaborate on each object, describing notable attributes. We present a coarse-to-fine method that decomposes the original image description into a skeleton sentence and its attributes, and generates the skeleton sentence and attribute phrases separately. By this decomposition, our method can generate more accurate and novel descriptions than the previous state-of-the-art. Experimental results on the MS-COCO and a larger scale Stock3M datasets show that our algorithm yields consistent improvements across different evaluation metrics, especially on the SPICE metric, which has much higher correlation with human ratings than the conventional metrics. Furthermore, our algorithm can generate descriptions with varied length, benefiting from the separate control of the skeleton and attributes. This enables image description generation that better accommodates user preferences.",http://arxiv.org/pdf/1704.06972v1
,,,,,Theory,3466,"A Low Power, Fully Event-Based Gesture Recognition System","Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, Jeff Kusnitz, Michael Debole, Steve Esser, Tobi Delbruck, Myron Flickner, Dharmendra Modha",,,
,,,,,Video Analytics,135,Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification,"Dangwei Li, Xiaotang Chen, Zhang Zhang, Kaiqi Huang",,,
,,,,,,1126,Recurrent Modeling of Interaction Context for Collective Activity Recognition,"Minsi Wang, Bingbing Ni, Xiaokang Yang",,,
,,,,,,1272,Primary Object Segmentation in Videos Based on Region Augmentation and Reduction,"Yeong Jun Koh, Chang-Su Kim",,,
,,,,,,1953,ROAM: A Rich Object Appearance Model With Application to Rotoscoping,"Ondrej Miksik, Juan-Manuel PÃ©rez-RÃºa, Philip H. S. Torr, Patrick PÃ©rez",ROAM: a Rich Object Appearance Model with Application to Rotoscoping,"Rotoscoping, the detailed delineation of scene elements through a video shot, is a painstaking task of tremendous importance in professional post-production pipelines. While pixel-wise segmentation techniques can help for this task, professional rotoscoping tools rely on parametric curves that offer the artists a much better interactive control on the definition, editing and manipulation of the segments of interest. Sticking to this prevalent rotoscoping paradigm, we propose a novel framework to capture and track the visual aspect of an arbitrary object in a scene, given a first closed outline of this object. This model combines a collection of local foreground/background appearance models spread along the outline, a global appearance model of the enclosed object and a set of distinctive foreground landmarks. The structure of this rich appearance model allows simple initialization, efficient iterative optimization with exact minimization at each step, and on-line adaptation in videos. We demonstrate qualitatively and quantitatively the merit of this framework through comparisons with tools based on either dynamic segmentation with a closed curve or pixel-wise binary labelling.",http://arxiv.org/pdf/1612.01495v1
,,,,,,1967,Temporal Residual Networks for Dynamic Scene Recognition,"Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes",,,
,,,,,,1982,Spatiotemporal Multiplier Networks for Video Action Recognition,"Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes",,,
,,,,,,2140,Learning to Learn From Noisy Web Videos,"Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg Mori, Li Fei-Fei",Exploiting Multi-modal Curriculum in Noisy Web Data for Large-scale Concept Learning,"Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web are associated with rich but noisy contextual information, such as the title, which provides weak annotations or labels about the video content. To leverage the big noisy web labels, this paper proposes a novel method called WEbly-Labeled Learning (WELL), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL introduces a number of novel multi-modal approaches to incorporate meaningful prior knowledge called curriculum from the noisy web videos. To investigate this problem, we empirically study the curriculum constructed from the multi-modal features of the videos collected from YouTube and Flickr. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. The comprehensive experimental results demonstrate that WELL outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL is robust to the level of noisiness in the video data. Notably, WELL trained on sufficient noisy web labels is able to achieve a comparable accuracy to supervised learning methods trained on the clean manually-labeled data.",http://arxiv.org/pdf/1607.04780v1
,,,,,,2227,YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video,"Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke",YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video,"We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization to provide a point of comparison for future work. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. Please see the PDF file to find the URL to download the data. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking.",http://arxiv.org/pdf/1702.00824v5
,,,,,,2517,Online Video Object Segmentation via Convolutional Trident Network,"Won-Dong Jang, Chang-Su Kim",,,