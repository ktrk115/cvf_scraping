Paper ID,Paper Title,Authors,Title-arxiv,Abstract,URL
2218,Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence,Dylan Campbell; Lars Petersson; Laurent Kneip; Hongdong Li,Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence,"Estimating the 6-DoF pose of a camera from a single image relative to a pre-computed 3D point-set is an important task for many computer vision applications. Perspective-n-Point (PnP) solvers are routinely used for camera pose estimation, provided that a good quality set of 2D-3D feature correspondences are known beforehand. However, finding optimal correspondences between 2D key-points and a 3D point-set is non-trivial, especially when only geometric (position) information is known. Existing approaches to the simultaneous pose and correspondence problem use local optimisation, and are therefore unlikely to find the optimal solution without a good pose initialisation, or introduce restrictive assumptions. Since a large proportion of outliers are common for this problem, we instead propose a globally-optimal inlier set cardinality maximisation approach which jointly estimates optimal camera pose and optimal correspondences. Our approach employs branch-and-bound to search the 6D space of camera poses, guaranteeing global optimality without requiring a pose prior. The geometry of SE(3) is used to find novel upper and lower bounds for the number of inliers and local optimisation is integrated to accelerate convergence. The evaluation empirically supports the optimality proof and shows that the method performs much more robustly than existing approaches, including on a large-scale outdoor data-set.",http://arxiv.org/pdf/1709.09384v1
212,Robust Pseudo Random Fields for Light-Field Stereo Matching,Chao-Tsung Huang,,,
1392,A Lightweight Approach for On-The-Fly Reflectance Estimation,Kihwan Kim; Jinwei Gu; Stephen Tyree; Pavlo Molchanov; Matthias Nie√üner; Jan Kautz,A Lightweight Approach for On-the-Fly Reflectance Estimation,"Estimating surface reflectance (BRDF) is one key component for complete 3D scene capture, with wide applications in virtual reality, augmented reality, and human computer interaction. Prior work is either limited to controlled environments (\eg gonioreflectometers, light stages, or multi-camera domes), or requires the joint optimization of shape, illumination, and reflectance, which is often computationally too expensive (\eg hours of running time) for real-time applications. Moreover, most prior work requires HDR images as input which further complicates the capture process. In this paper, we propose a lightweight approach for surface reflectance estimation directly from $8$-bit RGB images in real-time, which can be easily plugged into any 3D scanning-and-fusion system with a commodity RGBD sensor. Our method is learning-based, with an inference time of less than 90ms per scene and a model size of less than 340K bytes. We propose two novel network architectures, HemiCNN and Grouplet, to deal with the unstructured input data from multiple viewpoints under unknown illumination. We further design a loss function to resolve the color-constancy and scale ambiguity. In addition, we have created a large synthetic dataset, SynBRDF, which comprises a total of $500$K RGBD images rendered with a physically-based ray tracer under a variety of natural illumination, covering $5000$ materials and $5000$ shapes. SynBRDF is the first large-scale benchmark dataset for reflectance estimation. Experiments on both synthetic data and real data show that the proposed method effectively recovers surface reflectance, and outperforms prior work for reflectance estimation in uncontrolled environments.",http://arxiv.org/pdf/1705.07162v1
1019,Distributed Very Large Scale Bundle Adjustment by Global Camera Consensus,Runze Zhang; Siyu Zhu; Tian Fang; Long Quan,,,
532,Practical Projective Structure From Motion (P2SfM),Ludovic Magerand; Alessio Del Bue,,,
316,Anticipating Daily Intention Using On-Wrist Motion Triggered Sensing,Tz-Ying Wu; Ting-An Chien; Cheng-Sheng Chan; Chan-Wei Hu; Min Sun,,,
785,Rethinking Reprojection: Closing the Loop for Pose-Aware Shape Reconstruction From a Single Image,Rui Zhu; Hamed Kiani Galoogahi; Chaoyang Wang; Simon Lucey,Rethinking Reprojection: Closing the Loop for Pose-aware ShapeReconstruction from a Single Image,"An emerging problem in computer vision is the reconstruction of 3D shape and pose of an object from a single image. Hitherto, the problem has been addressed through the application of canonical deep learning methods to regress from the image directly to the 3D shape and pose labels. These approaches, however, are problematic from two perspectives. First, they are minimizing the error between 3D shapes and pose labels - with little thought about the nature of this label error when reprojecting the shape back onto the image. Second, they rely on the onerous and ill-posed task of hand labeling natural images with respect to 3D shape and pose. In this paper we define the new task of pose-aware shape reconstruction from a single image, and we advocate that cheaper 2D annotations of objects silhouettes in natural images can be utilized. We design architectures of pose-aware shape reconstruction which re-project the predicted shape back on to the image using the predicted pose. Our evaluation on several object categories demonstrates the superiority of our method for predicting pose-aware 3D shapes from natural images.",http://arxiv.org/pdf/1707.04682v2
1007,End-To-End Learning of Geometry and Context for Deep Stereo Regression,Alex Kendall; Hayk Martirosyan; Saumitro Dasgupta; Peter Henry; Ryan Kennedy; Abraham Bachrach; Adam Bry,End-to-End Learning of Geometry and Context for Deep Stereo Regression,"We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem's geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new state-of-the-art benchmark, while being significantly faster than competing approaches.",http://arxiv.org/pdf/1703.04309v1
1399,Using Sparse Elimination for Solving Minimal Problems in Computer Vision,Janne Heikkil√§,,,
1474,High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference,Xiaoguang Han; Zhen Li; Haibin Huang; Evangelos Kalogerakis; Yizhou Yu,High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference,"We propose a data-driven method for recovering miss-ing parts of 3D shapes. Our method is based on a new deep learning architecture consisting of two sub-networks: a global structure inference network and a local geometry refinement network. The global structure inference network incorporates a long short-term memorized context fusion module (LSTM-CF) that infers the global structure of the shape based on multi-view depth information provided as part of the input. It also includes a 3D fully convolutional (3DFCN) module that further enriches the global structure representation according to volumetric information in the input. Under the guidance of the global structure network, the local geometry refinement network takes as input lo-cal 3D patches around missing regions, and progressively produces a high-resolution, complete surface through a volumetric encoder-decoder architecture. Our method jointly trains the global structure inference and local geometry refinement networks in an end-to-end manner. We perform qualitative and quantitative evaluations on six object categories, demonstrating that our method outperforms existing state-of-the-art work on shape completion.",http://arxiv.org/pdf/1709.07599v1
1712,Temporal Tessellation: A Unified Approach for Video Analysis,Dotan Kaufman; Gil Levi; Tal Hassner; Lior Wolf,Temporal Tessellation: A Unified Approach for Video Analysis,"We present a general approach to video understanding, inspired by semantic transfer techniques that have been successfully used for 2D image analysis. Our method considers a video to be a 1D sequence of clips, each one associated with its own semantics. The nature of these semantics -- natural language captions or other labels -- depends on the task at hand. A test video is processed by forming correspondences between its clips and the clips of reference videos with known semantics, following which, reference semantics can be transferred to the test video. We describe two matching methods, both designed to ensure that (a) reference clips appear similar to test clips and (b), taken together, the semantics of the selected reference clips is consistent and maintains temporal coherence. We use our method for video captioning on the LSMDC'16 benchmark, video summarization on the SumMe and TVSum benchmarks, Temporal Action Detection on the Thumos2014 benchmark, and sound prediction on the Greatest Hits benchmark. Our method not only surpasses the state of the art, in four out of five benchmarks, but importantly, it is the only single method we know of that was successfully applied to such a diverse range of tasks.",http://arxiv.org/pdf/1612.06950v2
1976,Learning Policies for Adaptive Tracking With Deep Feature Cascades,Chen Huang; Simon Lucey; Deva Ramanan,Learning Policies for Adaptive Tracking with Deep Feature Cascades,"Visual object tracking is a fundamental and time-critical vision task. Recent years have seen many shallow tracking methods based on real-time pixel-based correlation filters, as well as deep methods that have top performance but need a high-end GPU. In this paper, we learn to improve the speed of deep trackers without losing accuracy. Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features. We formulate the adaptive tracking problem as a decision-making process, and learn an agent to decide whether to locate objects with high confidence on an early layer, or continue processing subsequent layers of a network. This significantly reduces the feed-forward cost for easy frames with distinct or slow-moving objects. We train the agent offline in a reinforcement learning fashion, and further demonstrate that learning all deep layers (so as to provide good features for adaptive tracking) can lead to near real-time average tracking speed of 23 fps on a single CPU while achieving state-of-the-art performance. Perhaps most tellingly, our approach provides a 100X speedup for almost 50% of the time, indicating the power of an adaptive approach.",http://arxiv.org/pdf/1708.02973v2
2216,Temporal Shape Super-Resolution by Intra-Frame Motion Encoding Using High-Fps Structured Light,Yuki Shiba; Satoshi Ono; Ryo Furukawa; Shinsaku Hiura; Hiroshi Kawasaki,Temporal shape super-resolution by intra-frame motion encoding using high-fps structured light,"One of the solutions of depth imaging of moving scene is to project a static pattern on the object and use just a single image for reconstruction. However, if the motion of the object is too fast with respect to the exposure time of the image sensor, patterns on the captured image are blurred and reconstruction fails. In this paper, we impose multiple projection patterns into each single captured image to realize temporal super resolution of the depth image sequences. With our method, multiple patterns are projected onto the object with higher fps than possible with a camera. In this case, the observed pattern varies depending on the depth and motion of the object, so we can extract temporal information of the scene from each single image. The decoding process is realized using a learning-based approach where no geometric calibration is needed. Experiments confirm the effectiveness of our method where sequential shapes are reconstructed from a single image. Both quantitative evaluations and comparisons with recent techniques were also conducted.",http://arxiv.org/pdf/1710.00517v1
6,Real-Time Monocular Pose Estimation of 3D Objects Using Temporally Consistent Local Color Histograms,Henning Tjaden; Ulrich Schwanecke; Elmar Sch√∂mer,,,
79,CAD Priors for Accurate and Flexible Instance Reconstruction,Tolga Birdal; Slobodan Ilic,CAD Priors for Accurate and Flexible Instance Reconstruction,"We present an efficient and automatic approach for accurate reconstruction of instances of big 3D objects from multiple, unorganized and unstructured point clouds, in presence of dynamic clutter and occlusions. In contrast to conventional scanning, where the background is assumed to be rather static, we aim at handling dynamic clutter where background drastically changes during the object scanning. Currently, it is tedious to solve this with available methods unless the object of interest is first segmented out from the rest of the scene. We address the problem by assuming the availability of a prior CAD model, roughly resembling the object to be reconstructed. This assumption almost always holds in applications such as industrial inspection or reverse engineering. With aid of this prior acting as a proxy, we propose a fully enhanced pipeline, capable of automatically detecting and segmenting the object of interest from scenes and creating a pose graph, online, with linear complexity. This allows initial scan alignment to the CAD model space, which is then refined without the CAD constraint to fully recover a high fidelity 3D reconstruction, accurate up to the sensor noise level. We also contribute a novel object detection method, local implicit shape models (LISM) and give a fast verification scheme. We evaluate our method on multiple datasets, demonstrating the ability to accurately reconstruct objects from small sizes up to $125m^3$.",http://arxiv.org/pdf/1705.03111v2
183,Colored Point Cloud Registration Revisited,Jaesik Park; Qian-Yi Zhou; Vladlen Koltun,,,
254,Learning Compact Geometric Features,Marc Khoury; Qian-Yi Zhou; Vladlen Koltun,Learning Compact Geometric Features,"We present an approach to learning features that represent the local geometry around a point in an unstructured point cloud. Such features play a central role in geometric registration, which supports diverse applications in robotics and 3D vision. Current state-of-the-art local features for unstructured point clouds have been manually crafted and none combines the desirable properties of precision, compactness, and robustness. We show that features with these properties can be learned from data, by optimizing deep networks that map high-dimensional histograms into low-dimensional Euclidean spaces. The presented approach yields a family of features, parameterized by dimension, that are both more compact and more accurate than existing descriptors.",http://arxiv.org/pdf/1709.05056v1
305,Joint Layout Estimation and Global Multi-View Registration for Indoor Reconstruction,Jeong-Kyun Lee; Jaewon Yea; Min-Gyu Park; Kuk-Jin Yoon,Joint Layout Estimation and Global Multi-View Registration for Indoor Reconstruction,"In this paper, we propose a novel method to jointly solve scene layout estimation and global registration problems for accurate indoor 3D reconstruction. Given a sequence of range data, we first build a set of scene fragments using KinectFusion and register them through pose graph optimization. Afterwards, we alternate between layout estimation and layout-based global registration processes in iterative fashion to complement each other. We extract the scene layout through hierarchical agglomerative clustering and energy-based multi-model fitting in consideration of noisy measurements. Having the estimated scene layout in one hand, we register all the range data through the global iterative closest point algorithm where the positions of 3D points that belong to the layout such as walls and a ceiling are constrained to be close to the layout. We experimentally verify the proposed method with the publicly available synthetic and real-world datasets in both quantitative and qualitative ways.",http://arxiv.org/pdf/1704.07632v2
168,A Geometric Framework for Statistical Analysis of Trajectories With Distinct Temporal Spans,Rudrasis Chakraborty; Vikas Singh; Nagesh Adluru; Baba C. Vemuri,,,
246,An Optimal Transportation Based Univariate Neuroimaging Index,Liang Mi; Wen Zhang; Junwei Zhang; Yonghui Fan; Dhruman Goradia; Kewei Chen; Eric M. Reiman; Xianfeng Gu; Yalin Wang,,,
22,S3FD: Single Shot Scale-Invariant Face Detector,Shifeng Zhang; Xiangyu Zhu; Zhen Lei; Hailin Shi; Xiaobo Wang; Stan Z. Li,,,
102,Amulet: Aggregating Multi-Level Convolutional Features for Salient Object Detection,Pingping Zhang; Dong Wang; Huchuan Lu; Hongyu Wang; Xiang Ruan,Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection,"Fully convolutional neural networks (FCNs) have shown outstanding performance in many dense labeling problems. One key pillar of these successes is mining relevant information from features in convolutional layers. However, how to better aggregate multi-level convolutional feature maps for salient object detection is underexplored. In this work, we present Amulet, a generic aggregating multi-level convolutional feature framework for salient object detection. Our framework first integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and fine details. Then it adaptively learns to combine these feature maps at each resolution and predict saliency maps with the combined features. Finally, the predicted results are efficiently fused to generate the final saliency map. In addition, to achieve accurate boundary inference and semantic enhancement, edge-aware feature maps in low-level layers and the predicted results of low resolution features are recursively embedded into the learning framework. By aggregating multi-level convolutional features in this efficient and flexible manner, the proposed saliency model provides accurate salient object labeling. Comprehensive experiments demonstrate that our method performs favorably against state-of-the art approaches in terms of near all compared evaluation metrics.",http://arxiv.org/pdf/1708.02001v1
146,Learning Uncertain Convolutional Features for Accurate Saliency Detection,Pingping Zhang; Dong Wang; Huchuan Lu; Hongyu Wang; Baocai Yin,Learning Uncertain Convolutional Features for Accurate Saliency Detection,"Deep convolutional neural networks (CNNs) have delivered superior performance in many computer vision tasks. In this paper, we propose a novel deep fully convolutional network model for accurate salient object detection. The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection. We achieve this via introducing a reformulated dropout (R-dropout) after specific convolutional layers to construct an uncertain ensemble of internal feature units. In addition, we propose an effective hybrid upsampling method to reduce the checkerboard artifacts of deconvolution operators in our decoder network. The proposed methods can also be applied to other deep convolutional networks. Compared with existing saliency detection methods, the proposed UCF model is able to incorporate uncertainties for more accurate object boundary inference. Extensive experiments demonstrate that our proposed saliency model performs favorably against state-of-the-art approaches. The uncertain feature learning mechanism as well as the upsampling method can significantly improve performance on other pixel-wise vision tasks.",http://arxiv.org/pdf/1708.02031v1
218,Zero-Order Reverse Filtering,Xin Tao; Chao Zhou; Xiaoyong Shen; Jue Wang; Jiaya Jia,Zero-order Reverse Filtering,"In this paper, we study an unconventional but practically meaningful reversibility problem of commonly used image filters. We broadly define filters as operations to smooth images or to produce layers via global or local algorithms. And we raise the intriguingly problem if they are reservable to the status before filtering. To answer it, we present a novel strategy to understand general filter via contraction mappings on a metric space. A very simple yet effective zero-order algorithm is proposed. It is able to practically reverse most filters with low computational cost. We present quite a few experiments in the paper and supplementary file to thoroughly verify its performance. This method can also be generalized to solve other inverse problems and enables new applications.",http://arxiv.org/pdf/1704.04037v1
237,Learning Blind Motion Deblurring,Patrick Wieschollek; Michael Hirsch; Bernhard Sch√∂lkopf; Hendrik P. A. Lensch,Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring,"Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem since blurs are caused by camera shake, scene depth as well as multiple object motions. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores blurred images caused by various sources in an end-to-end manner. Furthermore, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Moreover, we propose a new large scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.",http://arxiv.org/pdf/1612.02177v1
252,Joint Adaptive Sparsity and Low-Rankness on the Fly: An Online Tensor Reconstruction Scheme for Video Denoising,Bihan Wen; Yanjun Li; Luke Pfister; Yoram Bresler,,,
286,Learning to Super-Resolve Blurry Face and Text Images,Xiangyu Xu; Deqing Sun; Jinshan Pan; Yujin Zhang; Hanspeter Pfister; Ming-Hsuan Yang,,,
310,Video Frame Interpolation via Adaptive Separable Convolution,Simon Niklaus; Long Mai; Feng Liu,Video Frame Interpolation via Adaptive Separable Convolution,"Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.",http://arxiv.org/pdf/1708.01692v1
633,Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection,Pierre Baqu√©; Fran√ßois Fleuret; Pascal Fua,Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection,"People detection in single 2D images has improved greatly in recent years. However, comparatively little of this progress has percolated into multi-camera multi-people tracking algorithms, whose performance still degrades severely when scenes become very crowded. In this work, we introduce a new architecture that combines Convolutional Neural Nets and Conditional Random Fields to explicitly model those ambiguities. One of its key ingredients are high-order CRF terms that model potential occlusions and give our approach its robustness even when many people are present. Our model is trained end-to-end and we show that it outperforms several state-of-art algorithms on challenging scenes.",http://arxiv.org/pdf/1704.05775v2
181,Encouraging LSTMs to Anticipate Actions Very Early,Mohammad Sadegh Aliakbarian; Fatemeh Sadat Saleh; Mathieu Salzmann; Basura Fernando; Lars Petersson; Lars Andersson,Encouraging LSTMs to Anticipate Actions Very Early,"In contrast to the widely studied problem of recognizing an action given a complete sequence, action anticipation aims to identify the action from only partially available videos. As such, it is therefore key to the success of computer vision applications requiring to react as early as possible, such as autonomous navigation. In this paper, we propose a new action anticipation method that achieves high prediction accuracy even in the presence of a very small percentage of a video sequence. To this end, we develop a multi-stage LSTM architecture that leverages context-aware and action-aware features, and introduce a novel loss function that encourages the model to predict the correct class as early as possible. Our experiments on standard benchmark datasets evidence the benefits of our approach; We outperform the state-of-the-art action anticipation methods for early prediction by a relative increase in accuracy of 22.0% on JHMDB-21, 14.0% on UT-Interaction and 49.9% on UCF-101.",http://arxiv.org/pdf/1703.07023v3
226,PathTrack: Fast Trajectory Annotation With Path Supervision,Santiago Manen; Michael Gygli; Dengxin Dai; Luc Van Gool,PathTrack: Fast Trajectory Annotation with Path Supervision,"Progress in Multiple Object Tracking (MOT) has been historically limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. In our novel path supervision the annotator loosely follows the object with the cursor while watching the video, providing a path annotation for each object in the sequence. Our approach is able to turn such weak annotations into dense box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art, in a fraction of the time. We further validate our approach by crowdsourcing the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences. Tracking approaches can benefit training on such large-scale datasets, as did object recognition. We prove this by re-training an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. On the latter, we improve the top-performing tracker (NOMT) dropping the number of IDSwitches by 18% and fragments by 5%.",http://arxiv.org/pdf/1703.02437v2
228,Tracking the Untrackable: Learning to Track Multiple Cues With Long-Term Dependencies,Amir Sadeghian; Alexandre Alahi; Silvio Savarese,Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies,"The majority of existing solutions to the Multi-Target Tracking (MTT) problem do not combine cues in a coherent end-to-end fashion over a long period of time. However, we present an online method that encodes long-term temporal dependencies across multiple cues. One key challenge of tracking methods is to accurately track occluded targets or those which share similar appearance properties with surrounding objects. To address this challenge, we present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window. We are able to correct many data association errors and recover observations from an occluded state. We demonstrate the robustness of our data-driven approach by tracking multiple targets using their appearance, motion, and even interactions. Our method outperforms previous works on multiple publicly available datasets including the challenging MOT benchmark.",http://arxiv.org/pdf/1701.01909v2
230,MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation,Junhwa Hur; Stefan Roth,MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation,"Optical flow estimation is one of the most studied problems in computer vision, yet recent benchmark datasets continue to reveal problem areas of today's approaches. Occlusions have remained one of the key challenges. In this paper, we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions. In contrast to many state-of-the-art methods that consider occlusions as outliers, possibly filtered out during post-processing, we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow. The key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images. Specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model jointly estimates optical flow in both forward and backward direction, as well as consistent occlusion maps in both views. We demonstrate significant performance benefits on standard benchmarks, especially from the occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the most accurate two-frame results to date.",http://arxiv.org/pdf/1708.05355v1
266,Tracking as Online Decision-Making: Learning a Policy From Streaming Videos With Reinforcement Learning,"James Supanƒçiƒ, III; Deva Ramanan",Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning,"We formulate tracking as an online decision-making process, where a tracking agent must follow an object despite ambiguous image frames and a limited computational budget. Crucially, the agent must decide where to look in the upcoming frames, when to reinitialize because it believes the target has been lost, and when to update its appearance model for the tracked object. Such decisions are typically made heuristically. Instead, we propose to learn an optimal decision-making policy by formulating tracking as a partially observable decision-making process (POMDP). We learn policies with deep reinforcement learning algorithms that need supervision (a reward signal) only when the track has gone awry. We demonstrate that sparse rewards allow us to quickly train on massive datasets, several orders of magnitude more than past work. Interestingly, by treating the data source of Internet videos as unlimited streams, we both learn and evaluate our trackers in a single, unified computational stream.",http://arxiv.org/pdf/1707.04991v1
60,Non-Convex Rank/Sparsity Regularization and Local Minima,Carl Olsson; Marcus Carlsson; Fredrik Andersson; Viktor Larsson,Non-Convex Rank/Sparsity Regularization and Local Minima,"This paper considers the problem of recovering either a low rank matrix or a sparse vector from observations of linear combinations of the vector or matrix elements. Recent methods replace the non-convex regularization with $\ell_1$ or nuclear norm relaxations. It is well known that this approach can be guaranteed to recover a near optimal solutions if a so called restricted isometry property (RIP) holds. On the other hand it is also known to perform soft thresholding which results in a shrinking bias which can degrade the solution.   In this paper we study an alternative non-convex regularization term. This formulation does not penalize elements that are larger than a certain threshold making it much less prone to small solutions. Our main theoretical results show that if a RIP holds then the stationary points are often well separated, in the sense that their differences must be of high cardinality/rank. Thus, with a suitable initial solution the approach is unlikely to fall into a bad local minima. Our numerical tests show that the approach is likely to converge to a better solution than standard $\ell_1$/nuclear-norm relaxation even when starting from trivial initializations. In many cases our results can also be used to verify global optimality of our method.",http://arxiv.org/pdf/1703.07171v1
62,A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework,Weixin Luo; Wen Liu; Shenghua Gao,,,
23,HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis,Xihui Liu; Haiyu Zhao; Maoqing Tian; Lu Sheng; Jing Shao; Shuai Yi; Junjie Yan; Xiaogang Wang,HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis,"Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attention-based deep neural network, named as HydraPlus-Net (HP-net), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person re-identification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-the-art methods on various datasets.",http://arxiv.org/pdf/1709.09930v1
48,No Fuss Distance Metric Learning Using Proxies,Yair Movshovitz-Attias; Alexander Toshev; Thomas K. Leung; Sergey Ioffe; Saurabh Singh,No Fuss Distance Metric Learning using Proxies,"We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point $x$ is similar to a set of positive points $Y$, and dissimilar to a set of negative points $Z$, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.",http://arxiv.org/pdf/1703.07464v3
67,Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation,Matteo Ruggero Ronchi; Pietro Perona,Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation,"We propose a new method to analyze the impact of errors in algorithms for multi-instance pose estimation and a principled benchmark that can be used to compare them. We define and characterize three classes of errors - localization, scoring, and background - study how they are influenced by instance attributes and their impact on an algorithm's performance. Our technique is applied to compare the two leading methods for human pose estimation on the COCO Dataset, measure the sensitivity of pose estimation with respect to instance size, type and number of visible keypoints, clutter due to multiple instances, and the relative score of instances. The performance of algorithms, and the types of error they make, are highly dependent on all these variables, but mostly on the number of keypoints and the clutter. The analysis and software tools we propose offer a novel and insightful approach for understanding the behavior of pose estimation algorithms and an effective method for measuring their strengths and weaknesses.",http://arxiv.org/pdf/1707.05388v2
96,Orientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-Identification,Zhongdao Wang; Luming Tang; Xihui Liu; Zhuliang Yao; Shuai Yi; Jing Shao; Junjie Yan; Shengjin Wang; Hongsheng Li; Xiaogang Wang,,,
110,Fashion Forward: Forecasting Visual Style in Fashion,Ziad Al-Halah; Rainer Stiefelhagen; Kristen Grauman,Fashion Forward: Forecasting Visual Style in Fashion,"What is the future of fashion? Tackling this question from a data-driven vision perspective, we propose to forecast visual style trends before they occur. We introduce the first approach to predict the future popularity of styles discovered from fashion images in an unsupervised manner. Using these styles as a basis, we train a forecasting model to represent their trends over time. The resulting model can hypothesize new mixtures of styles that will become popular in the future, discover style dynamics (trendy vs. classic), and name the key visual attributes that will dominate tomorrow's fashion. We demonstrate our idea applied to three datasets encapsulating 80,000 fashion products sold across six years on Amazon. Results indicate that fashion forecasting benefits greatly from visual analysis, much more than textual or meta-data cues surrounding products.",http://arxiv.org/pdf/1705.06394v2
119,Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach,Xingyi Zhou; Qixing Huang; Xiao Sun; Xiangyang Xue; Yichen Wei,Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach,"In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.   We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep neutral network that presents two-stage cascaded structure. Our network augments a state-of-the-art 2D pose estimation sub-network with a 3D depth regression sub-network. Unlike previous two stage approaches that train the two sub-networks sequentially and separately, our training is end-to-end and fully exploits the correlation between the 2D pose and depth estimation sub-tasks. The deep features are better learnt through shared representations. In doing so, the 3D pose labels in controlled lab environments are transferred to in the wild images. In addition, we introduce a 3D geometric constraint to regularize the 3D pose prediction, which is effective in the absence of ground truth depth labels. Our method achieves competitive results on both 2D and 3D benchmarks.",http://arxiv.org/pdf/1704.02447v2
132,Flow-Guided Feature Aggregation for Video Object Detection,Xizhou Zhu; Yujie Wang; Jifeng Dai; Lu Yuan; Yichen Wei,Flow-Guided Feature Aggregation for Video Object Detection,"Extending state-of-the-art object detectors from image to video is challenging. The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc. Existing work attempts to exploit temporal information on box level, but such methods are not trained end-to-end. We present flow-guided feature aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead. It improves the per-frame features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy. Our method significantly improves upon strong single-frame baselines in ImageNet VID, especially for more challenging fast moving objects. Our framework is principled, and on par with the best engineered systems winning the ImageNet VID challenges 2016, without additional bells-and-whistles. The proposed method, together with Deep Feature Flow, powered the winning entry of ImageNet VID challenges 2017. The code is available at https://github.com/msracver/Flow-Guided-Feature-Aggregation.",http://arxiv.org/pdf/1703.10025v2
139,Reasoning About Fine-Grained Attribute Phrases Using Reference Games,Jong-Chyi Su; Chenyun Wu; Huaizu Jiang; Subhransu Maji,Reasoning about Fine-grained Attribute Phrases using Reference Games,"We present a framework for learning to describe fine-grained visual differences between instances using attribute phrases. Attribute phrases capture distinguishing aspects of an object (e.g., ""propeller on the nose"" or ""door near the wing"" for airplanes) in a compositional manner. Instances within a category can be described by a set of these phrases and collectively they span the space of semantic attributes for a category. We collect a large dataset of such phrases by asking annotators to describe several visual differences between a pair of instances within a category. We then learn to describe and ground these phrases to images in the context of a *reference game* between a speaker and a listener. The goal of a speaker is to describe attributes of an image that allows the listener to correctly identify it within a pair. Data collected in a pairwise manner improves the ability of the speaker to generate, and the ability of the listener to interpret visual descriptions. Moreover, due to the compositionality of attribute phrases, the trained listeners can interpret descriptions not seen during training for image retrieval, and the speakers can generate attribute-based explanations for differences between previously unseen categories. We also show that embedding an image into the semantic space of attribute phrases derived from listeners offers 20% improvement in accuracy over existing attribute-based representations on the FGVC-aircraft dataset.",http://arxiv.org/pdf/1708.08874v1
142,DeNet: Scalable Real-Time Object Detection With Directed Sparse Sampling,Lachlan Tychsen-Smith; Lars Petersson,DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling,"We define the object detection from imagery problem as estimating a very large but extremely sparse bounding box dependent probability distribution. Subsequently we identify a sparse distribution estimation scheme, Directed Sparse Sampling, and employ it in a single end-to-end CNN based detection model. This methodology extends and formalizes previous state-of-the-art detection models with an additional emphasis on high evaluation rates and reduced manual engineering. We introduce two novelties, a corner based region-of-interest estimator and a deconvolution based CNN model. The resulting model is scene adaptive, does not require manually defined reference bounding boxes and produces highly competitive results on MSCOCO, Pascal VOC 2007 and Pascal VOC 2012 with real-time evaluation rates. Further analysis suggests our model performs particularly well when finegrained object localization is desirable. We argue that this advantage stems from the significantly larger set of available regions-of-interest relative to other methods. Source-code is available from: https://github.com/lachlants/denet",http://arxiv.org/pdf/1703.10295v3
158,MIHash: Online Hashing With Mutual Information,Fatih Cakir; Kun He; Sarah Adel Bargal; Stan Sclaroff,MIHash: Online Hashing with Mutual Information,"Learning-based hashing methods are widely used for nearest neighbor retrieval, and recently, online hashing methods have demonstrated good performance-complexity trade-offs by learning hash functions from streaming data. In this paper, we first address a key challenge for online hashing: the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions, based on an information-theoretic quantity, mutual information, and use it successfully as a criterion to eliminate unnecessary hash table updates. Next, we also show how to optimize the mutual information objective using stochastic gradient descent. We thus develop a novel hashing method, MIHash, that can be used in both online and batch settings. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation, both in reducing hash table recomputations and in learning high-quality hash functions.",http://arxiv.org/pdf/1703.08919v2
166,SafetyNet: Detecting and Rejecting Adversarial Examples Robustly,Jiajun Lu; Theerasit Issaranon; David Forsyth,SafetyNet: Detecting and Rejecting Adversarial Examples Robustly,"We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat with both Type I and Type II attacks using several standard networks and datasets. This SafetyNet architecture is used to an important and novel application SceneProof, which can reliably detect whether an image is a picture of a real scene or not. SceneProof applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our SafetyNet is robust to adversarial examples built from currently known attacking approaches.",http://arxiv.org/pdf/1704.00103v2
195,Recurrent Models for Situation Recognition,Arun Mallya; Svetlana Lazebnik,Recurrent Models for Situation Recognition,"This work proposes Recurrent Neural Network (RNN) models to predict structured 'image situations' -- actions and noun entities fulfilling semantic roles related to the action. In contrast to prior work relying on Conditional Random Fields (CRFs), we use a specialized action prediction network followed by an RNN for noun prediction. Our system obtains state-of-the-art accuracy on the challenging recent imSitu dataset, beating CRF-based models, including ones trained with additional data. Further, we show that specialized features learned from situation prediction can be transferred to the task of image captioning to more accurately describe human-object interactions.",http://arxiv.org/pdf/1703.06233v2
1283,Multi-Label Image Recognition by Recurrently Discovering Attentional Regions,Zhouxia Wang; Tianshui Chen; Guanbin Li; Ruijia Xu; Liang Lin,,,
200,Deep Determinantal Point Process for Large-Scale Multi-Label Classification,Pengtao Xie; Ruslan Salakhutdinov; Luntian Mou; Eric P. Xing,,,
202,Visual Semantic Planning Using Deep Successor Representations,Yuke Zhu; Daniel Gordon; Eric Kolve; Dieter Fox; Li Fei-Fei; Abhinav Gupta; Roozbeh Mottaghi; Ali Farhadi,Visual Semantic Planning using Deep Successor Representations,"A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.",http://arxiv.org/pdf/1705.08080v2
216,Neural Person Search Machines,Hao Liu; Jiashi Feng; Zequn Jie; Karlekar Jayashree; Bo Zhao; Meibin Qi; Jianguo Jiang; Shuicheng Yan,Neural Person Search Machines,"We investigate the problem of person search in the wild in this work. Instead of comparing the query against all candidate regions generated in a query-blind manner, we propose to recursively shrink the search area from the whole image till achieving precise localization of the target person, by fully exploiting information from the query and contextual cues in every recursive search step. We develop the Neural Person Search Machines (NPSM) to implement such recursive localization for person search. Benefiting from its neural search mechanism, NPSM is able to selectively shrink its focus from a loose region to a tighter one containing the target automatically. In this process, NPSM employs an internal primitive memory component to memorize the query representation which modulates the attention and augments its robustness to other distracting regions. Evaluations on two benchmark datasets, CUHK-SYSU Person Search dataset and PRW dataset, have demonstrated that our method can outperform current state-of-the-arts in both mAP and top-1 evaluation protocols.",http://arxiv.org/pdf/1707.06777v1
227,DualNet: Learn Complementary Features for Image Recognition,Saihui Hou; Xu Liu; Zilei Wang,,,
240,Higher-Order Integration of Hierarchical Convolutional Activations for Fine-Grained Visual Categorization,Sijia Cai; Wangmeng Zuo; Lei Zhang,,,
248,"Show, Adapt and Tell: Adversarial Training of Cross-Domain Image Captioner",Tseng-Hung Chen; Yuan-Hong Liao; Ching-Yao Chuang; Wan-Ting Hsu; Jianlong Fu; Min Sun,"Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner","Impressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries -- captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.",http://arxiv.org/pdf/1705.00930v2
262,Attribute Recognition by Joint Recurrent Learning of Context and Correlation,Jingya Wang; Xiatian Zhu; Shaogang Gong; Wei Li,Attribute Recognition by Joint Recurrent Learning of Context and Correlation,"Recognising semantic pedestrian attributes in surveillance images is a challenging task for computer vision, particularly when the imaging quality is poor with complex background clutter and uncontrolled viewing conditions, and the number of labelled training data is small. In this work, we formulate a Joint Recurrent Learning (JRL) model for exploring attribute context and correlation in order to improve attribute recognition given small sized training data with poor quality images. The JRL model learns jointly pedestrian attribute correlations in a pedestrian image and in particular their sequential ordering dependencies (latent high-order correlation) in an end-to-end encoder/decoder recurrent network. We demonstrate the performance advantage and robustness of the JRL model over a wide range of state-of-the-art deep models for pedestrian attribute recognition, multi-label image classification, and multi-person image annotation on two largest pedestrian attribute benchmarks PETA and RAP.",http://arxiv.org/pdf/1709.08553v1
274,VegFru: A Domain-Specific Dataset for Fine-Grained Visual Categorization,Saihui Hou; Yushan Feng; Zilei Wang,,,
306,Increasing CNN Robustness to Occlusions by Reducing Filter Support,Elad Osherov; Michael Lindenbaum,,,
330,Exploiting Multi-Grain Ranking Constraints for Precisely Searching Visually-Similar Vehicles,Ke Yan; Yonghong Tian; Yaowei Wang; Wei Zeng; Tiejun Huang,,,
331,Recurrent Scale Approximation for Object Detection in CNN,Yu Liu; Hongyang Li; Junjie Yan; Fangyin Wei; Xiaogang Wang; Xiaoou Tang,Recurrent Scale Approximation for Object Detection in CNN,"Since convolutional neural network (CNN) lacks an inherent mechanism to handle large scale variations, we always need to compute feature maps multiple times for multi-scale object detection, which has the bottleneck of computational cost in practice. To address this, we devise a recurrent scale approximation (RSA) to compute feature map once only, and only through this map can we approximate the rest maps on other levels. At the core of RSA is the recursive rolling out mechanism: given an initial map on a particular scale, it generates the prediction on a smaller scale that is half the size of input. To further increase efficiency and accuracy, we (a): design a scale-forecast network to globally predict potential scales in the image since there is no need to compute maps on all levels of the pyramid. (b): propose a landmark retracing network (LRN) to retrace back locations of the regressed landmarks and generate a confidence score for each landmark; LRN can effectively alleviate false positives due to the accumulated error in RSA. The whole system could be trained end-to-end in a unified CNN framework. Experiments demonstrate that our proposed algorithm is superior against state-of-the-arts on face detection benchmarks and achieves comparable results for generic proposal generation. The source code of RSA is available at github.com/sciencefans/RSA-for-object-detection.",http://arxiv.org/pdf/1707.09531v1
163,Embedding 3D Geometric Features for Rigid Object Part Segmentation,Yafei Song; Xiaowu Chen; Jia Li; Qinping Zhao,,,
100,Towards Context-Aware Interaction Recognition for Visual Relationship Detection,Bohan Zhuang; Lingqiao Liu; Chunhua Shen; Ian Reid,,,
122,When Unsupervised Domain Adaptation Meets Tensor Representations,Hao Lu; Lei Zhang; Zhiguo Cao; Wei Wei; Ke Xian; Chunhua Shen; Anton van den Hengel,When Unsupervised Domain Adaptation Meets Tensor Representations,"Domain adaption (DA) allows machine learning methods trained on data sampled from one distribution to be applied to data sampled from another. It is thus of great practical importance to the application of such methods. Despite the fact that tensor representations are widely used in Computer Vision to capture multi-linear relationships that affect the data, most existing DA methods are applicable to vectors only. This renders them incapable of reflecting and preserving important structure in many problems. We thus propose here a learning-based method to adapt the source and target tensor representations directly, without vectorization. In particular, a set of alignment matrices is introduced to align the tensor representations from both domains into the invariant tensor subspace. These alignment matrices and the tensor subspace are modeled as a joint optimization problem and can be learned adaptively from the data using the proposed alternative minimization scheme. Extensive experiments show that our approach is capable of preserving the discriminative power of the source domain, of resisting the effects of label noise, and works effectively for small sample sizes, and even one-shot DA. We show that our method outperforms the state-of-the-art on the task of cross-domain visual recognition in both efficacy and efficiency, and particularly that it outperforms all comparators when applied to DA of the convolutional activations of deep convolutional networks.",http://arxiv.org/pdf/1707.05956v1
191,"Look, Listen and Learn",Relja Arandjeloviƒ; Andrew Zisserman,"Look, Listen and Learn","We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel ""Audio-Visual Correspondence"" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.",http://arxiv.org/pdf/1705.08168v2
222,Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization,Ramprasaath R. Selvaraju; Michael Cogswell; Abhishek Das; Ramakrishna Vedantam; Devi Parikh; Dhruv Batra,Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization,"We propose a technique for producing ""visual explanations"" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a ""stronger"" deep network from a ""weaker"" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.",http://arxiv.org/pdf/1610.02391v3
229,Image-Based Localization Using LSTMs for Structured Feature Correlation,Florian Walch; Caner Hazirbas; Laura Leal-Taix√©; Torsten Sattler; Sebastian Hilsenbeck; Daniel Cremers,Image-based localization using LSTMs for structured feature correlation,"In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output, which play the role of a structured dimensionality reduction on the feature vector, leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based and SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor dataset with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces, where classic SIFT-based methods fail.",http://arxiv.org/pdf/1611.07890v4
244,Personalized Image Aesthetics,Jian Ren; Xiaohui Shen; Zhe Lin; Radom√≠r Mƒõch; David J. Foran,Modeling Photographic Composition via Triangles,"The capacity of automatically modeling photographic composition is valuable for many real-world machine vision applications such as digital photography, image retrieval, image understanding, and image aesthetics assessment. The triangle technique is among those indispensable composition methods on which professional photographers often rely. This paper proposes a system that can identify prominent triangle arrangements in two major categories of photographs: natural or urban scenes, and portraits. For the natural or urban scene pictures, the focus is on the effect of linear perspective. For portraits, we carefully examine the positioning of human subjects in a photo. We show that line analysis is highly advantageous for modeling composition in both categories. Based on the detected triangles, new mathematical descriptors for composition are formulated and used to retrieve similar images. Leveraging the rich source of high aesthetics photos online, similar approaches can potentially be incorporated in future smart cameras to enhance a person's photo composition skills.",http://arxiv.org/pdf/1605.09559v1
270,Predicting Deeper Into the Future of Semantic Segmentation,Pauline Luc; Natalia Neverova; Camille Couprie; Jakob Verbeek; Yann LeCun,Predicting Deeper into the Future of Semantic Segmentation,"The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",http://arxiv.org/pdf/1703.07684v3
279,Coordinating Filters for Faster Deep Neural Networks,Wei Wen; Cong Xu; Chunpeng Wu; Yandan Wang; Yiran Chen; Hai Li,Coordinating Filters for Faster Deep Neural Networks,"Very large-scale Deep Neural Networks (DNNs) have achieved remarkable successes in a large variety of computer vision tasks. However, the high computation intensity of DNNs makes it challenging to deploy these models on resource-limited systems. Some studies used low-rank approaches that approximate the filters by low-rank basis to accelerate the testing. Those works directly decomposed the pre-trained DNNs by Low-Rank Approximations (LRA). How to train DNNs toward lower-rank space for more efficient DNNs, however, remains as an open area. To solve the issue, in this work, we propose Force Regularization, which uses attractive forces to enforce filters so as to coordinate more weight information into lower-rank space. We mathematically and empirically verify that after applying our technique, standard LRA methods can reconstruct filters using much lower basis and thus result in faster DNNs. The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. Moreover, Force Regularization better initializes the low-rank DNNs such that the fine-tuning can converge faster toward higher accuracy. The obtained lower-rank DNNs can be further sparsified, proving that Force Regularization can be integrated with state-of-the-art sparsity-based acceleration methods. Source code is available in https://github.com/wenwei202/caffe",http://arxiv.org/pdf/1703.09746v3
315,Unsupervised Representation Learning by Sorting Sequences,Hsin-Ying Lee; Jia-Bin Huang; Maneesh Singh; Ming-Hsuan Yang,Unsupervised Representation Learning by Sorting Sequences,"We present an unsupervised representation learning approach using videos without semantic labels. We leverage the temporal coherence as a supervisory signal by formulating representation learning as a sequence sorting task. We take temporally shuffled frames (i.e., in non-chronological order) as inputs and train a convolutional neural network to sort the shuffled sequences. Similar to comparison-based sorting algorithms, we propose to extract features from all frame pairs and aggregate them to predict the correct order. As sorting shuffled image sequence requires an understanding of the statistical temporal structure of images, training with such a proxy task allows us to learn rich and generalizable visual representation. We validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification and object detection tasks.",http://arxiv.org/pdf/1708.01246v1
164,A Read-Write Memory Network for Movie Story Understanding,Seil Na; Sangho Lee; Jisung Kim; Gunhee Kim,A Read-Write Memory Network for Movie Story Understanding,"We propose a novel memory network model named Read-Write Memory Network (RWMN) to perform question and answering tasks for large-scale, multimodal movie story understanding. The key focus of our RWMN model is to design the read network and the write network that consist of multiple convolutional layers, which enable memory read and write operations to have high capacity and flexibility. While existing memory-augmented network models treat each memory slot as an independent block, our use of multi-layered CNNs allows the model to read and write sequential memory cells as chunks, which is more reasonable to represent a sequential story because adjacent memory blocks often have strong correlations. For evaluation, we apply our model to all the six tasks of the MovieQA benchmark, and achieve the best accuracies on several tasks, especially on the visual QA task. Our model shows a potential to better understand not only the content in the story, but also more abstract information, such as relationships between characters and the reasons for their actions.",http://arxiv.org/pdf/1709.09345v1
211,SegFlow: Joint Learning for Video Object Segmentation and Optical Flow,Jingchun Cheng; Yi-Hsuan Tsai; Shengjin Wang; Ming-Hsuan Yang,SegFlow: Joint Learning for Video Object Segmentation and Optical Flow,"This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.",http://arxiv.org/pdf/1709.06750v1
243,Unsupervised Action Discovery and Localization in Videos,Khurram Soomro; Mubarak Shah,,,
271,Dense-Captioning Events in Videos,Ranjay Krishna; Kenji Hata; Frederic Ren; Li Fei-Fei; Juan Carlos Niebles,Dense-Captioning Events in Videos,"Most natural videos contain numerous events. For example, in a video of a ""man playing a piano"", the video might also contain ""another man dancing"" or ""a crowd clapping"". We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with it's unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.",http://arxiv.org/pdf/1705.00754v1
292,Learning Long-Term Dependencies for Action Recognition With a Biologically-Inspired Deep Network,Yemin Shi; Yonghong Tian; Yaowei Wang; Wei Zeng; Tiejun Huang,Learning long-term dependencies for action recognition with a biologically-inspired deep network,"Despite a lot of research efforts devoted in recent years, how to efficiently learn long-term dependencies from sequences still remains a pretty challenging task. As one of the key models for sequence learning, recurrent neural network (RNN) and its variants such as long short term memory (LSTM) and gated recurrent unit (GRU) are still not powerful enough in practice. One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of both feedforward and feedback connections. To address this problem, this paper proposes a biologically-inspired deep network, called shuttleNet\footnote{Our code is available at \url{https://github.com/shiyemin/shuttlenet}}. Technologically, the shuttleNet consists of several processors, each of which is a GRU while associated with multiple groups of cells and states. Unlike traditional RNNs, all processors inside shuttleNet are loop connected to mimic the brain's feedforward and feedback connections, in which they are shared across multiple pathways in the loop connection. Attention mechanism is then employed to select the best information flow pathway. Extensive experiments conducted on two benchmark datasets (i.e UCF101 and HMDB51) show that we can beat state-of-the-art methods by simply embedding shuttleNet into a CNN-RNN framework.",http://arxiv.org/pdf/1611.05216v3
299,Compressive Quantization for Fast Object Instance Search in Videos,Tan Yu; Zhenzhen Wang; Junsong Yuan,,,
326,Complex Event Detection by Identifying Reliable Shots From Untrimmed Videos,Hehe Fan; Xiaojun Chang; De Cheng; Yi Yang; Dong Xu; Alexander G. Hauptmann,,,
44,Deep Direct Regression for Multi-Oriented Scene Text Detection,Wenhao He; Xu-Yao Zhang; Fei Yin; Cheng-Lin Liu,Deep Direct Regression for Multi-Oriented Scene Text Detection,"In this paper, we first provide a new perspective to divide existing high performance object detection methods into direct and indirect regressions. Direct regression performs boundary regression by predicting the offsets from a given point, while indirect regression predicts the offsets from some bounding box proposals. Then we analyze the drawbacks of the indirect regression, which the recent state-of-the-art detection structures like Faster-RCNN and SSD follows, for multi-oriented scene text detection, and point out the potential superiority of direct regression. To verify this point of view, we propose a deep direct regression based method for multi-oriented scene text detection. Our detection framework is simple and effective with a fully convolutional network and one-step post processing. The fully convolutional network is optimized in an end-to-end way and has bi-task outputs where one is pixel-wise classification between text and non-text, and the other is direct regression to determine the vertex coordinates of quadrilateral text boundaries. The proposed method is particularly beneficial for localizing incidental scene texts. On the ICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure of 81%, which is a new state-of-the-art and significantly outperforms previous approaches. On other standard datasets with focused scene texts, our method also reaches the state-of-the-art performance.",http://arxiv.org/pdf/1703.08289v1
878,Open Set Domain Adaptation,Pau Panareda Busto; Juergen Gall,Multi-domain Neural Network Language Generation for Spoken Dialogue Systems,"Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with the number of domains. Therefore, it is important to leverage existing resources and exploit similarities between domains to facilitate domain adaptation. In this paper, we propose a procedure to train multi-domain, Recurrent Neural Network-based (RNN) language generators via multiple adaptation steps. In this procedure, a model is first trained on counterfeited data synthesised from an out-of-domain dataset, and then fine tuned on a small set of in-domain utterances with a discriminative objective function. Corpus-based evaluation results show that the proposed procedure can achieve competitive performance in terms of BLEU score and slot error rate while significantly reducing the data needed to train generators in new, unseen domains. In subjective testing, human judges confirm that the procedure greatly improves generator performance when only a small amount of data is available in the domain.",http://arxiv.org/pdf/1603.01232v1
133,Deformable Convolutional Networks,Jifeng Dai; Haozhi Qi; Yuwen Xiong; Yi Li; Guodong Zhang; Han Hu; Yichen Wei,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,"Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling' (MP) layers to extract deformation-invariant features, but we argue in favor of a more refined treatment. First, we introduce epitomic convolution as a building block alternative to the common convolution-MP cascade of DCNNs; while having identical complexity to MP, Epitomic Convolution allows for parameter sharing across different filters, resulting in faster convergence and better generalization. Second, we introduce a Multiple Instance Learning approach to explicitly accommodate global translation and scaling when training a DCNN exclusively with class labels. For this we rely on a `patchwork' data structure that efficiently lays out all image scales and positions as candidates to a DCNN. Factoring global and local deformations allows a DCNN to `focus its resources' on the treatment of non-rigid deformations and yields a substantial classification accuracy improvement. Third, further pursuing this idea, we develop an efficient DCNN sliding window object detector that employs explicit search over position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on the Pascal VOC 2007 benchmark.",http://arxiv.org/pdf/1412.0296v1
693,Ensemble Diffusion for Retrieval,Song Bai; Zhichao Zhou; Jingdong Wang; Xiang Bai; Longin Jan Latecki; Qi Tian,Schroedinger-like PageRank equation and localization in the WWW,"The WorldWide Web is one of the most important communication systems we use in our everyday life. Despite its central role, the growth and the development of the WWW is not controlled by any central authority. This situation has created a huge ensemble of connections whose complexity can be fruitfully described and quantified by network theory. One important application that allows to sort out the information present in these connections is given by the PageRank alghorithm. Computation of this quantity is usually made iteratively with a large use of computational time. In this paper we show that the PageRank can be expressed in terms of a wave function obeying a Schroedinger-like equation. In particular the topological disorder given by the unbalance of outgoing and ingoing links between pages, induces wave function and potential structuring. This allows to directly localize the pages with the largest score. Through this new representation we can now compute the PageRank without iterative techniques. For most of the cases of interest our method is faster than the original one. Our results also clarify the role of topology in the diffusion of information within complex networks. The whole approach opens the possibility to novel techniques inspired by quantum physics for the analysis of the WWW properties.",http://arxiv.org/pdf/0807.4325v1
830,FoveaNet: Perspective-Aware Urban Scene Parsing,Xin Li; Zequn Jie; Wei Wang; Changsong Liu; Jimei Yang; Xiaohui Shen; Zhe Lin; Qiang Chen; Shuicheng Yan; Jiashi Feng,FoveaNet: Perspective-aware Urban Scene Parsing,"Parsing urban scene images benefits many applications, especially self-driving. Most of the current solutions employ generic image parsing models that treat all scales and locations in the images equally and do not consider the geometry property of car-captured urban scene images. Thus, they suffer from heterogeneous object scales caused by perspective projection of cameras on actual scenes and inevitably encounter parsing failures on distant objects as well as other boundary and recognition errors. In this work, we propose a new FoveaNet model to fully exploit the perspective geometry of scene images and address the common failures of generic parsing models. FoveaNet estimates the perspective geometry of a scene image through a convolutional network which integrates supportive evidence from contextual objects within the image. Based on the perspective geometry information, FoveaNet ""undoes"" the camera perspective projection analyzing regions in the space of the actual scene, and thus provides much more reliable parsing results. Furthermore, to effectively address the recognition errors, FoveaNet introduces a new dense CRFs model that takes the perspective geometry as a prior potential. We evaluate FoveaNet on two urban scene parsing datasets, Cityspaces and CamVid, which demonstrates that FoveaNet can outperform all the well-established baselines and provide new state-of-the-art performance.",http://arxiv.org/pdf/1708.02421v1
1128,Beyond Planar Symmetry: Modeling Human Perception of Reflection and Rotation Symmetries in the Wild,Christopher Funk; Yanxi Liu,Beyond Planar Symmetry: Modeling human perception of reflection and rotation symmetries in the wild,"Humans take advantage of real world symmetries for various tasks, yet capturing their superb symmetry perception mechanism with a computational model remains elusive. Motivated by a new study demonstrating the extremely high inter-person accuracy of human perceived symmetries in the wild, we have constructed the first deep-learning neural network for reflection and rotation symmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common Object in COntext) dataset with nearly 11K consistent symmetry-labels from more than 400 human observers. We employ novel methods to convert discrete human labels into symmetry heatmaps, capture symmetry densely in an image and quantitatively evaluate Sym-NET against multiple existing computer vision algorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO photos, Sym-NET significantly outperforms all other competitors. Beyond mathematically well-defined symmetries on a plane, Sym-NET demonstrates abilities to identify viewpoint-varied 3D symmetries, partially occluded symmetrical objects, and symmetries at a semantic level.",http://arxiv.org/pdf/1704.03568v2
470,Learning to Reason: End-To-End Module Networks for Visual Question Answering,Ronghang Hu; Jacob Andreas; Marcus Rohrbach; Trevor Darrell; Kate Saenko,Learning to Reason: End-to-End Module Networks for Visual Question Answering,"Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer ""is there an equal number of balls and boxes?"" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question.",http://arxiv.org/pdf/1704.05526v3
1013,Hard-Aware Deeply Cascaded Embedding,Yuhui Yuan; Kuiyuan Yang; Chao Zhang,Hard-Aware Deeply Cascaded Embedding,"Riding on the waves of deep neural networks, deep metric learning has also achieved promising results in various tasks using triplet network or Siamese network. Though the basic goal of making images from the same category closer than the ones from different categories is intuitive, it is hard to directly optimize due to the quadratic or cubic sample size. To solve the problem, hard example mining which only focuses on a subset of samples that are considered hard is widely used. However, hard is defined relative to a model, where complex models treat most samples as easy ones and vice versa for simple models, and both are not good for training. Samples are also with different hard levels, it is hard to define a model with the just right complexity and choose hard examples adequately. This motivates us to ensemble a set of models with different complexities in cascaded manner and mine hard examples adaptively, a sample is judged by a series of models with increasing complexities and only updates models that consider the sample as a hard case. We evaluate our method on CARS196, CUB-200-2011, Stanford Online Products, VehicleID and DeepFashion datasets. Our method outperforms state-of-the-art methods by a large margin.",http://arxiv.org/pdf/1611.05720v2
1201,Query-Guided Regression Network With Context Policy for Phrase Grounding,Kan Chen; Rama Kovvuri; Ram Nevatia,Query-guided Regression Network with Context Policy for Phrase Grounding,"Given a textual description of an image, phrase grounding localizes objects in the image referred by query phrases in the description. State-of-the-art methods address the problem by ranking a set of proposals based on the relevance to each query, which are limited by the performance of independent proposal generation systems and ignore useful cues from context in the description. In this paper, we adopt a spatial regression method to break the performance limit, and introduce reinforcement learning techniques to further leverage semantic context information. We propose a novel Query-guided Regression network with Context policy (QRC Net) which jointly learns a Proposal Generation Network (PGN), a Query-guided Regression Network (QRN) and a Context Policy Network (CPN). Experiments show QRC Net provides a significant improvement in accuracy on two popular datasets: Flickr30K Entities and Referit Game, with 14.25% and 17.14% increase over the state-of-the-arts respectively.",http://arxiv.org/pdf/1708.01676v1
1413,"SUBIC: A Supervised, Structured Binary Code for Image Search",Himalaya Jain; Joaquin Zepeda; Patrick P√©rez; R√©mi Gribonval,"SUBIC: A supervised, structured binary code for image search","For large-scale visual search, highly compressed yet meaningful representations of images are essential. Structured vector quantizers based on product quantization and its variants are usually employed to achieve such compression while minimizing the loss of accuracy. Yet, unlike binary hashing schemes, these unsupervised methods have not yet benefited from the supervision, end-to-end learning and novel architectures ushered in by the deep learning revolution. We hence propose herein a novel method to make deep convolutional neural networks produce supervised, compact, structured binary codes for visual search. Our method makes use of a novel block-softmax non-linearity and of batch-based entropy losses that together induce structure in the learned encodings. We show that our method outperforms state-of-the-art compact representations based on deep hashing or structured quantization in single and cross-domain category retrieval, instance retrieval and classification. We make our code and models publicly available online.",http://arxiv.org/pdf/1708.02932v1
1498,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,Chen Sun; Abhinav Shrivastava; Saurabh Singh; Abhinav Gupta,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",http://arxiv.org/pdf/1707.02968v2
1783,A Generative Model of People in Clothing,Christoph Lassner; Gerard Pons-Moll; Peter V. Gehler,A Generative Model of People in Clothing,"We present the first image-based generative model of people in clothing for the full body. We sidestep the commonly used complex graphics rendering pipeline and the need for high-quality 3D scans of dressed people. Instead, we learn generative models from a large image database. The main challenge is to cope with the high variance in human pose, shape and appearance. For this reason, pure image-based approaches have not been considered so far. We show that this challenge can be overcome by splitting the generating process in two parts. First, we learn to generate a semantic segmentation of the body and clothing. Second, we learn a conditional model on the resulting segments that creates realistic images. The full model is differentiable and can be conditioned on pose, shape or color. The result are samples of people in different clothing items and styles. The proposed model can generate entirely new people with realistic clothing. In several experiments we present encouraging results that suggest an entirely data-driven approach to people generation is possible.",http://arxiv.org/pdf/1705.04098v3
2204,Escape From Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models,Roman Klokov; Victor Lempitsky,Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point Cloud Models,"We present a new deep learning architecture (called Kd-network) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and share parameters of these transformations according to the subdivisions of the point clouds imposed onto them by Kd-trees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform two-dimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behaviour. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.",http://arxiv.org/pdf/1704.01222v1
2633,Improved Image Captioning via Policy Gradient Optimization of SPIDEr,Siqi Liu; Zhenhai Zhu; Ning Ye; Sergio Guadarrama; Kevin Murphy,Improved Image Captioning via Policy Gradient optimization of SPIDEr,"Current image captioning methods are usually trained via (penalized) maximum likelihood estimation. However, the log-likelihood score of a caption does not correlate well with human assessments of quality. Standard syntactic evaluation metrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The newer SPICE and CIDEr metrics are better correlated, but have traditionally been hard to optimize for. In this paper, we show how to use a policy gradient (PG) method to directly optimize a linear combination of SPICE and CIDEr (a combination we call SPIDEr): the SPICE score ensures our captions are semantically faithful to the image, while CIDEr score ensures our captions are syntactically fluent. The PG method we propose improves on the prior MIXER approach, by using Monte Carlo rollouts instead of mixing MLE training with PG. We show empirically that our algorithm leads to easier optimization and improved results compared to MIXER. Finally, we show that using our PG method we can optimize any of the metrics, including the proposed SPIDEr metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize MLE or the COCO metrics.",http://arxiv.org/pdf/1612.00370v3
341,Rolling Shutter Correction in Manhattan World,Pulak Purkait; Christopher Zach; Ale≈° Leonardis,,,
342,Local-To-Global Point Cloud Registration Using a Dictionary of Viewpoint Descriptors,David Avidar; David Malah; Meir Barzohar,,,
384,3D-PRNN: Generating Shape Primitives With Recurrent Neural Networks,Chuhang Zou; Ersin Yumer; Jimei Yang; Duygu Ceylan; Derek Hoiem,3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks,"The success of various applications including robotics, digital content creation, and visualization demand a structured and abstract representation of the 3D world from limited sensor data. Inspired by the nature of human perception of 3D shapes as a collection of simple parts, we explore such an abstract shape representation based on primitives. Given a single depth image of an object, we present 3D-PRNN, a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives. Our generative model encodes symmetry characteristics of common man-made objects, preserves long-range structural coherence, and describes objects of varying complexity with a compact representation. We also propose a method based on Gaussian Fields to generate a large scale dataset of primitive-based shape representations to train our network. We evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space.",http://arxiv.org/pdf/1708.01648v1
415,BodyFusion: Real-Time Capture of Human Motion and Surface Geometry Using a Single Depth Camera,Tao Yu; Kaiwen Guo; Feng Xu; Yuan Dong; Zhaoqi Su; Jianhui Zhao; Jianguo Li; Qionghai Dai; Yebin Liu,,,
472,Quasiconvex Plane Sweep for Triangulation With Outliers,Qianggong Zhang; Tat-Jun Chin; David Suter,,,
478,"""Maximizing Rigidity"" Revisited: A Convex Programming Approach for Generic 3D Shape Reconstruction From Multiple Perspective Views",Pan Ji; Hongdong Li; Yuchao Dai; Ian Reid,,,
534,Surface Registration via Foliation,Xiaopeng Zheng; Chengfeng Wen; Na Lei; Ming Ma; Xianfeng Gu,,,
596,Rolling-Shutter-Aware Differential SfM and Image Rectification,Bingbing Zhuang; Loong-Fah Cheong; Gim Hee Lee,,,
622,Corner-Based Geometric Calibration of Multi-Focus Plenoptic Cameras,Sotiris Nousias; Fran√ßois Chadebecq; Jonas Pichat; Pearse Keane; S√©bastien Ourselin; Christos Bergeles,,,
461,Focal Track: Depth and Accommodation With Oscillating Lens Deformation,Qi Guo; Emma Alexander; Todd Zickler,,,
466,Reconfiguring the Imaging Pipeline for Computer Vision,Mark Buckler; Suren Jayasuriya; Adrian Sampson,Reconfiguring the Imaging Pipeline for Computer Vision,"Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be designed to be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for skipping these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, lower-precision image data. This vision mode can save ~75% of the average energy of a baseline photography mode while having only a small impact on vision task accuracy.",http://arxiv.org/pdf/1705.04352v3
580,Catadioptric HyperSpectral Light Field Imaging,Yujia Xue; Kang Zhu; Qiang Fu; Xilin Chen; Jingyi Yu,,,
378,Cross-View Asymmetric Metric Learning for Unsupervised Person Re-Identification,Hong-Xing Yu; Ancong Wu; Wei-Shi Zheng,Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification,"While metric learning is important for Person re-identification (RE-ID), a significant problem in visual surveillance for cross-view pedestrian matching, existing metric models for RE-ID are mostly based on supervised learning that requires quantities of labeled samples in all pairs of camera views for training. However, this limits their scalabilities to realistic applications, in which a large amount of data over multiple disjoint camera views is available but not labelled. To overcome the problem, we propose unsupervised asymmetric metric learning for unsupervised RE-ID. Our model aims to learn an asymmetric metric, i.e., specific projection for each view, based on asymmetric clustering on cross-view person images. Our model finds a shared space where view-specific bias is alleviated and thus better matching performance can be achieved. Extensive experiments have been conducted on a baseline and five large-scale RE-ID datasets to demonstrate the effectiveness of the proposed model. Through the comparison, we show that our model works much more suitable for unsupervised RE-ID compared to classical unsupervised metric learning models. We also compare with existing unsupervised RE-ID methods, and our model outperforms them with notable margins. Specifically, we report the results on large-scale unlabelled RE-ID dataset, which is important but unfortunately less concerned in literatures.",http://arxiv.org/pdf/1708.08062v1
420,Real Time Eye Gaze Tracking With 3D Deformable Eye-Face Model,Kang Wang; Qiang Ji,,,
457,Ensemble Deep Learning for Skeleton-Based Action Recognition Using Temporal Sliding LSTM Networks,Inwoong Lee; Doyoung Kim; Seoungyoon Kang; Sanghoon Lee,,,
560,"How Far Are We From Solving the 2D & 3D Face Alignment Problem? (And a Dataset of 230,000 3D Facial Landmarks)",Adrian Bulat; Georgios Tzimiropoulos,,,
603,Large Pose 3D Face Reconstruction From a Single Image via Direct Volumetric CNN Regression,Aaron S. Jackson; Adrian Bulat; Vasileios Argyriou; Georgios Tzimiropoulos,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,"3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon",http://arxiv.org/pdf/1703.07834v2
340,RankIQA: Learning From Rankings for No-Reference Image Quality Assessment,Xialei Liu; Joost van de Weijer; Andrew D. Bagdanov,RankIQA: Learning from Rankings for No-reference Image Quality Assessment,"We propose a no-reference image quality assessment (NR-IQA) approach that learns from rankings (RankIQA). To address the problem of limited IQA dataset size, we train a Siamese Network to rank images in terms of image quality by using synthetically generated distortions for which relative image quality is known. These ranked image sets can be automatically generated without laborious human labeling. We then use fine-tuning to transfer the knowledge represented in the trained Siamese Network to a traditional CNN that estimates absolute image quality from single images. We demonstrate how our approach can be made significantly more efficient than traditional Siamese Networks by forward propagating a batch of images through a single network and backpropagating gradients derived from all pairs of images in the batch. Experiments on the TID2013 benchmark show that we improve the state-of-the-art by over 5%. Furthermore, on the LIVE benchmark we show that our approach is superior to existing NR-IQA techniques and that we even outperform the state-of-the-art in full-reference IQA (FR-IQA) methods without having to resort to high-quality reference images to infer IQA.",http://arxiv.org/pdf/1707.08347v1
360,"Look, Perceive and Segment: Finding the Salient Objects in Images via Two-Stream Fixation-Semantic CNNs",Xiaowu Chen; Anlin Zheng; Jia Li; Feng Lu,,,
369,Delving Into Salient Object Subitizing and Detection,Shengfeng He; Jianbo Jiao; Xiaodan Zhang; Guoqiang Han; Rynson W.H. Lau,,,
376,Learning Discriminative Data Fitting Functions for Blind Image Deblurring,Jinshan Pan; Jiangxin Dong; Yu-Wing Tai; Zhixun Su; Ming-Hsuan Yang,,,
487,Video Deblurring via Semantic Segmentation and Pixel-Wise Non-Linear Kernel,Wenqi Ren; Jinshan Pan; Xiaochun Cao; Ming-Hsuan Yang,Video Deblurring via Semantic Segmentation and Pixel-Wise Non-Linear Kernel,"Video deblurring is a challenging problem as the blur is complex and usually caused by the combination of camera shakes, object motions, and depth variations. Optical flow can be used for kernel estimation since it predicts motion trajectories. However, the estimates are often inaccurate in complex scenes at object boundaries, which are crucial in kernel estimation. In this paper, we exploit semantic segmentation in each blurry frame to understand the scene contents and use different motion models for image regions to guide optical flow estimation. While existing pixel-wise blur models assume that the blur kernel is the same as optical flow during the exposure time, this assumption does not hold when the motion blur trajectory at a pixel is different from the estimated linear optical flow. We analyze the relationship between motion blur trajectory and optical flow, and present a novel pixel-wise non-linear kernel model to account for motion blur. The proposed blur model is based on the non-linear optical flow, which describes complex motion blur more effectively. Extensive experiments on challenging blurry videos demonstrate the proposed algorithm performs favorably against the state-of-the-art methods.",http://arxiv.org/pdf/1708.03423v1
555,On-Demand Learning for Deep Image Restoration,Ruohan Gao; Kristen Grauman,On-Demand Learning for Deep Image Restoration,"While machine learning approaches to image restoration offer great promise, current methods risk training models fixated on performing well only for image corruption of a particular level of difficulty---such as a certain level of noise or blur. First, we examine the weakness of conventional ""fixated"" models and demonstrate that training general models to handle arbitrary levels of corruption is indeed non-trivial. Then, we propose an on-demand learning algorithm for training image restoration models with deep convolutional neural networks. The main idea is to exploit a feedback mechanism to self-generate training instances where they are needed most, thereby learning models that can generalize across difficulty levels. On four restoration tasks---image inpainting, pixel interpolation, image deblurring, and image denoising---and three diverse datasets, our approach consistently outperforms both the status quo training procedure and curriculum learning alternatives.",http://arxiv.org/pdf/1612.01380v3
572,Multi-Channel Weighted Nuclear Norm Minimization for Real Color Image Denoising,Jun Xu; Lei Zhang; David Zhang; Xiangchu Feng,Multi-channel Weighted Nuclear Norm Minimization for Real Color Image Denoising,"Most of the existing denoising algorithms are developed for grayscale images, while it is not a trivial work to extend them for color image denoising because the noise statistics in R, G, B channels can be very different for real noisy images. In this paper, we propose a multi-channel (MC) optimization model for real color image denoising under the weighted nuclear norm minimization (WNNM) framework. We concatenate the RGB patches to make use of the channel redundancy, and introduce a weight matrix to balance the data fidelity of the three channels in consideration of their different noise statistics. The proposed MC-WNNM model does not have an analytical solution. We reformulate it into a linear equality-constrained problem and solve it with the alternating direction method of multipliers. Each alternative updating step has closed-form solution and the convergence can be guaranteed. Extensive experiments on both synthetic and real noisy image datasets demonstrate the superiority of the proposed MC-WNNM over state-of-the-art denoising methods.",http://arxiv.org/pdf/1705.09912v1
615,Coherent Online Video Style Transfer,Dongdong Chen; Jing Liao; Lu Yuan; Nenghai Yu; Gang Hua,Coherent Online Video Style Transfer,"Training a feed-forward network for fast neural style transfer of images is proven to be successful. However, the naive extension to process video frame by frame is prone to producing flickering results. We propose the first end-to-end network for online video style transfer, which generates temporally coherent stylized video sequences in near real-time. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures the consistency over larger period of time. Our network can incorporate different image stylization networks. We show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitudes faster in runtime.",http://arxiv.org/pdf/1703.09211v2
351,SHaPE: A Novel Graph Theoretic Algorithm for Making Consensus-Based Decisions in Person Re-Identification Systems,Arko Barman; Shishir K. Shah,,,
435,Need for Speed: A Benchmark for Higher Frame Rate Object Tracking,Hamed Kiani Galoogahi; Ashton Fagg; Chen Huang; Deva Ramanan; Simon Lucey,Need for Speed: A Benchmark for Higher Frame Rate Object Tracking,"In this paper, we propose the first higher frame rate video dataset (called Need for Speed - NfS) and benchmark for visual object tracking. The dataset consists of 100 videos (380K frames) captured with now commonly available higher frame rate (240 FPS) cameras from real world scenarios. All frames are annotated with axis aligned bounding boxes and all sequences are manually labelled with nine visual attributes - such as occlusion, fast motion, background clutter, etc. Our benchmark provides an extensive evaluation of many recent and state-of-the-art trackers on higher frame rate sequences. We ranked each of these trackers according to their tracking accuracy and real-time performance. One of our surprising conclusions is that at higher frame rates, simple trackers such as correlation filters outperform complex methods based on deep networks. This suggests that for practical applications (such as in robotics or embedded vision), one needs to carefully tradeoff bandwidth constraints associated with higher frame rate acquisition, computational costs of real-time analysis, and the required application accuracy. Our dataset and benchmark allows for the first time (to our knowledge) systematic exploration of such issues, and will be made available to allow for further research in this space.",http://arxiv.org/pdf/1703.05884v2
436,Learning Background-Aware Correlation Filters for Visual Tracking,Hamed Kiani Galoogahi; Ashton Fagg; Simon Lucey,Learning Background-Aware Correlation Filters for Visual Tracking,"Correlation Filters (CFs) have recently demonstrated excellent performance in terms of rapidly tracking objects under challenging photometric and geometric variations. The strength of the approach comes from its ability to efficiently learn - ""on the fly"" - how the object is changing over time. A fundamental drawback to CFs, however, is that the background of the object is not be modelled over time which can result in suboptimal results. In this paper we propose a Background-Aware CF that can model how both the foreground and background of the object varies over time. Our approach, like conventional CFs, is extremely computationally efficient - and extensive experiments over multiple tracking benchmarks demonstrate the superior accuracy and real-time performance of our method compared to the state-of-the-art trackers including those based on a deep learning paradigm.",http://arxiv.org/pdf/1703.04590v2
456,Robust Object Tracking Based on Temporal and Spatial Deep Networks,Zhu Teng; Junliang Xing; Qiang Wang; Congyan Lang; Songhe Feng; Yi Jin,Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking,"In this paper, we develop a new approach of spatially supervised recurrent convolutional neural networks for visual object tracking. Our recurrent convolutional network exploits the history of locations as well as the distinctive visual features learned by the deep neural networks. Inspired by recent bounding box regression methods for object detection, we study the regression capability of Long Short-Term Memory (LSTM) in the temporal domain, and propose to concatenate high-level visual features produced by convolutional networks with region information. In contrast to existing deep learning based trackers that use binary classification for region candidates, we use regression for direct prediction of the tracking locations both at the convolutional layer and at the recurrent unit. Our extensive experimental results and performance comparison with state-of-the-art tracking methods on challenging benchmark video tracking datasets shows that our tracker is more accurate and robust while maintaining low computational cost. For most test video sequences, our method achieves the best tracking performance, often outperforms the second best by a large margin.",http://arxiv.org/pdf/1607.05781v1
514,Real-Time Hand Tracking Under Occlusion From an Egocentric RGB-D Sensor,Franziska Mueller; Dushyant Mehta; Oleksandr Sotnychenko; Srinath Sridhar; Dan Casas; Christian Theobalt,,,
566,Predicting Human Activities Using Stochastic Grammar,Siyuan Qi; Siyuan Huang; Ping Wei; Song-Chun Zhu,Predicting Human Activities Using Stochastic Grammar,"This paper presents a novel method to predict future human activities from partially observed RGB-D videos. Human activity prediction is generally difficult due to its non-Markovian property and the rich context between human and environments.   We use a stochastic grammar model to capture the compositional structure of events, integrating human actions, objects, and their affordances. We represent the event by a spatial-temporal And-Or graph (ST-AOG). The ST-AOG is composed of a temporal stochastic grammar defined on sub-activities, and spatial graphs representing sub-activities that consist of human actions, objects, and their affordances. Future sub-activities are predicted using the temporal grammar and Earley parsing algorithm. The corresponding action, object, and affordance labels are then inferred accordingly. Extensive experiments are conducted to show the effectiveness of our model on both semantic event parsing and future activity prediction.",http://arxiv.org/pdf/1708.00945v1
619,ProbFlow: Joint Optical Flow and Uncertainty Estimation,Anne S. Wannenwetsch; Margret Keuper; Stefan Roth,ProbFlow: Joint Optical Flow and Uncertainty Estimation,"Optical flow estimation remains challenging due to untextured areas, motion boundaries, occlusions, and more. Thus, the estimated flow is not equally reliable across the image. To that end, post-hoc confidence measures have been introduced to assess the per-pixel reliability of the flow. We overcome the artificial separation of optical flow and confidence estimation by introducing a method that jointly predicts optical flow and its underlying uncertainty. Starting from common energy-based formulations, we rely on the corresponding posterior distribution of the flow given the images. We derive a variational inference scheme based on mean field, which incorporates best practices from energy minimization. An uncertainty measure is obtained along the flow at every pixel as the (marginal) entropy of the variational distribution. We demonstrate the flexibility of our probabilistic approach by applying it to two different energies and on two benchmarks. We not only obtain flow results that are competitive with the underlying energy minimization approach, but also a reliable uncertainty measure that significantly outperforms existing post-hoc approaches.",http://arxiv.org/pdf/1708.06509v1
502,Sublabel-Accurate Discretization of Nonconvex Free-Discontinuity Problems,Thomas M√∂llenhoff; Daniel Cremers,Sublabel-Accurate Discretization of Nonconvex Free-Discontinuity Problems,"In this work we show how sublabel-accurate multilabeling approaches can be derived by approximating a classical label-continuous convex relaxation of nonconvex free-discontinuity problems. This insight allows to extend these sublabel-accurate approaches from total variation to general convex and nonconvex regularizations. Furthermore, it leads to a systematic approach to the discretization of continuous convex relaxations. We study the relationship to existing discretizations and to discrete-continuous MRFs. Finally, we apply the proposed approach to obtain a sublabel-accurate and convex solution to the vectorial Mumford-Shah functional and show in several experiments that it leads to more precise solutions using fewer labels.",http://arxiv.org/pdf/1611.06987v2
350,DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding,Yinda Zhang; Mingru Bai; Pushmeet Kohli; Shahram Izadi; Jianxiong Xiao,DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding,"While deep neural networks have led to human-level performance on computer vision tasks, they have yet to demonstrate similar gains for holistic scene understanding. In particular, 3D context has been shown to be an extremely important cue for scene understanding - yet very little research has been done on integrating context information with deep models. This paper presents an approach to embed 3D context into the topology of a neural network trained to perform holistic scene understanding. Given a depth image depicting a 3D scene, our network aligns the observed scene with a predefined 3D scene template, and then reasons about the existence and location of each object within the scene template. In doing so, our model recognizes multiple objects in a single forward pass of a 3D convolutional neural network, capturing both global scene and local object information simultaneously. To create training data for this 3D network, we generate partly hallucinated depth images which are rendered by replacing real objects with a repository of CAD models of the same object category. Extensive experiments demonstrate the effectiveness of our algorithm compared to the state-of-the-arts. Source code and data are available at http://deepcontext.cs.princeton.edu.",http://arxiv.org/pdf/1603.04922v4
353,BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography,Michael J. Wilber; Chen Fang; Hailin Jin; Aaron Hertzmann; John Collomosse; Serge Belongie,,,
373,Adversarial PoseNet: A Structure-Aware Convolutional Network for Human Pose Estimation,Yu Chen; Chunhua Shen; Xiu-Shen Wei; Lingqiao Liu; Jian Yang,Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation,"For human pose estimation in monocular images, joint occlusions and overlapping upon human bodies often result in deviated pose predictions. Under these circumstances, biologically implausible pose predictions may be produced. In contrast, human vision is able to predict poses by exploiting geometric constraints of joint inter-connectivity. To address the problem by incorporating priors about the structure of human bodies, we propose a novel structure-aware convolutional network to implicitly take such priors into account during training of the deep network. Explicit learning of such constraints is typically challenging. Instead, we design discriminators to distinguish the real poses from the fake ones (such as biologically implausible ones). If the pose generator (G) generates results that the discriminator fails to distinguish from real ones, the network successfully learns the priors.",http://arxiv.org/pdf/1705.00389v2
393,An Empirical Study of Language CNN for Image Captioning,Jiuxiang Gu; Gang Wang; Jianfei Cai; Tsuhan Chen,An Empirical Study of Language CNN for Image Captioning,"Language Models based on recurrent neural networks have dominated recent image caption generation tasks. In this paper, we introduce a Language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning. In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies of history words, which are critical for image captioning. The effectiveness of our approach is validated on two datasets MS COCO and Flickr30K. Our extensive experimental results show that our method outperforms the vanilla recurrent neural network based language models and is competitive with the state-of-the-art methods.",http://arxiv.org/pdf/1612.07086v3
403,Attributes2Classname: A Discriminative Model for Attribute-Based Unsupervised Zero-Shot Learning,Berkan Demirel; Ramazan Gokberk Cinbis; Nazli Ikizler-Cinbis,Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning,"We propose a novel approach for unsupervised zero-shot learning (ZSL) of classes based on their names. Most existing unsupervised ZSL methods aim to learn a model for directly comparing image features and class names. However, this proves to be a difficult task due to dominance of non-visual semantics in underlying vector-space embeddings of class names. To address this issue, we discriminatively learn a word representation such that the similarities between class and combination of attribute names fall in line with the visual similarity. Contrary to the traditional zero-shot learning approaches that are built upon attribute presence, our approach bypasses the laborious attribute-class relation annotations for unseen classes. In addition, our proposed approach renders text-only training possible, hence, the training can be augmented without the need to collect additional image data. The experimental results show that our method yields state-of-the-art results for unsupervised ZSL in three benchmark datasets.",http://arxiv.org/pdf/1705.01734v2
410,Areas of Attention for Image Captioning,Marco Pedersoli; Thomas Lucas; Cordelia Schmid; Jakob Verbeek,Areas of Attention for Image Captioning,"We propose ""Areas of Attention"", a novel attention-based model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attention-based approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.o meaningful latent semantic structure in the generated captions.",http://arxiv.org/pdf/1612.01033v2
426,Generative Modeling of Audible Shapes for Object Perception,Zhoutong Zhang; Jiajun Wu; Qiujia Li; Zhengjia Huang; James Traer; Josh H. McDermott; Joshua B. Tenenbaum; William T. Freeman,,,
428,"Scene Graph Generation From Objects, Phrases and Region Captions",Yikang Li; Wanli Ouyang; Bolei Zhou; Kun Wang; Xiaogang Wang,"Scene Graph Generation from Objects, Phrases and Region Captions","Object detection, scene graph generation and region captioning, which are three scene understanding tasks at different semantic levels, are tied together: scene graphs are generated on top of objects detected in an image with their pairwise relationship predicted, while region captioning gives a language description of the objects, their attributes, relations, and other context information. In this work, to leverage the mutual connections across semantic levels, we propose a novel neural network model, termed as Multi-level Scene Description Network (denoted as MSDN), to solve the three vision tasks jointly in an end-to-end manner. Objects, phrases, and caption regions are first aligned with a dynamic graph based on their spatial and semantic connections. Then a feature refining structure is used to pass messages across the three levels of semantic tasks through the graph. We benchmark the learned model on three tasks, and show the joint learning across three tasks with our proposed method can bring mutual improvements over previous models. Particularly, on the scene graph generation task, our proposed method outperforms the state-of-art method with more than 3% margin.",http://arxiv.org/pdf/1707.09700v2
459,Recurrent Multimodal Interaction for Referring Image Segmentation,Chenxi Liu; Zhe Lin; Xiaohui Shen; Jimei Yang; Xin Lu; Alan Yuille,Recurrent Multimodal Interaction for Referring Image Segmentation,"In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.",http://arxiv.org/pdf/1703.07939v2
460,Learning Feature Pyramids for Human Pose Estimation,Wei Yang; Shuang Li; Wanli Ouyang; Hongsheng Li; Xiaogang Wang,Learning Feature Pyramids for Human Pose Estimation,"Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multi-branch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.",http://arxiv.org/pdf/1708.01101v1
526,Structured Attentions for Visual Question Answering,Chen Zhu; Yanpeng Zhao; Shuaiyi Huang; Kewei Tu; Yi Ma,SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning,"Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.",http://arxiv.org/pdf/1611.05594v2
588,"Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",Debidatta Dwibedi; Ishan Misra; Martial Hebert,"Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection","A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data.",http://arxiv.org/pdf/1708.01642v1
620,Cascaded Feature Network for Semantic Segmentation of RGB-D Images,Di Lin; Guangyong Chen; Daniel Cohen-Or; Pheng-Ann Heng; Hui Huang,,,
432,Encoder Based Lifelong Learning,Amal Rannen; Rahaf Aljundi; Matthew B. Blaschko; Tinne Tuytelaars,Encoder Based Lifelong Learning,"This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art",http://arxiv.org/pdf/1704.01920v1
442,Transitive Invariance for Self-Supervised Visual Representation Learning,Xiaolong Wang; Kaiming He; Abhinav Gupta,Transitive Invariance for Self-supervised Visual Representation Learning,"Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance: ""different instances but a similar viewpoint and category"" and ""different viewpoints of the same instance"". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network in the surface normal estimation task.",http://arxiv.org/pdf/1708.02901v3
530,Weakly Supervised Learning of Deep Metrics for Stereo Reconstruction,Stepan Tulyakov; Anton Ivanov; Fran√ßois Fleuret,,,
558,Fine-Grained Recognition in the Wild: A Multi-Task Domain Adaptation Approach,Timnit Gebru; Judy Hoffman; Li Fei-Fei,Fine-grained Recognition in the Wild: A Multi-Task Domain Adaptation Approach,"While fine-grained object recognition is an important problem in computer vision, current models are unlikely to accurately classify objects in the wild. These fully supervised models need additional annotated images to classify objects in every new scenario, a task that is infeasible. However, sources such as e-commerce websites and field guides provide annotated images for many classes. In this work, we study fine-grained domain adaptation as a step towards overcoming the dataset shift between easily acquired annotated images and the real world. Adaptation has not been studied in the fine-grained setting where annotations such as attributes could be used to increase performance. Our work uses an attribute based multi-task adaptation loss to increase accuracy from a baseline of 4.1% to 19.1% in the semi-supervised adaptation case. Prior do- main adaptation works have been benchmarked on small datasets such as [46] with a total of 795 images for some domains, or simplistic datasets such as [41] consisting of digits. We perform experiments on a subset of a new challenging fine-grained dataset consisting of 1,095,021 images of 2, 657 car categories drawn from e-commerce web- sites and Google Street View.",http://arxiv.org/pdf/1709.02476v1
567,SORT: Second-Order Response Transform for Visual Recognition,Yan Wang; Lingxi Xie; Chenxi Liu; Siyuan Qiao; Ya Zhang; Wenjun Zhang; Qi Tian; Alan Yuille,SORT: Second-Order Response Transform for Visual Recognition,"In this paper, we reveal the importance and benefits of introducing second-order operations into deep neural networks. We propose a novel approach named Second-Order Response Transform (SORT), which appends element-wise product transform to the linear sum of a two-branch network module. A direct advantage of SORT is to facilitate cross-branch response propagation, so that each branch can update its weights based on the current status of the other branch. Moreover, SORT augments the family of transform operations and increases the nonlinearity of the network, making it possible to learn flexible functions to fit the complicated distribution of feature space. SORT can be applied to a wide range of network architectures, including a branched variant of a chain-styled network and a residual network, with very light-weighted modifications. We observe consistent accuracy gain on both small (CIFAR10, CIFAR100 and SVHN) and big (ILSVRC2012) datasets. In addition, SORT is very efficient, as the extra computation overhead is less than 5%.",http://arxiv.org/pdf/1703.06993v3
568,Adversarial Examples for Semantic Segmentation and Object Detection,Cihang Xie; Jianyu Wang; Zhishuai Zhang; Yuyin Zhou; Lingxi Xie; Alan Yuille,Adversarial Examples for Semantic Segmentation and Object Detection,"It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.",http://arxiv.org/pdf/1703.08603v3
569,Genetic CNN,Lingxi Xie; Alan Yuille,A Genetic Programming Approach to Designing Convolutional Neural Network Architectures,"The convolutional neural network (CNN), which is one of the deep learning models, has seen much success in a variety of computer vision tasks. However, designing CNN architectures still requires expert knowledge and a lot of trial and error. In this paper, we attempt to automatically construct CNN architectures for an image classification task based on Cartesian genetic programming (CGP). In our method, we adopt highly functional modules, such as convolutional blocks and tensor concatenation, as the node functions in CGP. The CNN structure and connectivity represented by the CGP encoding method are optimized to maximize the validation accuracy. To evaluate the proposed method, we constructed a CNN architecture for the image classification task with the CIFAR-10 dataset. The experimental result shows that the proposed method can be used to automatically find the competitive CNN architecture compared with state-of-the-art models.",http://arxiv.org/pdf/1704.00764v2
600,Channel Pruning for Accelerating Very Deep Neural Networks,Yihui He; Xiangyu Zhang; Jian Sun,Channel Pruning for Accelerating Very Deep Neural Networks,"In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant. Code has been made publicly available.",http://arxiv.org/pdf/1707.06168v2
626,Infinite Latent Feature Selection: A Probabilistic Latent Graph-Based Ranking Approach,Giorgio Roffo; Simone Melzi; Umberto Castellani; Alessandro Vinciarelli,Infinite Latent Feature Selection: A Probabilistic Latent Graph-Based Ranking Approach,"Feature selection is playing an increasingly significant role with respect to many computer vision applications spanning from object recognition to visual object tracking. However, most of the recent solutions in feature selection are not robust across different and heterogeneous set of data. In this paper, we address this issue proposing a robust probabilistic latent graph-based feature selection algorithm that performs the ranking step while considering all the possible subsets of features, as paths on a graph, bypassing the combinatorial problem analytically. An appealing characteristic of the approach is that it aims to discover an abstraction behind low-level sensory data, that is, relevancy. Relevancy is modelled as a latent variable in a PLSA-inspired generative process that allows the investigation of the importance of a feature when injected into an arbitrary set of cues. The proposed method has been tested on ten diverse benchmarks, and compared against eleven state of the art feature selection methods. Results show that the proposed approach attains the highest performance levels across many different scenarios and difficulties, thereby confirming its strong robustness while setting a new state of the art in feature selection domain.",http://arxiv.org/pdf/1707.07538v1
346,Video Fill in the Blank Using LR/RL LSTMs With Spatial-Temporal Attentions,Amir Mazaheri; Dong Zhang; Mubarak Shah,Video Fill In the Blank using LR/RL LSTMs with Spatial-Temporal Attentions,"Given a video and a description sentence with one missing word (we call it the ""source sentence""), Video-Fill-In-the-Blank (VFIB) problem is to find the missing word automatically. The contextual information of the sentence, as well as visual cues from the video, are important to infer the missing word accurately. Since the source sentence is broken into two fragments: the sentence's left fragment (before the blank) and the sentence's right fragment (after the blank), traditional Recurrent Neural Networks cannot encode this structure accurately because of many possible variations of the missing word in terms of the location and type of the word in the source sentence. For example, a missing word can be the first word or be in the middle of the sentence and it can be a verb or an adjective. In this paper, we propose a framework to tackle the textual encoding: Two separate LSTMs (the LR and RL LSTMs) are employed to encode the left and right sentence fragments and a novel structure is introduced to combine each fragment with an ""external memory"" corresponding the opposite fragments. For the visual encoding, end-to-end spatial and temporal attention models are employed to select discriminative visual representations to find the missing word. In the experiments, we demonstrate the superior performance of the proposed method on challenging VFIB problem. Furthermore, we introduce an extended and more generalized version of VFIB, which is not limited to a single blank. Our experiments indicate the generalization capability of our method in dealing with such more realistic scenarios.",http://arxiv.org/pdf/1704.04689v1
359,Primary Video Object Segmentation via Complementary CNNs and Neighborhood Reversible Flow,Jia Li; Anlin Zheng; Xiaowu Chen; Bin Zhou,,,
510,Attentive Semantic Video Generation Using Captions,Tanya Marwah; Gaurav Mittal; Vineeth N. Balasubramanian,Attentive Semantic Video Generation using Captions,"This paper proposes a network architecture to perform variable length semantic video generation using captions. We adopt a new perspective towards video generation where we allow the captions to be combined with the long-term and short-term dependencies between video frames and thus generate a video in an incremental manner. Our experiments demonstrate our network architecture's ability to distinguish between objects, actions and interactions in a video and combine them to generate videos for unseen captions. The network also exhibits the capability to perform spatio-temporal style transfer when asked to generate videos for a sequence of captions. We also show that the network's ability to learn a latent representation allows it generate videos in an unsupervised manner and perform other tasks such as action recognition.",http://arxiv.org/pdf/1708.05980v2
561,Following Gaze in Video,Adri√_ Recasens; Carl Vondrick; Aditya Khosla; Antonio Torralba,Following Gaze Across Views,"Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze across views by predicting where a particular person is looking throughout a scene. We collect VideoGaze, a new dataset which we use as a benchmark to both train and evaluate models. Given one view with a person in it and a second view of the scene, our model estimates a density for gaze location in the second view. A key aspect of our approach is an end-to-end model that solves the following sub-problems: saliency, gaze pose, and geometric relationships between views. Although our model is supervised only with gaze, we show that the model learns to solve these subproblems automatically without supervision. Experiments suggest that our approach follows gaze better than standard baselines and produces plausible results for everyday situations.",http://arxiv.org/pdf/1612.03094v1
562,Adaptive RNN Tree for Large-Scale Human Action Recognition,Wenbo Li; Longyin Wen; Ming-Ching Chang; Ser Nam Lim; Siwei Lyu,,,
591,Spatio-Temporal Person Retrieval via Natural Language Queries,Masataka Yamaguchi; Kuniaki Saito; Yoshitaka Ushiku; Tatsuya Harada,Spatio-temporal Person Retrieval via Natural Language Queries,"In this paper, we address the problem of spatio-temporal person retrieval from multiple videos using a natural language query, in which we output a tube (i.e., a sequence of bounding boxes) which encloses the person described by the query. For this problem, we introduce a novel dataset consisting of videos containing people annotated with bounding boxes for each second and with five natural language descriptions. To retrieve the tube of the person described by a given natural language query, we design a model that combines methods for spatio-temporal human detection and multimodal retrieval. We conduct comprehensive experiments to compare a variety of tube and text representations and multimodal retrieval methods, and present a strong baseline in this task as well as demonstrate the efficacy of our tube representation and multimodal feature embedding technique. Finally, we demonstrate the versatility of our model by applying it to two other important tasks.",http://arxiv.org/pdf/1704.07945v2
343,Automatic Spatially-Aware Fashion Concept Discovery,Xintong Han; Zuxuan Wu; Phoenix X. Huang; Xiao Zhang; Menglong Zhu; Yuan Li; Yang Zhao; Larry S. Davis,Automatic Spatially-aware Fashion Concept Discovery,"This paper proposes an automatic spatially-aware concept discovery approach using weakly labeled image-text data from shopping websites. We first fine-tune GoogleNet by jointly modeling clothing images and their corresponding descriptions in a visual-semantic embedding space. Then, for each attribute (word), we generate its spatially-aware representation by combining its semantic word vector representation with its spatial representation derived from the convolutional maps of the fine-tuned network. The resulting spatially-aware representations are further used to cluster attributes into multiple groups to form spatially-aware concepts (e.g., the neckline concept might consist of attributes like v-neck, round-neck, etc). Finally, we decompose the visual-semantic embedding space into multiple concept-specific subspaces, which facilitates structured browsing and attribute-feedback product retrieval by exploiting multimodal linguistic regularities. We conducted extensive experiments on our newly collected Fashion200K dataset, and results on clustering quality evaluation and attribute-feedback product retrieval task demonstrate the effectiveness of our automatically discovered spatially-aware concepts.",http://arxiv.org/pdf/1708.01311v1
430,ChromaTag: A Colored Marker and Fast Detection Algorithm,Joseph DeGol; Timothy Bretl; Derek Hoiem,ChromaTag: A Colored Marker and Fast Detection Algorithm,"Current fiducial marker detection algorithms rely on marker IDs for false positive rejection. Time is wasted on potential detections that will eventually be rejected as false positives. We introduce ChromaTag, a fiducial marker and detection algorithm designed to use opponent colors to limit and quickly reject initial false detections and grayscale for precise localization. Through experiments, we show that ChromaTag is significantly faster than current fiducial markers while achieving similar or better detection accuracy. We also show how tag size and viewing direction effect detection accuracy. Our contribution is significant because fiducial markers are often used in real-time applications (e.g. marker assisted robot navigation) where heavy computation is required by other parts of the system.",http://arxiv.org/pdf/1708.02982v1
489,Adversarial Image Perturbation for Privacy Protection ‚Ä A Game Theory Perspective,Seong Joon Oh; Mario Fritz; Bernt Schiele,,,
515,WeText: Scene Text Detection Under Weak Supervision,Shangxuan Tian; Shijian Lu; Chongshou Li,,,
180,Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization,Xun Huang; Serge Belongie,Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization,"Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",http://arxiv.org/pdf/1703.06868v2
253,Photographic Image Synthesis With Cascaded Refinement Networks,Qifeng Chen; Vladlen Koltun,Photographic Image Synthesis with Cascaded Refinement Networks,"We present an approach to synthesizing photographic images conditioned on semantic layouts. Given a semantic label map, our approach produces an image with photographic appearance that conforms to the input layout. The approach thus functions as a rendering engine that takes a two-dimensional semantic specification of the scene and produces a corresponding photographic image. Unlike recent and contemporaneous work, our approach does not rely on adversarial training. We show that photographic images can be synthesized from semantic layouts by a single feedforward network with appropriate structure, trained end-to-end with a direct regression objective. The presented approach scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, the full resolution of our training data. Extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by the presented approach are considerably more realistic than alternative approaches. The results are shown in the supplementary video at https://youtu.be/0fhUJT21-bs",http://arxiv.org/pdf/1707.09405v1
894,SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again,Wadim Kehl; Fabian Manhardt; Federico Tombari; Slobodan Ilic; Nassir Navab,,,
1661,Unsupervised Creation of Parameterized Avatars,Lior Wolf; Yaniv Taigman; Adam Polyak,Unsupervised Creation of Parameterized Avatars,"We study the problem of mapping an input image to a tied pair consisting of a vector of parameters and an image that is created using a graphical engine from the vector of parameters. The mapping's objective is to have the output image as similar as possible to the input image. During training, no supervision is given in the form of matching inputs and outputs.   This learning problem extends two literature problems: unsupervised domain adaptation and cross domain transfer. We define a generalization bound that is based on discrepancy, and employ a GAN to implement a network solution that corresponds to this bound. Experimentally, our method is shown to solve the problem of automatically creating avatars.",http://arxiv.org/pdf/1704.05693v2
1812,Learning for Active 3D Mapping,Karel Zimmermann; Tom√°≈° Pet≈ô√≠ƒçek; Vojtƒõch ≈_alansk√Ω; Tom√°≈° Svoboda,Learning for Active 3D Mapping,"We propose an active 3D mapping method for depth sensors, which allow individual control of depth-measuring rays, such as the newly emerging solid-state lidars. The method simultaneously (i) learns to reconstruct a dense 3D occupancy map from sparse depth measurements, and (ii) optimizes the reactive control of depth-measuring rays. To make the first step towards the online control optimization, we propose a fast prioritized greedy algorithm, which needs to update its cost function in only a small fraction of pos- sible rays. The approximation ratio of the greedy algorithm is derived. An experimental evaluation on the subset of the KITTI dataset demonstrates significant improve- ment in the 3D map accuracy when learning-to-reconstruct from sparse measurements is coupled with the optimization of depth-measuring rays.",http://arxiv.org/pdf/1708.02074v1
648,Toward Perceptually-Consistent Stereo: A Scanline Study,Jialiang Wang; Daniel Glasner; Todd Zickler,,,
788,Surface Normals in the Wild,Weifeng Chen; Donglai Xiang; Jia Deng,Surface Normals in the Wild,We study the problem of single-image depth estimation for images in the wild. We collect human annotated surface normals and use them to train a neural network that directly predicts pixel-wise depth. We propose two novel loss functions for training with surface normal annotations. Experiments on NYU Depth and our own dataset demonstrate that our approach can significantly improve the quality of depth estimation in the wild.,http://arxiv.org/pdf/1704.02956v1
833,Unsupervised Learning of Stereo Matching,Chao Zhou; Hong Zhang; Xiaoyong Shen; Jiaya Jia,,,
838,Unrestricted Facial Geometry Reconstruction Using Image-To-Image Translation,Matan Sela; Elad Richardson; Ron Kimmel,Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation,"It has been recently shown that neural networks can recover the geometric structure of a face from a single given image. A common denominator of most existing face geometry reconstruction methods is the restriction of the solution space to some low-dimensional subspace. While such a model significantly simplifies the reconstruction problem, it is inherently limited in its expressiveness. As an alternative, we propose an Image-to-Image translation network that jointly maps the input image to a depth image and a facial correspondence map. This explicit pixel-based mapping can then be utilized to provide high quality reconstructions of diverse faces under extreme expressions, using a purely geometric refinement process. In the spirit of recent approaches, the network is trained only with synthetic data, and is then evaluated on in-the-wild facial images. Both qualitative and quantitative analyses demonstrate the accuracy and the robustness of our approach.",http://arxiv.org/pdf/1703.10131v2
839,Learned Multi-Patch Similarity,Wilfried Hartmann; Silvano Galliani; Michal Havlena; Luc Van Gool; Konrad Schindler,Learned Multi-Patch Similarity,"Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.",http://arxiv.org/pdf/1703.08836v2
862,Click Here: Human-Localized Keypoints as Guidance for Viewpoint Estimation,Ryan Szeto; Jason J. Corso,Click Here: Human-Localized Keypoints as Guidance for Viewpoint Estimation,"We motivate and address a human-in-the-loop variant of the monocular viewpoint estimation task in which the location and class of one semantic object keypoint is available at test time. In order to leverage the keypoint information, we devise a Convolutional Neural Network called Click-Here CNN (CH-CNN) that integrates the keypoint information with activations from the layers that process the image. It transforms the keypoint information into a 2D map that can be used to weigh features from certain parts of the image more heavily. The weighted sum of these spatial features is combined with global image features to provide relevant information to the prediction layers. To train our network, we collect a novel dataset of 3D keypoint annotations on thousands of CAD models, and synthetically render millions of images with 2D keypoint information. On test instances from PASCAL 3D+, our model achieves a mean class accuracy of 90.7%, whereas the state-of-the-art baseline only obtains 85.7% mean class accuracy, justifying our argument for human-in-the-loop inference.",http://arxiv.org/pdf/1703.09859v2
893,Unsupervised Adaptation for Deep Stereo,Alessio Tonioni; Matteo Poggi; Stefano Mattoccia; Luigi Di Stefano,,,
754,Composite Focus Measure for High Quality Depth Maps,Parikshit Sakurikar; P. J. Narayanan,,,
647,Reconstruction-Based Disentanglement for Pose-Invariant Face Recognition,Xi Peng; Xiang Yu; Kihyuk Sohn; Dimitris N. Metaxas; Manmohan Chandraker,Reconstruction-Based Disentanglement for Pose-invariant Face Recognition,"Deep neural networks (DNNs) trained on large-scale datasets have recently achieved impressive improvements in face recognition. But a persistent challenge remains to develop methods capable of handling large pose variations that are relatively underrepresented in training data. This paper presents a method for learning a feature representation that is invariant to pose, without requiring extensive pose coverage in training data. We first propose to generate non-frontal views from a single frontal face, in order to increase the diversity of training data while preserving accurate facial details that are critical for identity discrimination. Our next contribution is to seek a rich embedding that encodes identity features, as well as non-identity ones such as pose and landmark locations. Finally, we propose a new feature reconstruction metric learning to explicitly disentangle identity and pose, by demanding alignment between the feature reconstructions through various combinations of identity and pose features, which is obtained from two images of the same subject. Experiments on both controlled and in-the-wild face datasets, such as MultiPIE, 300WLP and the profile view database CFP, show that our method consistently outperforms the state-of-the-art, especially on images with large head pose variations. Detail results and resource are referred to https://sites.google.com/site/xipengcshomepage/iccv2017",http://arxiv.org/pdf/1702.03041v2
680,Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection,Shengtao Xiao; Jiashi Feng; Luoqi Liu; Xuecheng Nie; Wei Wang; Shuicheng Yan; Ashraf Kassim,,,
848,Anchored Regression Networks Applied to Age Estimation and Super Resolution,Eirikur Agustsson; Radu Timofte; Luc Van Gool,,,
896,Infant Footprint Recognition,Eryun Liu,,,
670,Self-Paced Kernel Estimation for Robust Blind Image Deblurring,Dong Gong; Mingkui Tan; Yanning Zhang; Anton van den Hengel; Qinfeng Shi,,,
711,Super-Trajectory for Video Segmentation,Wenguan Wang; Jianbing Shen; Jianwen Xie; Fatih Porikli,Super-Trajectory for Video Segmentation,"We introduce a novel semi-supervised video segmentation approach based on an efficient video representation, called as ""super-trajectory"". Each super-trajectory corresponds to a group of compact trajectories that exhibit consistent motion patterns, similar appearance and close spatiotemporal relationships. We generate trajectories using a probabilistic model, which handles occlusions and drifts in a robust and natural way. To reliably group trajectories, we adopt a modified version of the density peaks based clustering algorithm that allows capturing rich spatiotemporal relations among trajectories in the clustering process. The presented video representation is discriminative enough to accurately propagate the initial annotations in the first frame onto the remaining video frames. Extensive experimental analysis on challenging benchmarks demonstrate our method is capable of distinguishing the target objects from complex backgrounds and even reidentifying them after long-term occlusions.",http://arxiv.org/pdf/1702.08634v4
795,Be Your Own Prada: Fashion Synthesis With Structural Coherence,Shizhan Zhu; Raquel Urtasun; Sanja Fidler; Dahua Lin; Chen Change Loy,,,
796,Wavelet-SRNet: A Wavelet-Based CNN for Multi-Scale Face Super Resolution,Huaibo Huang; Ran He; Zhenan Sun; Tieniu Tan,,,
825,Learning Gaze Transitions From Depth to Improve Video Saliency Estimation,George Leifman; Dmitry Rudoy; Tristan Swedish; Eduardo Bayro-Corrochano; Ramesh Raskar,Learning Gaze Transitions from Depth to Improve Video Saliency Estimation,"In this paper we introduce a novel Depth-Aware Video Saliency approach to predict human focus of attention when viewing RGBD videos on regular 2D screens. We train a generative convolutional neural network which predicts a saliency map for a frame, given the fixation map of the previous frame. Saliency estimation in this scenario is highly important since in the near future 3D video content will be easily acquired and yet hard to display. This can be explained, on the one hand, by the dramatic improvement of 3D-capable acquisition equipment. On the other hand, despite the considerable progress in 3D display technologies, most of the 3D displays are still expensive and require wearing special glasses. To evaluate the performance of our approach, we present a new comprehensive database of eye-fixation ground-truth for RGBD videos. Our experiments indicate that integrating depth into video saliency calculation is beneficial. We demonstrate that our approach outperforms state-of-the-art methods for video saliency, achieving 15% relative improvement.",http://arxiv.org/pdf/1603.03669v1
829,Joint Convolutional Analysis and Synthesis Sparse Representation for Single Image Layer Separation,Shuhang Gu; Deyu Meng; Wangmeng Zuo; Lei Zhang,,,
869,Modelling the Scene Dependent Imaging in Cameras With a Deep Neural Network,Seonghyeon Nam; Seon Joo Kim,Modelling the Scene Dependent Imaging in Cameras with a Deep Neural Network,"We present a novel deep learning framework that models the scene dependent image processing inside cameras. Often called as the radiometric calibration, the process of recovering RAW images from processed images (JPEG format in the sRGB color space) is essential for many computer vision tasks that rely on physically accurate radiance values. All previous works rely on the deterministic imaging model where the color transformation stays the same regardless of the scene and thus they can only be applied for images taken under the manual mode. In this paper, we propose a data-driven approach to learn the scene dependent and locally varying image processing inside cameras under the automode. Our method incorporates both the global and the local scene context into pixel-wise features via multi-scale pyramid of learnable histogram layers. The results show that we can model the imaging pipeline of different cameras that operate under the automode accurately in both directions (from RAW to sRGB, from sRGB to RAW) and we show how we can apply our method to improve the performance of image deblurring.",http://arxiv.org/pdf/1707.08350v1
881,Transformed Low-Rank Model for Line Pattern Noise Removal,Yi Chang; Luxin Yan; Sheng Zhong,,,
916,Weakly Supervised Manifold Learning for Dense Semantic Object Correspondence,Utkarsh Gaur; B. S. Manjunath,,,
2864,PanNet: A Deep Network Architecture for Pan-Sharpening,Junfeng Yang; Xueyang Fu; Yuwen Hu; Yue Huang; Xinghao Ding; John Paisley,,,
157,Dual Motion GAN for Future-Flow Embedded Video Prediction,Xiaodan Liang; Lisa Lee; Wei Dai; Eric P. Xing,Dual Motion GAN for Future-Flow Embedded Video Prediction,"Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.",http://arxiv.org/pdf/1708.00284v2
676,Online Robust Image Alignment via Subspace Learning From Gradient Orientations,Qingqing Zheng; Yi Wang; Pheng-Ann Heng,,,
688,Learning Dynamic Siamese Network for Visual Object Tracking,Qing Guo; Wei Feng; Ce Zhou; Rui Huang; Liang Wan; Song Wang,,,
871,High Order Tensor Formulation for Convolutional Sparse Coding,Adel Bibi; Bernard Ghanem,,,
919,Learning Proximal Operators: Using Denoising Networks for Regularizing Inverse Imaging Problems,Tim Meinhardt; Michael M√∂ller; Caner Hazirbas; Daniel Cremers,Learning Proximal Operators: Using Denoising Networks for Regularizing Inverse Imaging Problems,"While variational methods have been among the most powerful tools for solving linear inverse problems in imaging, deep (convolutional) neural networks have recently taken the lead in many challenging benchmarks. A remaining drawback of deep learning approaches is their requirement for an expensive retraining whenever the specific problem, the noise level, noise type, or desired measure of fidelity changes. On the contrary, variational methods have a plug-and-play nature as they usually consist of separate data fidelity and regularization terms.   In this paper we study the possibility of replacing the proximal operator of the regularization used in many convex energy minimization algorithms by a denoising neural network. The latter therefore serves as an implicit natural image prior, while the data term can still be chosen independently. Using a fixed denoising neural network in exemplary problems of image deconvolution with different blur kernels and image demosaicking, we obtain state-of-the-art reconstruction results. These indicate the high generalizability of our approach and a reduction of the need for problem-specific training. Additionally, we discuss novel results on the analysis of possible optimization algorithms to incorporate the network into, as well as the choices of algorithm parameters and their relation to the noise level the neural network is trained on.",http://arxiv.org/pdf/1704.03488v2
649,ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond,Siyuan Qiao; Wei Shen; Weichao Qiu; Chenxi Liu; Alan Yuille,ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond,"Motivated by product detection in supermarkets, this paper studies the problem of object proposal generation in supermarket images and other natural images. We argue that estimation of object scales in images is helpful for generating object proposals, especially for supermarket images where object scales are usually within a small range. Therefore, we propose to estimate object scales of images before generating object proposals. The proposed method for predicting object scales is called ScaleNet. To validate the effectiveness of ScaleNet, we build three supermarket datasets, two of which are real-world datasets used for testing and the other one is a synthetic dataset used for training. In short, we extend the previous state-of-the-art object proposal methods by adding a scale prediction phase. The resulted method outperforms the previous state-of-the-art on the supermarket datasets by a large margin. We also show that the approach works for object proposal on other natural images and it outperforms the previous state-of-the-art object proposal methods on the MS COCO dataset. The supermarket datasets, the virtual supermarkets, and the tools for creating more synthetic datasets will be made public.",http://arxiv.org/pdf/1704.06752v1
664,Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection,Yuan Yuan; Xiaodan Liang; Xiaolong Wang; Dit-Yan Yeung; Abhinav Gupta,Temporal Dynamic Graph LSTM for Action-driven Video Object Detection,"In this paper, we investigate a weakly-supervised object detection framework. Most existing frameworks focus on using static images to learn object detectors. However, these detectors often fail to generalize to videos because of the existing domain shift. Therefore, we investigate learning these detectors directly from boring videos of daily activities. Instead of using bounding boxes, we explore the use of action descriptions as supervision since they are relatively easy to gather. A common issue, however, is that objects of interest that are not involved in human actions are often absent in global action descriptions known as ""missing label"". To tackle this problem, we propose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph LSTM). TD-Graph LSTM enables global temporal reasoning by constructing a dynamic graph that is based on temporal correlations of object proposals and spans the entire video. The missing label issue for each individual frame can thus be significantly alleviated by transferring knowledge across correlated objects proposals in the whole video. Extensive evaluations on a large-scale daily-life action dataset (i.e., Charades) demonstrates the superiority of our proposed method. We also release object bounding-box annotations for more than 5,000 frames in Charades. We believe this annotated data can also benefit other research on video-based object recognition in the future.",http://arxiv.org/pdf/1708.00666v1
667,VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation,Chuang Gan; Yandong Li; Haoxiang Li; Chen Sun; Boqing Gong,VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation,"Rich and dense human labeled datasets are among the main enabling factors for the recent advance on vision-language understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes --- and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.",http://arxiv.org/pdf/1708.04686v1
675,Multi-Modal Factorized Bilinear Pooling With Co-Attention Learning for Visual Question Answering,Zhou Yu; Jun Yu; Jianping Fan; Dacheng Tao,Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering,"Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multi-modal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multi-modal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a co-attention mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-the-art performance on the real-world VQA dataset. Code available at https://github.com/yuzcccc/mfb.",http://arxiv.org/pdf/1708.01471v1
682,SCNet: Learning Semantic Correspondence,Kai Han; Rafael S. Rezende; Bumsub Ham; Kwan-Yee K. Wong; Minsu Cho; Cordelia Schmid; Jean Ponce,SCNet: Learning Semantic Correspondence,"This paper addresses the problem of establishing semantic correspondences between images depicting different instances of the same object or scene category. Previous approaches focus on either combining a spatial regularizer with hand-crafted features, or learning a correspondence model for appearance only. We propose instead a convolutional neural network architecture, called SCNet, for learning a geometrically plausible model for semantic correspondence. SCNet uses region proposals as matching primitives, and explicitly incorporates geometric consistency in its loss function. It is trained on image pairs obtained from the PASCAL VOC 2007 keypoint dataset, and a comparative evaluation on several standard benchmarks demonstrates that the proposed approach substantially outperforms both recent deep learning architectures and previous methods based on hand-crafted features.",http://arxiv.org/pdf/1705.04043v3
690,Soft Proposal Networks for Weakly Supervised Object Localization,Yi Zhu; Yanzhao Zhou; Qixiang Ye; Qiang Qiu; Jianbin Jiao,Soft Proposal Networks for Weakly Supervised Object Localization,"Weakly supervised object localization remains challenging, where only image labels instead of bounding boxes are available during training. Object proposal is an effective component in localization, but often computationally expensive and incapable of joint optimization with some of the remaining modules. In this paper, to the best of our knowledge, we for the first time integrate weakly supervised object proposal into convolutional neural networks (CNNs) in an end-to-end learning manner. We design a network component, Soft Proposal (SP), to be plugged into any standard convolutional architecture to introduce the nearly cost-free object proposal, orders of magnitude faster than state-of-the-art methods. In the SP-augmented CNNs, referred to as Soft Proposal Networks (SPNs), iteratively evolved object proposals are generated based on the deep feature maps then projected back, and further jointly optimized with network parameters, with image-level supervision only. Through the unified learning process, SPNs learn better object-centric filters, discover more discriminative visual evidence, and suppress background interference, significantly boosting both weakly supervised object localization and classification performance. We report the best results on popular benchmarks, including PASCAL VOC, MS COCO, and ImageNet.",http://arxiv.org/pdf/1709.01829v1
695,Class Rectification Hard Mining for Imbalanced Deep Learning,Qi Dong; Shaogang Gong; Xiatian Zhu,,,
717,Generating High-Quality Crowd Density Maps Using Contextual Pyramid CNNs,Vishwanath A. Sindagi; Vishal M. Patel,Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs,"We present a novel method called Contextual Pyramid CNN (CP-CNN) for generating high-quality crowd density and count estimation by explicitly incorporating global and local contextual information of crowd images. The proposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local Context Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN). GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes. DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN. To generate high resolution and high-quality density maps, F-CNN uses a set of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion using a combination of adversarial loss and pixel-level Euclidean loss. Extensive experiments on highly challenging datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.",http://arxiv.org/pdf/1708.00953v1
719,"See the Glass Half Full: Reasoning About Liquid Containers, Their Volume and Content",Roozbeh Mottaghi; Connor Schenck; Dieter Fox; Ali Farhadi,"See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content","Humans have rich understanding of liquid containers and their contents; for example, we can effortlessly pour water from a pitcher to a cup. Doing so requires estimating the volume of the cup, approximating the amount of water in the pitcher, and predicting the behavior of water when we tilt the pitcher. Very little attention in computer vision has been made to liquids and their containers. In this paper, we study liquid containers and their contents, and propose methods to estimate the volume of containers, approximate the amount of liquid in them, and perform comparative volume estimations all from a single RGB image. Furthermore, we show the results of the proposed model for predicting the behavior of liquids inside containers when one tilts the containers. We also introduce a new dataset of Containers Of liQuid contEnt (COQE) that contains more than 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models.",http://arxiv.org/pdf/1701.02718v2
732,Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding,Zhenxing Niu; Mo Zhou; Le Wang; Xinbo Gao; Gang Hua,,,
735,Identity-Aware Textual-Visual Matching With Latent Co-Attention,Shuang Li; Tong Xiao; Hongsheng Li; Wei Yang; Xiaogang Wang,Identity-Aware Textual-Visual Matching with Latent Co-attention,"Textual-visual matching aims at measuring similarities between sentence descriptions and images. Most existing methods tackle this problem without effectively utilizing identity-level annotations. In this paper, we propose an identity-aware two-stage framework for the textual-visual matching problem. Our stage-1 CNN-LSTM network learns to embed cross-modal features with a novel Cross-Modal Cross-Entropy (CMCE) loss. The stage-1 network is able to efficiently screen easy incorrect matchings and also provide initial training point for the stage-2 training. The stage-2 CNN-LSTM network refines the matching results with a latent co-attention mechanism. The spatial attention relates each word with corresponding image regions while the latent semantic attention aligns different sentence structures to make the matching results more robust to sentence structure variations. Extensive experiments on three datasets with identity-level annotations show that our framework outperforms state-of-the-art approaches by large margins.",http://arxiv.org/pdf/1708.01988v1
747,Learning Deep Neural Networks for Vehicle Re-ID With Visual-Spatio-Temporal Path Proposals,Yantao Shen; Tong Xiao; Hongsheng Li; Shuai Yi; Xiaogang Wang,Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals,"Vehicle re-identification is an important problem and has many applications in video surveillance and intelligent transportation. It gains increasing attention because of the recent advances of person re-identification techniques. However, unlike person re-identification, the visual differences between pairs of vehicle images are usually subtle and even challenging for humans to distinguish. Incorporating additional spatio-temporal information is vital for solving the challenging re-identification task. Existing vehicle re-identification methods ignored or used over-simplified models for the spatio-temporal relations between vehicle images. In this paper, we propose a two-stage framework that incorporates complex spatio-temporal information for effectively regularizing the re-identification results. Given a pair of vehicle images with their spatio-temporal information, a candidate visual-spatio-temporal path is first generated by a chain MRF model with a deeply learned potential function, where each visual-spatio-temporal state corresponds to an actual vehicle image with its spatio-temporal information. A Siamese-CNN+Path-LSTM model takes the candidate path as well as the pairwise queries to generate their similarity score. Extensive experiments and analysis show the effectiveness of our proposed method and individual components.",http://arxiv.org/pdf/1708.03918v1
752,Learning From Noisy Labels With Distillation,Yuncheng Li; Jianchao Yang; Yale Song; Liangliang Cao; Jiebo Luo; Li-Jia Li,Learning from Noisy Labels with Distillation,"The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, the label noises have been treated as statistical outliers, and approaches such as importance re-weighting and bootstrap have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multi-mode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use side information, including a small clean dataset and label relations in knowledge graph, to ""hedge the risk"" of learning from noisy labels. Furthermore, unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.",http://arxiv.org/pdf/1703.02391v2
767,DSOD: Learning Deeply Supervised Object Detectors From Scratch,Zhiqiang Shen; Zhuang Liu; Jianguo Li; Yu-Gang Jiang; Yurong Chen; Xiangyang Xue,DSOD: Learning Deeply Supervised Object Detectors from Scratch,"We present Deeply Supervised Object Detector (DSOD), a framework that can learn object detectors from scratch. State-of-the-art object objectors rely heavily on the off-the-shelf networks pre-trained on large-scale classification datasets like ImageNet, which incurs learning bias due to the difference on both the loss functions and the category distributions between classification and detection tasks. Model fine-tuning for the detection task could alleviate this bias to some extent but not fundamentally. Besides, transferring pre-trained models from classification to detection between discrepant domains is even more difficult (e.g. RGB to depth images). A better solution to tackle these two critical problems is to train object detectors from scratch, which motivates our proposed DSOD. Previous efforts in this direction mostly failed due to much more complicated loss functions and limited training data in object detection. In DSOD, we contribute a set of design principles for training object detectors from scratch. One of the key findings is that deep supervision, enabled by dense layer-wise connections, plays a critical role in learning a good detector. Combining with several other principles, we develop DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better results than the state-of-the-art solutions with much more compact models. For instance, DSOD outperforms SSD on all three benchmarks with real-time detection speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster RCNN. Our code and models are available at: https://github.com/szq0214/DSOD .",http://arxiv.org/pdf/1708.01241v1
784,Phrase Localization and Visual Relationship Detection With Comprehensive Image-Language Cues,Bryan A. Plummer; Arun Mallya; Christopher M. Cervantes; Julia Hockenmaier; Svetlana Lazebnik,Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues,"This paper presents a framework for localization or grounding of phrases in images using a large collection of linguistic and visual cues. We model the appearance, size, and position of entity bounding boxes, adjectives that contain attribute information, and spatial relationships between pairs of entities connected by verbs or prepositions. Special attention is given to relationships between people and clothing or body part mentions, as they are useful for distinguishing individuals. We automatically learn weights for combining these cues and at test time, perform joint inference over all phrases in a caption. The resulting system produces state of the art performance on phrase localization on the Flickr30k Entities dataset and visual relationship detection on the Stanford VRD dataset.",http://arxiv.org/pdf/1611.06641v4
798,Chained Cascade Network for Object Detection,Wanli Ouyang; Kun Wang; Xin Zhu; Xiaogang Wang,Learning Chained Deep Features and Classifiers for Cascade in Object Detection,"Cascade is a widely used approach that rejects obvious negative samples at early stages for learning better classifier and faster inference. This paper presents chained cascade network (CC-Net). In this CC-Net, the cascaded classifier at a stage is aided by the classification scores in previous stages. Feature chaining is further proposed so that the feature learning for the current cascade stage uses the features in previous stages as the prior information. The chained ConvNet features and classifiers of multiple stages are jointly learned in an end-to-end network. In this way, features and classifiers at latter stages handle more difficult samples with the help of features and classifiers in previous stages. It yields consistent boost in detection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined with better region proposal, CC-Net leads to state-of-the-art result of 81.1% mAP on PASCAL VOC 2007.",http://arxiv.org/pdf/1702.07054v1
822,VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition,Seokju Lee; Junsik Kim; Jae Shin Yoon; Seunghak Shin; Oleksandr Bailo; Namil Kim; Tae-Hee Lee; Hyun Seok Hong; Seung-Hoon Han; In So Kweon,,,
882,Unsupervised Learning of Important Objects From First-Person Videos,Gedas Bertasius; Hyun Soo Park; Stella X. Yu; Jianbo Shi,Unsupervised Learning of Important Objects from First-Person Videos,"A first-person camera, placed at a person's head, captures, which objects are important to the camera wearer. Most prior methods for this task learn to detect such important objects from the manually labeled first-person data in a supervised fashion. However, important objects are strongly related to the camera wearer's internal state such as his intentions and attention, and thus, only the person wearing the camera can provide the importance labels. Such a constraint makes the annotation process costly and limited in scalability.   In this work, we show that we can detect important objects in first-person images without the supervision by the camera wearer or even third-person labelers. We formulate an important detection problem as an interplay between the 1) segmentation and 2) recognition agents. The segmentation agent first proposes a possible important object segmentation mask for each image, and then feeds it to the recognition agent, which learns to predict an important object mask using visual semantics and spatial features.   We implement such an interplay between both agents via an alternating cross-pathway supervision scheme inside our proposed Visual-Spatial Network (VSN). Our VSN consists of spatial (""where"") and visual (""what"") pathways, one of which learns common visual semantics while the other focuses on the spatial location cues. Our unsupervised learning is accomplished via a cross-pathway supervision, where one pathway feeds its predictions to a segmentation agent, which proposes a candidate important object segmentation mask that is then used by the other pathway as a supervisory signal. We show our method's success on two different important object datasets, where our method achieves similar or better results as the supervised methods.",http://arxiv.org/pdf/1611.05335v3
899,An Analysis of Visual Question Answering Algorithms,Kushal Kafle; Christopher Kanan,An Analysis of Visual Question Answering Algorithms,"In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.",http://arxiv.org/pdf/1703.09684v2
906,Visual Relationship Detection With Internal and External Linguistic Knowledge Distillation,Ruichi Yu; Ang Li; Vlad I. Morariu; Larry S. Davis,Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation,"Understanding visual relationships involves identifying the subject, the object, and a predicate relating them. We leverage the strong correlations between the predicate and the (subj,obj) pair (both semantically and spatially) to predict the predicates conditioned on the subjects and the objects. Modeling the three entities jointly more accurately reflects their relationships, but complicates learning since the semantic space of visual relationships is huge and the training data is limited, especially for the long-tail relationships that have few instances. To overcome this, we use knowledge of linguistic statistics to regularize visual model learning. We obtain linguistic knowledge by mining from both training annotations (internal knowledge) and publicly available text, e.g., Wikipedia (external knowledge), computing the conditional probability distribution of a predicate given a (subj,obj) pair. Then, we distill the knowledge into a deep model to achieve better generalization. Our experimental results on the Visual Relationship Detection (VRD) and Visual Genome datasets suggest that with this linguistic knowledge distillation, our model outperforms the state-of-the-art methods significantly, especially when predicting unseen relationships (e.g., recall improved from 8.45% to 19.17% on VRD zero-shot testing set).",http://arxiv.org/pdf/1707.09423v2
920,A Two Stream Siamese Convolutional Neural Network for Person Re-Identification,Dahjung Chung; Khalid Tahboub; Edward J. Delp,,,
1842,Joint Learning of Object and Action Detectors,Vicky Kalogeiton; Philippe Weinzaepfel; Vittorio Ferrari; Cordelia Schmid,Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge,"This paper addresses the problem of joint detection and recounting of abnormal events in videos. Recounting of abnormal events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task in video surveillance, because it helps human observers quickly judge if they are false alarms or not. To describe the events in the human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object and action) is crucial. Although convolutional neural networks (CNNs) have achieved promising results in learning such concepts, it remains an open question as to how to effectively use CNNs for abnormal event detection, mainly due to the environment-dependent nature of the anomaly detection. In this paper, we tackle this problem by integrating a generic CNN model and environment-dependent anomaly detectors. Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. By appropriately plugging the model into anomaly detectors, we can detect and recount abnormal events while taking advantage of the discriminative power of CNNs. Our approach outperforms the state-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event detection and also produces promising results of abnormal event recounting.",http://arxiv.org/pdf/1709.09121v1
694,No More Discrimination: Cross City Adaptation of Road Scene Segmenters,Yi-Hsin Chen; Wei-Yu Chen; Yu-Ting Chen; Bo-Cheng Tsai; Yu-Chiang Frank Wang; Min Sun,No More Discrimination: Cross City Adaptation of Road Scene Segmenters,"Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its time-machine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.",http://arxiv.org/pdf/1704.08509v1
710,Open Vocabulary Scene Parsing,Hang Zhao; Xavier Puig; Bolei Zhou; Sanja Fidler; Antonio Torralba,Pyramid Scene Parsing Network,"Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",http://arxiv.org/pdf/1612.01105v2
718,Learned Watershed: End-To-End Learning of Seeded Segmentation,Steffen Wolf; Lukas Schott; Ullrich K√∂the; Fred Hamprecht,Learned Watershed: End-to-End Learning of Seeded Segmentation,"Learned boundary maps are known to outperform hand- crafted ones as a basis for the watershed algorithm. We show, for the first time, how to train watershed computation jointly with boundary map prediction. The estimator for the merging priorities is cast as a neural network that is con- volutional (over space) and recurrent (over iterations). The latter allows learning of complex shape priors. The method gives the best known seeded segmentation results on the CREMI segmentation challenge.",http://arxiv.org/pdf/1704.02249v2
725,Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes,Yang Zhang; Philip David; Boqing Gong,Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes,"During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is a core task of various emerging industrial applications such as autonomous driving and medical imaging. However, to train CNNs requires a huge amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNN models on photo-realistic synthetic data with computer-generated annotations. Despite this, the domain mismatch between the real images and the synthetic data significantly decreases the models' performance. Hence we propose a curriculum-style learning approach to minimize the domain gap in semantic segmentation. The curriculum domain adaptation solves easy tasks first in order to infer some necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban traffic scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train the segmentation network in such a way that the network predictions in the target domain follow those inferred properties. In experiments, our method significantly outperforms the baselines as well as the only known existing approach to the same problem.",http://arxiv.org/pdf/1707.09465v2
729,Scale-Adaptive Convolutions for Scene Parsing,Rui Zhang; Sheng Tang; Yongdong Zhang; Jintao Li; Shuicheng Yan,Recurrent Scene Parsing with Perspective Understanding in the Loop,"Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process an image at a fixed resolution. We propose a depth-aware gating module that adaptively chooses the pooling field size in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details can be preserved for objects at distance and a larger receptive field can be used for objects nearer to the camera. The depth gating signal is provided from stereo disparity (when available) or estimated directly from a single image. We integrate this depth-aware gating into a recurrent convolutional neural network trained in an end-to-end fashion to perform semantic segmentation. Our recurrent module iteratively refines the segmentation results, leveraging the depth estimate and output prediction from the previous loop. Through extensive experiments on three popular large-scale RGB-D datasets, we demonstrate our approach achieves competitive semantic segmentation performance using more compact model than existing methods. Interestingly, we find segmentation performance improves when we estimate depth directly from the image rather than using ""ground-truth"" and the model produces state-of-the-art results for quantitative depth estimation from a single image.",http://arxiv.org/pdf/1705.07238v1
641,Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption,Ryo Yonetani; Vishnu Naresh Boddeti; Kris M. Kitani; Yoichi Sato,Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption,"We propose a privacy-preserving framework for learning visual classifiers by leveraging distributed private image data. This framework is designed to aggregate multiple classifiers updated locally using private data and to ensure that no private information about the data is exposed during and after its learning procedure. We utilize a homomorphic cryptosystem that can aggregate the local classifiers while they are encrypted and thus kept secret. To overcome the high computational cost of homomorphic encryption of high-dimensional classifiers, we (1) impose sparsity constraints on local classifier updates and (2) propose a novel efficient encryption scheme named doubly-permuted homomorphic encryption (DPHE) which is tailored to sparse high-dimensional data. DPHE (i) decomposes sparse data into its constituent non-zero values and their corresponding support indices, (ii) applies homomorphic encryption only to the non-zero values, and (iii) employs double permutations on the support indices to make them secret. Our experimental evaluation on several public datasets shows that the proposed approach achieves comparable performance against state-of-the-art visual recognition methods while preserving privacy and significantly outperforms other privacy-preserving methods.",http://arxiv.org/pdf/1704.02203v2
697,Multi-Task Self-Supervised Visual Learning,Carl Doersch; Andrew Zisserman,Multi-task Self-Supervised Visual Learning,"We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for ""harmonizing"" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.",http://arxiv.org/pdf/1708.07860v1
713,A Self-Balanced Min-Cut Algorithm for Image Clustering,Xiaojun Chen; Joshua Zhexue Haung; Feiping Nie; Renjie Chen; Qingyao Wu,,,
733,Is Second-Order Information Helpful for Large-Scale Visual Recognition?,Peihua Li; Jiangtao Xie; Qilong Wang; Wangmeng Zuo,,,
820,Factorized Bilinear Models for Image Recognition,Yanghao Li; Naiyan Wang; Jiaying Liu; Xiaodi Hou,Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering,"Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multi-modal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multi-modal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a co-attention mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-the-art performance on the real-world VQA dataset. Code available at https://github.com/yuzcccc/mfb.",http://arxiv.org/pdf/1708.01471v1
828,Octree Generating Networks: Efficient Convolutional Architectures for High-Resolution 3D Outputs,Maxim Tatarchenko; Alexey Dosovitskiy; Thomas Brox,Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs,"We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute- and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image.",http://arxiv.org/pdf/1703.09438v3
836,Truncating Wide Networks Using Binary Tree Architectures,Yan Zhang; Mete Ozay; Shuohao Li; Takayuki Okatani,Truncating Wide Networks using Binary Tree Architectures,"Recent study shows that a wide deep network can obtain accuracy comparable to a deeper but narrower network. Compared to narrower and deeper networks, wide networks employ relatively less number of layers and have various important benefits, such that they have less running time on parallel computing devices, and they are less affected by gradient vanishing problems. However, the parameter size of a wide network can be very large due to use of large width of each layer in the network. In order to keep the benefits of wide networks meanwhile improve the parameter size and accuracy trade-off of wide networks, we propose a binary tree architecture to truncate architecture of wide networks by reducing the width of the networks. More precisely, in the proposed architecture, the width is continuously reduced from lower layers to higher layers in order to increase the expressive capacity of network with a less increase on parameter size. Also, to ease the gradient vanishing problem, features obtained at different layers are concatenated to form the output of our architecture. By employing the proposed architecture on a baseline wide network, we can construct and train a new network with same depth but considerably less number of parameters. In our experimental analyses, we observe that the proposed architecture enables us to obtain better parameter size and accuracy trade-off compared to baseline networks using various benchmark image classification datasets. The results show that our model can decrease the classification error of baseline from 20.43% to 19.22% on Cifar-100 using only 28% of parameters that baseline has. Code is available at https://github.com/ZhangVision/bitnet.",http://arxiv.org/pdf/1704.00509v1
679,Bringing Background Into the Foreground: Making All Classes Equal in Weakly-Supervised Video Semantic Segmentation,Fatemeh Sadat Saleh; Mohammad Sadegh Aliakbarian; Mathieu Salzmann; Lars Petersson; Jose M. √Ålvarez,Bringing Background into the Foreground: Making All Classes Equal in Weakly-supervised Video Semantic Segmentation,"Pixel-level annotations are expensive and time-consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recent years have seen great progress in weakly-supervised semantic segmentation, whether from a single image or from videos. However, most existing methods are designed to handle a single background class. In practical applications, such as autonomous navigation, it is often crucial to reason about multiple background classes. In this paper, we introduce an approach to doing so by making use of classifier heatmaps. We then develop a two-stream deep architecture that jointly leverages appearance and motion, and design a loss based on our heatmaps to train it. Our experiments demonstrate the benefits of our classifier heatmaps and of our two-stream architecture on challenging urban scene datasets and on the YouTube-Objects benchmark, where we obtain state-of-the-art results.",http://arxiv.org/pdf/1708.04400v1
683,View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data,Pengfei Zhang; Cuiling Lan; Junliang Xing; Wenjun Zeng; Jianru Xue; Nanning Zheng,View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data,"Skeleton-based human action recognition has recently attracted increasing attention due to the popularity of 3D skeleton data. One main challenge lies in the large view variations in captured human actions. We propose a novel view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. Rather than re-positioning the skeletons based on a human defined prior criterion, we design a view adaptive recurrent neural network (RNN) with LSTM architecture, which enables the network itself to adapt to the most suitable observation viewpoints from end to end. Extensive experiment analyses show that the proposed view adaptive RNN model strives to (1) transform the skeletons of various views to much more consistent viewpoints and (2) maintain the continuity of the action rather than transforming every frame to the same position with the same body orientation. Our model achieves significant improvement over the state-of-the-art approaches on three benchmark datasets.",http://arxiv.org/pdf/1703.08274v2
703,Joint Discovery of Object States and Manipulation Actions,Jean-Baptiste Alayrac; Ivan Laptev; Josef Sivic; Simon Lacoste-Julien,Joint Discovery of Object States and Manipulation Actions,"Many human activities involve object manipulations aiming to modify the object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulation actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost with constraints. We assume a consistent temporal order for the changes in object states and manipulation actions, and introduce new optimization techniques to learn model parameters without additional supervision. We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations. We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.",http://arxiv.org/pdf/1702.02738v3
707,What Actions Are Needed for Understanding Human Actions in Videos?,Gunnar A. Sigurdsson; Olga Russakovsky; Abhinav Gupta,,,
736,Lattice Long Short-Term Memory for Human Action Recognition,Lin Sun; Kui Jia; Kevin Chen; Dit-Yan Yeung; Bertram E. Shi; Silvio Savarese,Lattice Long Short-Term Memory for Human Action Recognition,"Human actions captured in video sequences are three-dimensional signals characterizing visual appearance and motion dynamics. To learn action patterns, existing methods adopt Convolutional and/or Recurrent Neural Networks (CNNs and RNNs). CNN based methods are effective in learning spatial appearances, but are limited in modeling long-term motion dynamics. RNNs, especially Long Short-Term Memory (LSTM), are able to learn temporal motion dynamics. However, naively applying RNNs to video sequences in a convolutional manner implicitly assumes that motions in videos are stationary across different spatial locations. This assumption is valid for short-term motions but invalid when the duration of the motion is long.   In this work, we propose Lattice-LSTM (L2STM), which extends LSTM by learning independent hidden state transitions of memory cells for individual spatial locations. This method effectively enhances the ability to model dynamics across time and addresses the non-stationary issue of long-term motion dynamics without significantly increasing the model complexity. Additionally, we introduce a novel multi-modal training procedure for training our network. Unlike traditional two-stream architectures which use RGB and optical flow information as input, our two-stream model leverages both modalities to jointly train both input gates and both forget gates in the network rather than treating the two streams as separate entities with no information about the other. We apply this end-to-end system to benchmark datasets (UCF-101 and HMDB-51) of human action recognition. Experiments show that on both datasets, our proposed method outperforms all existing ones that are based on LSTM and/or CNNs of similar model complexities.",http://arxiv.org/pdf/1708.03958v1
750,Common Action Discovery and Localization in Unconstrained Videos,Jiong Yang; Junsong Yuan,,,
760,Pixel-Level Matching for Video Object Segmentation Using Convolutional Neural Networks,Jae Shin Yoon; Francois Rameau; Junsik Kim; Seokju Lee; Seunghak Shin; In So Kweon,Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks,"We propose a novel video object segmentation algorithm based on pixel-level matching using Convolutional Neural Networks (CNN). Our network aims to distinguish the target area from the background on the basis of the pixel-level similarity between two object units. The proposed network represents a target object using features from different depth layers in order to take advantage of both the spatial details and the category-level semantic information. Furthermore, we propose a feature compression technique that drastically reduces the memory requirements while maintaining the capability of feature representation. Two-stage training (pre-training and fine-tuning) allows our network to handle any target object regardless of its category (even if the object's type does not belong to the pre-training data) or of variations in its appearance through a video sequence. Experiments on large datasets demonstrate the effectiveness of our model - against related methods - in terms of accuracy, speed, and stability. Finally, we introduce the transferability of our network to different domains, such as the infrared data domain.",http://arxiv.org/pdf/1708.05137v1
883,Am I a Baller? Basketball Performance Assessment From First-Person Videos,Gedas Bertasius; Hyun Soo Park; Stella X. Yu; Jianbo Shi,,,
712,Deep Cropping via Attention Box Prediction and Aesthetics Assessment,Wenguan Wang; Jianbing Shen,,,
724,Raster-To-Vector: Revisiting Floorplan Transformation,Chen Liu; Jiajun Wu; Pushmeet Kohli; Yasutaka Furukawa,,,
772,Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework,Michal Bu≈°ta; Luk√°≈° Neumann; Ji≈ô√≠ Matas,,,
257,Playing for Benchmarks,Stephan R. Richter; Zeeshan Hayder; Vladlen Koltun,Hierarchical benchmark graphs for testing community detection algorithms,"Hierarchical organization is an important, prevalent characteristic of complex systems; in order to understand their organization, the study of the underlying (generally complex) networks that describe the interactions between their constituents plays a central role. Numerous previous works have shown that many real-world networks in social, biologic and technical systems present hierarchical organization, often in the form of a hierarchy of community structures. Many artificial benchmark graphs have been proposed in order to test different community detection methods, but no benchmark has been developed to throughly test the detection of hierarchical community structures. In this study, we fill this vacancy by extending the Lancichinetti-Fortunato-Radicchi (LFR) ensemble of benchmark graphs, adopting the rule of constructing hierarchical networks proposed by Ravasz and Barab\'asi. We employ this benchmark to test three of the most popular community detection algorithms, and quantify their accuracy using the traditional Mutual Information and the recently introduced Hierarchical Mutual Information. The results indicate that the Ravasz-Barab\'asi-Lancichinetti-Fortunato-Radicchi (RB-LFR) benchmark generates a complex hierarchical structure constituting a challenging benchmark for the considered community detection methods.",http://arxiv.org/pdf/1708.06969v1
488,Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks,Jun-Yan Zhu; Taesung Park; Phillip Isola; Alexei A. Efros,Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,"Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",http://arxiv.org/pdf/1703.10593v1
606,GANs for Biological Image Synthesis,Anton Osokin; Anatole Chessel; Rafael E. Carazo Salas; Federico Vaggi,GANs for Biological Image Synthesis,"In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.",http://arxiv.org/pdf/1708.04692v2
753,Learning to Synthesize a 4D RGBD Light Field From a Single Image,Pratul P. Srinivasan; Tongzhou Wang; Ashwin Sreelal; Ravi Ramamoorthi; Ren Ng,Learning to Synthesize a 4D RGBD Light Field from a Single Image,"We present a machine learning algorithm that takes as input a 2D RGB image and synthesizes a 4D RGBD light field (color and depth of the scene in each ray direction). For training, we introduce the largest public light field dataset, consisting of over 3300 plenoptic camera light fields of scenes containing flowers and plants. Our synthesis pipeline consists of a convolutional neural network (CNN) that estimates scene geometry, a stage that renders a Lambertian light field using that geometry, and a second CNN that predicts occluded rays and non-Lambertian effects. Our algorithm builds on recent view synthesis methods, but is unique in predicting RGBD for each light field ray and improving unsupervised single image depth estimation by enforcing consistency of ray depths that should intersect the same scene point. Please see our supplementary video at https://youtu.be/yLCvWoQLnms",http://arxiv.org/pdf/1708.03292v1
863,Neural EPI-Volume Networks for Shape From Light Field,Stefan Heber; Wei Yu; Thomas Pock,,,
1936,Material Editing Using a Physically Based Rendering Network,Guilin Liu; Duygu Ceylan; Ersin Yumer; Jimei Yang; Jyh-Ming Lien,Material Editing Using a Physically Based Rendering Network,"The ability to edit materials of objects in images is desirable by many content creators. However, this is an extremely challenging task as it requires to disentangle intrinsic physical properties of an image. We propose an end-to-end network architecture that replicates the forward image formation process to accomplish this task. Specifically, given a single image, the network first predicts intrinsic properties, i.e. shape, illumination, and material, which are then provided to a rendering layer. This layer performs in-network image synthesis, thereby enabling the network to understand the physics behind the image formation process. The proposed rendering layer is fully differentiable, supports both diffuse and specular materials, and thus can be applicable in a variety of problem settings. We demonstrate a rich set of visually plausible material editing examples and provide an extensive comparative study.",http://arxiv.org/pdf/1708.00106v2
1983,Turning Corners Into Cameras: Principles and Methods,Katherine L. Bouman; Vickie Ye; Adam B. Yedidia; Fr√©do Durand; Gregory W. Wornell; Antonio Torralba; William T. Freeman,,,
2270,Linear Differential Constraints for Photo-Polarimetric Height Estimation,Silvia Tozza; William A. P. Smith; Dizhong Zhu; Ravi Ramamoorthi; Edwin R. Hancock,Linear Differential Constraints for Photo-polarimetric Height Estimation,"In this paper we present a differential approach to photo-polarimetric shape estimation. We propose several alternative differential constraints based on polarisation and photometric shading information and show how to express them in a unified partial differential system. Our method uses the image ratios technique to combine shading and polarisation information in order to directly reconstruct surface height, without first computing surface normal vectors. Moreover, we are able to remove the non-linearities so that the problem reduces to solving a linear differential problem. We also introduce a new method for estimating a polarisation image from multichannel data and, finally, we show it is possible to estimate the illumination directions in a two source setup, extending the method into an uncalibrated scenario. From a numerical point of view, we use a least-squares formulation of the discrete version of the problem. To the best of our knowledge, this is the first work to consider a unified differential approach to solve photo-polarimetric shape estimation directly for height. Numerical results on synthetic and real-world data confirm the effectiveness of our proposed method.",http://arxiv.org/pdf/1708.07718v1
935,Polynomial Solvers for Saturated Ideals,Viktor Larsson; Kalle √str√∂m; Magnus Oskarsson,,,
946,Shape Inpainting Using 3D Generative Adversarial Network and Recurrent Convolutional Networks,Weiyue Wang; Qiangui Huang; Suya You; Chao Yang; Ulrich Neumann,,,
948,SurfaceNet: An End-To-End 3D Neural Network for Multiview Stereopsis,Mengqi Ji; Juergen Gall; Haitian Zheng; Yebin Liu; Lu Fang,SurfaceNet: An End-to-end 3D Neural Network for Multiview Stereopsis,This paper proposes an end-to-end learning framework for multiview stereopsis. We term the network SurfaceNet. It takes a set of images and their corresponding camera parameters as input and directly infers the 3D model. The key advantage of the framework is that both photo-consistency as well geometric relations of the surface structure can be directly learned for the purpose of multiview stereopsis in an end-to-end fashion. SurfaceNet is a fully 3D convolutional network which is achieved by encoding the camera parameters together with the images in a 3D voxel representation. We evaluate SurfaceNet on the large-scale DTU benchmark.,http://arxiv.org/pdf/1708.01749v1
986,Making Minimal Solvers for Absolute Pose Estimation Compact and Robust,Viktor Larsson; Zuzana Kukelova; Yinqiang Zheng,,,
1033,3D Surface Detail Enhancement From a Single Normal Map,Wuyuan Xie; Miaohui Wang; Xianbiao Qi; Lei Zhang,,,
1060,RMPE: Regional Multi-Person Pose Estimation,Hao-Shu Fang; Shuqin Xie; Yu-Wing Tai; Cewu Lu,RMPE: Regional Multi-person Pose Estimation,"Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.Our model and source codes are publicly available.",http://arxiv.org/pdf/1612.00137v4
1061,Online Video Object Detection Using Association LSTM,Yongyi Lu; Cewu Lu; Chi-Keung Tang,,,
1127,PolyFit: Polygonal Surface Reconstruction From Point Clouds,Liangliang Nan; Peter Wonka,,,
1190,Progressive Large Scale-Invariant Image Matching in Scale Space,Lei Zhou; Siyu Zhu; Tianwei Shen; Jinglu Wang; Tian Fang; Long Quan,,,
1229,Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map,Liu Liu; Hongdong Li; Yuchao Dai,,,
1269,Multi-View Non-Rigid Refinement and Normal Selection for High Quality 3D Reconstruction,Sk. Mohammadul Haque; Venu Madhav Govindu,,,
933,Multi-Stage Multi-Recursive-Input Fully Convolutional Networks for Neuronal Boundary Detection,Wei Shen; Bin Wang; Yuan Jiang; Yan Wang; Alan Yuille,Multi-stage Multi-recursive-input Fully Convolutional Networks for Neuronal Boundary Detection,"In the field of connectomics, neuroscientists seek to identify cortical connectivity comprehensively. Neuronal boundary detection from the Electron Microscopy (EM) images is often done to assist the automatic reconstruction of neuronal circuit. But the segmentation of EM images is a challenging problem, as it requires the detector to be able to detect both filament-like thin and blob-like thick membrane, while suppressing the ambiguous intracellular structure. In this paper, we propose multi-stage multi-recursive-input fully convolutional networks to address this problem. The multiple recursive inputs for one stage, i.e., the multiple side outputs with different receptive field sizes learned from the lower stage, provide multi-scale contextual boundary information for the consecutive learning. This design is biologically-plausible, as it likes a human visual system to compare different possible segmentation solutions to address the ambiguous boundary issue. Our multi-stage networks are trained end-to-end. It achieves promising results on two public available EM segmentation datasets, the mouse piriform cortex dataset and the ISBI 2012 EM dataset.",http://arxiv.org/pdf/1703.08493v2
1043,Depth and Image Restoration From Light Field in a Scattering Medium,Jiandong Tian; Zachary Murez; Tong Cui; Zhen Zhang; David Kriegman; Ravi Ramamoorthi,,,
1277,Video Reflection Removal Through Spatio-Temporal Optimization,Ajay Nandoriya; Mohamed Elgharib; Changil Kim; Mohamed Hefeeda; Wojciech Matusik,,,
938,Efficient Online Local Metric Adaptation via Negative Samples for Person Re-Identification,Jiahuan Zhou; Pei Yu; Wei Tang; Ying Wu,,,
983,Stepwise Metric Promotion for Unsupervised Video Person Re-Identification,Zimo Liu; Dong Wang; Huchuan Lu,,,
1029,Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis,Rui Huang; Shu Zhang; Tianyu Li; Ran He,Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis,"Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoder-decoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition.",http://arxiv.org/pdf/1704.04086v2
1037,Group Re-Identification via Unsupervised Transfer of Sparse Features Encoding,Giuseppe Lisanti; Niki Martinel; Alberto Del Bimbo; Gian Luca Foresti,Group Re-Identification via Unsupervised Transfer of Sparse Features Encoding,"Person re-identification is best known as the problem of associating a single person that is observed from one or more disjoint cameras. The existing literature has mainly addressed such an issue, neglecting the fact that people usually move in groups, like in crowded scenarios. We believe that the additional information carried by neighboring individuals provides a relevant visual context that can be exploited to obtain a more robust match of single persons within the group. Despite this, re-identifying groups of people compound the common single person re-identification problems by introducing changes in the relative position of persons within the group and severe self-occlusions. In this paper, we propose a solution for group re-identification that grounds on transferring knowledge from single person re-identification to group re-identification by exploiting sparse dictionary learning. First, a dictionary of sparse atoms is learned using patches extracted from single person images. Then, the learned dictionary is exploited to obtain a sparsity-driven residual group representation, which is finally matched to perform the re-identification. Extensive experiments on the i-LIDS groups and two newly collected datasets show that the proposed solution outperforms state-of-the-art approaches.",http://arxiv.org/pdf/1707.09173v1
1107,Visual Transformation Aided Contrastive Learning for Video-Based Kinship Verification,Hamdi Dibeklioƒülu,,,
945,Decoder Network Over Lightweight Reconstructed Feature for Fast Semantic Style Transfer,Ming Lu; Hao Zhao; Anbang Yao; Feng Xu; Yurong Chen; Li Zhang,,,
996,Blind Image Deblurring With Outlier Handling,Jiangxin Dong; Jinshan Pan; Zhixun Su; Ming-Hsuan Yang,,,
1075,Paying Attention to Descriptions Generated by Image Captioning Models,Hamed R. Tavakoli; Rakshith Shetty; Ali Borji; Jorma Laaksonen,Paying Attention to Descriptions Generated by Image Captioning Models,"To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliency-boosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.",http://arxiv.org/pdf/1704.07434v3
1134,Fast Image Processing With Fully-Convolutional Networks,Qifeng Chen; Jia Xu; Vladlen Koltun,Fully Convolutional Neural Networks for Crowd Segmentation,"In this paper, we propose a fast fully convolutional neural network (FCNN) for crowd segmentation. By replacing the fully connected layers in CNN with 1 by 1 convolution kernels, FCNN takes whole images as inputs and directly outputs segmentation maps by one pass of forward propagation. It has the property of translation invariance like patch-by-patch scanning but with much lower computation cost. Once FCNN is learned, it can process input images of any sizes without warping them to a standard size. These attractive properties make it extendable to other general image segmentation problems. Based on FCNN, a multi-stage deep learning is proposed to integrate appearance and motion cues for crowd segmentation. Both appearance filters and motion filers are pretrained stage-by-stage and then jointly optimized. Different combination methods are investigated. The effectiveness of our approach and component-wise analysis are evaluated on two crowd segmentation datasets created by us, which include image frames from 235 and 11 scenes, respectively. They are currently the largest crowd segmentation datasets and will be released to the public.",http://arxiv.org/pdf/1411.4464v1
1152,Robust Video Super-Resolution With Learned Temporal Dynamics,Ding Liu; Zhaowen Wang; Yuchen Fan; Xianming Liu; Zhangyang Wang; Shiyu Chang; Thomas Huang,,,
1167,Should We Encode Rain Streaks in Video as Deterministic or Stochastic?,Wei Wei; Lixuan Yi; Qi Xie; Qian Zhao; Deyu Meng; Zongben Xu,,,
1199,Joint Bi-Layer Optimization for Single-Image Rain Streak Removal,Lei Zhu; Chi-Wing Fu; Dani Lischinski; Pheng-Ann Heng,,,
923,Low-Dimensionality Calibration Through Local Anisotropic Scaling for Robust Hand Model Personalization,Edoardo Remelli; Anastasia Tkach; Andrea Tagliasacchi; Mark Pauly,,,
1098,Non-Markovian Globally Consistent Multi-Object Tracking,Andrii Maksai; Xinchao Wang; Fran√ßois Fleuret; Pascal Fua,,,
1118,CREST: Convolutional Residual Learning for Visual Tracking,Yibing Song; Chao Ma; Lijun Gong; Jiawei Zhang; Rynson W. H. Lau; Ming-Hsuan Yang,CREST: Convolutional Residual Learning for Visual Tracking,"Discriminative correlation filters (DCFs) have been shown to perform superiorly in visual tracking. They only need a small set of training samples from the initial frame to generate an appearance model. However, existing DCFs learn the filters separately from feature extraction, and update these filters using a moving average operation with an empirical weight. These DCF trackers hardly benefit from the end-to-end training. In this paper, we propose the CREST algorithm to reformulate DCFs as a one-layer convolutional neural network. Our method integrates feature extraction, response map generation as well as model update into the neural networks for an end-to-end training. To reduce model degradation during online update, we apply residual learning to take appearance changes into account. Extensive experiments on the benchmark datasets demonstrate that our CREST tracker performs favorably against state-of-the-art trackers.",http://arxiv.org/pdf/1708.00225v1
1123,Volumetric Flow Estimation for Incompressible Fluids Using the Stationary Stokes Equations,Katrin Lasinger; Christoph Vogel; Konrad Schindler,,,
1149,"Bounding Boxes, Segmentations and Object Coordinates: How Important Is Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios?",Aseem Behl; Omid Hosseini Jafari; Siva Karthik Mustikovela; Hassan Abu Alhaija; Carsten Rother; Andreas Geiger,,,
1020,Performance Guaranteed Network Acceleration via High-Order Residual Quantization,Zefan Li; Bingbing Ni; Wenjun Zhang; Xiaokang Yang; Wen Gao,Performance Guaranteed Network Acceleration via High-Order Residual Quantization,"Input binarization has shown to be an effective way for network acceleration. However, previous binarization scheme could be regarded as simple pixel-wise thresholding operations (i.e., order-one approximation) and suffers a big accuracy loss. In this paper, we propose a highorder binarization scheme, which achieves more accurate approximation while still possesses the advantage of binary operation. In particular, the proposed scheme recursively performs residual quantization and yields a series of binary input images with decreasing magnitude scales. Accordingly, we propose high-order binary filtering and gradient propagation operations for both forward and backward computations. Theoretical analysis shows approximation error guarantee property of proposed method. Extensive experimental results demonstrate that the proposed scheme yields great recognition accuracy while being accelerated.",http://arxiv.org/pdf/1708.08687v1
966,Deep Metric Learning With Angular Loss,Jian Wang; Feng Zhou; Shilei Wen; Xiao Liu; Yuanqing Lin,Deep Metric Learning with Angular Loss,"The modern image search system requires semantic understanding of image, and a key yet under-addressed problem is to learn a good metric for measuring the similarity between images. While deep metric learning has yielded impressive performance gains by extracting high level abstractions from image data, a proper objective loss function becomes the central issue to boost the performance. In this paper, we propose a novel angular loss, which takes angle relationship into account, for learning better similarity metric. Whereas previous metric learning methods focus on optimizing the similarity (contrastive loss) or relative similarity (triplet loss) of image pairs, our proposed method aims at constraining the angle at the negative point of triplet triangles. Several favorable properties are observed when compared with conventional methods. First, scale invariance is introduced, improving the robustness of objective against feature variance. Second, a third-order geometric constraint is inherently imposed, capturing additional local structure of triplet triangles than contrastive loss or triplet loss. Third, better convergence has been demonstrated by experiments on three publicly available datasets.",http://arxiv.org/pdf/1708.01682v1
971,Compositional Human Pose Regression,Xiao Sun; Jiaxiang Shang; Shuang Liang; Yichen Wei,Compositional Human Pose Regression,"Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII.",http://arxiv.org/pdf/1704.00159v3
1112,MUTAN: Multimodal Tucker Fusion for Visual Question Answering,Hedi Ben-younes; Remi Cadene; Matthieu Cord; Nicolas Thome,MUTAN: Multimodal Tucker Fusion for Visual Question Answering,"Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how our MUTAN model generalizes some of the latest VQA architectures, providing state-of-the-art results.",http://arxiv.org/pdf/1705.06676v1
1126,Revisiting IM2GPS in the Deep Learning Era,Nam Vo; Nathan Jacobs; James Hays,Revisiting IM2GPS in the Deep Learning Era,"Image geolocalization, inferring the geographic location of an image, is a challenging computer vision problem with many potential applications. The recent state-of-the-art approach to this problem is a deep image classification approach in which the world is spatially divided into cells and a deep network is trained to predict the correct cell for a given image. We propose to combine this approach with the original Im2GPS approach in which a query image is matched against a database of geotagged images and the location is inferred from the retrieved set. We estimate the geographic location of a query image by applying kernel density estimation to the locations of its nearest neighbors in the reference database. Interestingly, we find that the best features for our retrieval task are derived from networks trained with classification loss even though we do not use a classification approach at test time. Training with classification loss outperforms several deep feature learning methods (e.g. Siamese networks with contrastive of triplet loss) more typical for retrieval applications. Our simple approach achieves state-of-the-art geolocalization accuracy while also requiring significantly less training data.",http://arxiv.org/pdf/1705.04838v1
1143,Scene Parsing With Global Context Embedding,Wei-Chih Hung; Yi-Hsuan Tsai; Xiaohui Shen; Zhe Lin; Kalyan Sunkavalli; Xin Lu; Ming-Hsuan Yang,,,
1154,A Simple yet Effective Baseline for 3D Human Pose Estimation,Julieta Martinez; Rayat Hossain; Javier Romero; James J. Little,A simple yet effective baseline for 3d human pose estimation,"Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3-dimensional positions. With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, ""lifting"" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feed-forward network outperforms the best reported result by about 30\% on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (\ie, using images as input) yields state of the art results -- this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.",http://arxiv.org/pdf/1705.03098v2
1163,Dual-Glance Model for Deciphering Social Relationships,Junnan Li; Yongkang Wong; Qi Zhao; Mohan S. Kankanhalli,Dual-Glance Model for Deciphering Social Relationships,"Since the beginning of early civilizations, social relationships derived from each individual fundamentally form the basis of social structure in our daily life. In the computer vision literature, much progress has been made in scene understanding, such as object detection and scene parsing. Recent research focuses on the relationship between objects based on its functionality and geometrical relations. In this work, we aim to study the problem of social relationship recognition, in still images. We have proposed a dual-glance model for social relationship recognition, where the first glance fixates at the individual pair of interest and the second glance deploys attention mechanism to explore contextual cues. We have also collected a new large scale People in Social Context (PISC) dataset, which comprises of 22,670 images and 76,568 annotated samples from 9 types of social relationship. We provide benchmark results on the PISC dataset, and qualitatively demonstrate the efficacy of the proposed model.",http://arxiv.org/pdf/1708.00634v1
1166,Sketching With Style: Visual Search With Sketches and Aesthetic Context,John Collomosse; Tu Bui; Michael J. Wilber; Chen Fang; Hailin Jin,,,
1273,Point Set Registration With Global-Local Correspondence and Transformation Estimation,Su Zhang; Yang Yang; Kun Yang; Yi Luo; Sim-Heng Ong,,,
1068,SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-Training on Indoor Segmentation?,John McCormac; Ankur Handa; Stefan Leutenegger; Andrew J. Davison,,,
1102,A Unified Model for Near and Remote Sensing,Scott Workman; Menghua Zhai; David J. Crandall; Nathan Jacobs,A Unified Model for Near and Remote Sensing,"We propose a novel convolutional neural network architecture for estimating geospatial functions such as population density, land cover, or land use. In our approach, we combine overhead and ground-level images in an end-to-end trainable neural network, which uses kernel regression and density estimation to convert features extracted from the ground-level images into a dense feature map. The output of this network is a dense estimate of the geospatial function in the form of a pixel-level labeling of the overhead image. To evaluate our approach, we created a large dataset of overhead and ground-level images from a major urban area with three sets of labels: land use, building function, and building age. We find that our approach is more accurate for all tasks, in some cases dramatically so.",http://arxiv.org/pdf/1708.03035v1
1103,Directionally Convolutional Networks for 3D Shape Segmentation,Haotian Xu; Ming Dong; Zichun Zhong,,,
1142,AMAT: Medial Axis Transform for Natural Images,Stavros Tsogkas; Sven Dickinson,AMAT: Medial Axis Transform for Natural Images,"We introduce Appearance-MAT (AMAT), a generalization of the medial axis transform for natural images, that is framed as a weighted geometric set cover problem. We make the following contributions: i) we extend previous medial point detection methods for color images, by associating each medial point with a local scale; ii) inspired by the invertibility property of the binary MAT, we also associate each medial point with a local encoding that allows us to invert the AMAT, reconstructing the input image; iii) we describe a clustering scheme that takes advantage of the additional scale and appearance information to group individual points into medial branches, providing a shape decomposition of the underlying image regions. In our experiments, we show state-of-the-art performance in medial point detection on Berkeley Medial AXes (BMAX500), a new dataset of medial axes based on the BSDS500 database, and good generalization on the SK506 and WH-SYMMAX datasets. We also measure the quality of reconstructed images from BMAX500, obtained by inverting their computed AMAT. Our approach delivers significantly better reconstruction quality with respect to three baselines, using just 10% of the image pixels. Our code and annotations are available at https://github.com/tsogkas/amat .",http://arxiv.org/pdf/1703.08628v2
1192,Deep Dual Learning for Semantic Image Segmentation,Ping Luo; Guangrun Wang; Liang Lin; Xiaogang Wang,An Adaptive Sampling Scheme to Efficiently Train Fully Convolutional Networks for Semantic Segmentation,"Deep convolutional neural networks (CNNs) have shown excellent performance in object recognition tasks and dense classification problems such as semantic segmentation. However, training deep neural networks on large and sparse datasets is still challenging and can require large amounts of computation and memory. In this work, we address the task of performing semantic segmentation on large data sets, such as three-dimensional medical images. We propose an adaptive sampling scheme that uses a-posterior error maps, generated throughout training, to focus sampling on difficult regions, resulting in improved learning. Our contribution is threefold: 1) We give a detailed description of the proposed sampling algorithm to speed up and improve learning performance on large images. We propose a deep dual path CNN that captures information at fine and coarse scales, resulting in a network with a large field of view and high resolution outputs. We show that our method is able to attain new state-of-the-art results on the VISCERAL Anatomy benchmark.",http://arxiv.org/pdf/1709.02764v3
1251,Regional Interactive Image Segmentation Networks,Jun Hao Liew; Yunchao Wei; Wei Xiong; Sim-Heng Ong; Jiashi Feng,Icon: An Interactive Approach to Train Deep Neural Networks for Segmentation of Neuronal Structures,"We present an interactive approach to train a deep neural network pixel classifier for the segmentation of neuronal structures. An interactive training scheme reduces the extremely tedious manual annotation task that is typically required for deep networks to perform well on image segmentation problems. Our proposed method employs a feedback loop that captures sparse annotations using a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in almost real-time. Our implementation of the algorithm also allows multiple users to provide annotations in parallel and receive feedback from the same classifier. Quick feedback on classifier performance in an interactive setting enables users to identify and label examples that are more important than others for segmentation purposes. Our experiments show that an interactively-trained pixel classifier produces better region segmentation results on Electron Microscopy (EM) images than those generated by a network of the same architecture trained offline on exhaustive ground-truth labels.",http://arxiv.org/pdf/1610.09032v1
936,Learning Efficient Convolutional Networks Through Network Slimming,Zhuang Liu; Jianguo Li; Zhiqiang Shen; Gao Huang; Shoumeng Yan; Changshui Zhang,Learning Efficient Convolutional Networks through Network Slimming,"The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.",http://arxiv.org/pdf/1708.06519v1
997,CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training,Jianmin Bao; Dong Chen; Fang Wen; Houqiang Li; Gang Hua,,,
1046,Universal Adversarial Perturbations Against Semantic Image Segmentation,Jan Hendrik Metzen; Mummadi Chaithanya Kumar; Thomas Brox; Volker Fischer,Universal Adversarial Perturbations Against Semantic Image Segmentation,"While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.",http://arxiv.org/pdf/1704.05712v3
1052,Associative Domain Adaptation,Philip Haeusser; Thomas Frerix; Alexander Mordvintsev; Daniel Cremers,Associative Domain Adaptation,"We propose associative domain adaptation, a novel technique for end-to-end domain adaptation with neural networks, the task of inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain. Our training scheme follows the paradigm that in order to effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. We accomplish this by reinforcing associations between source and target data directly in embedding space. Our method can easily be added to any existing classification network with no structural and almost no computational overhead. We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a generic convolutional neural network architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space.",http://arxiv.org/pdf/1708.00938v1
1119,Introspective Neural Networks for Generative Modeling,Justin Lazarow; Long Jin; Zhuowen Tu,,,
1136,Towards a Unified Compositional Model for Visual Pattern Modeling,Wei Tang; Pei Yu; Jiahuan Zhou; Ying Wu,,,
1174,Least Squares Generative Adversarial Networks,Xudong Mao; Qing Li; Haoran Xie; Raymond Y.K. Lau; Zhen Wang; Stephen Paul Smolley,Least Squares Generative Adversarial Networks,"Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",http://arxiv.org/pdf/1611.04076v3
1176,Centered Weight Normalization in Accelerating Training of Deep Neural Networks,Lei Huang; Xianglong Liu; Yang Liu; Bo Lang; Dacheng Tao,,,
1203,Deep Growing Learning,Guangcong Wang; Xiaohua Xie; Jianhuang Lai; Jiaxuan Zhuo,Learning Discriminative Features using Encoder-Decoder type Deep Neural Nets,"As machine learning is applied to an increasing variety of complex problems, which are defined by high dimensional and complex data sets, the necessity for task oriented feature learning grows in importance. With the advancement of Deep Learning algorithms, various successful feature learning techniques have evolved. In this paper, we present a novel way of learning discriminative features by training Deep Neural Nets which have Encoder or Decoder type architecture similar to an Autoencoder. We demonstrate that our approach can learn discriminative features which can perform better at pattern classification tasks when the number of training samples is relatively small in size.",http://arxiv.org/pdf/1607.01354v1
1205,Smart Mining for Deep Metric Learning,Ben Harwood; Vijay Kumar B G; Gustavo Carneiro; Ian Reid; Tom Drummond,Smart Mining for Deep Metric Learning,"To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.",http://arxiv.org/pdf/1704.01285v3
1230,Temporal Generative Adversarial Nets With Singular Value Clipping,Masaki Saito; Eiichi Matsumoto; Shunta Saito,Temporal Generative Adversarial Nets with Singular Value Clipping,"In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.",http://arxiv.org/pdf/1611.06624v3
1246,Sampling Matters in Deep Embedding Learning,Chao-Yuan Wu; R. Manmatha; Alexander J. Smola; Philipp Kr√§henb√ºhl,Sampling Matters in Deep Embedding Learning,"Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.",http://arxiv.org/pdf/1706.07567v1
1281,DualGAN: Unsupervised Dual Learning for Image-To-Image Translation,Zili Yi; Hao Zhang; Ping Tan; Minglun Gong,DualGAN: Unsupervised Dual Learning for Image-to-Image Translation,"Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.",http://arxiv.org/pdf/1704.02510v3
921,Learning View-Invariant Features for Person Identification in Temporally Synchronized Videos Taken by Wearable Cameras,Kang Zheng; Xiaochuan Fan; Yuewei Lin; Hao Guo; Hongkai Yu; Dazhou Guo; Song Wang,,,
1022,MarioQA: Answering Questions by Watching Gameplay Videos,Jonghwan Mun; Paul Hongsuck Seo; Ilchae Jung; Bohyung Han,MarioQA: Answering Questions by Watching Gameplay Videos,"We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos. Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences. Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on ad-hoc implementations of deep neural networks without clear insight into datasets and models. We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images. To address this objective, we automatically generate a customized synthetic VideoQA dataset using {\em Super Mario Bros.} gameplay videos so that it contains events with different levels of reasoning complexity. Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.",http://arxiv.org/pdf/1612.01669v2
1071,SBGAR: Semantics Based Group Activity Recognition,Xin Li; Mooi Choo Chuah,,,
1085,Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video,Davide Moltisanti; Michael Wray; Walterio Mayol-Cuevas; Dima Damen,Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video,"Manual annotations of temporal bounds for object interactions (i.e. start and end times) are typical training input to recognition, localization and detection algorithms. For three publicly available egocentric datasets, we uncover inconsistencies in ground truth temporal bounds within and across annotators and datasets. We systematically assess the robustness of state-of-the-art approaches to changes in labeled temporal bounds, for object interaction recognition. As boundaries are trespassed, a drop of up to 10% is observed for both Improved Dense Trajectories and Two-Stream Convolutional Neural Network.   We demonstrate that such disagreement stems from a limited understanding of the distinct phases of an action, and propose annotating based on the Rubicon Boundaries, inspired by a similarly named cognitive model, for consistent temporal bounds of object interactions. Evaluated on a public dataset, we report a 4% increase in overall accuracy, and an increase in accuracy for 55% of classes when Rubicon Boundaries are used for temporal annotations.",http://arxiv.org/pdf/1703.09026v2
1101,Unmasking the Abnormal Events in Video,Radu Tudor Ionescu; Sorina Smeureanu; Bogdan Alexe; Marius Popescu,Unmasking the abnormal events in video,"We propose a novel framework for abnormal event detection in video that requires no training sequences. Our framework is based on unmasking, a technique previously used for authorship verification in text documents, which we adapt to our task. We iteratively train a binary classifier to distinguish between two consecutive video sequences while removing at each step the most discriminant features. Higher training accuracy rates of the intermediately obtained classifiers represent abnormal events. To the best of our knowledge, this is the first work to apply unmasking for a computer vision task. We compare our method with several state-of-the-art supervised and unsupervised methods on four benchmark data sets. The empirical results indicate that our abnormal event detection framework can achieve state-of-the-art results, while running in real-time at 20 frames per second.",http://arxiv.org/pdf/1705.08182v3
1104,"Chained Multi-Stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection",Mohammadreza Zolfaghari; Gabriel L. Oliveira; Nima Sedaghat; Thomas Brox,"Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection","General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.",http://arxiv.org/pdf/1704.00616v2
1218,Temporal Action Detection With Structured Segment Networks,Yue Zhao; Yuanjun Xiong; Limin Wang; Zhirong Wu; Xiaoou Tang; Dahua Lin,Temporal Action Detection with Structured Segment Networks,"Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.",http://arxiv.org/pdf/1704.06228v2
1223,Jointly Recognizing Object Fluents and Tasks in Egocentric Videos,Yang Liu; Ping Wei; Song-Chun Zhu,,,
1262,Transferring Objects: Joint Inference of Container and Human Pose,Hanqing Wang; Wei Liang; Lap-Fai Yu,,,
1161,Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention,Jinkyu Kim; John Canny,Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention,"Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-to-interpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.",http://arxiv.org/pdf/1703.10631v1
175,Learning Cooperative Visual Dialog Agents With Deep Reinforcement Learning,Abhishek Das; Satwik Kottur; Jos√© M. F. Moura; Stefan Lee; Dhruv Batra,Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning,"We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.",http://arxiv.org/pdf/1703.06585v2
16,Mask R-CNN,Kaiming He; Georgia Gkioxari; Piotr Doll√°r; Ross Girshick,,,
1076,Towards Diverse and Natural Image Descriptions via a Conditional GAN,Bo Dai; Sanja Fidler; Raquel Urtasun; Dahua Lin,Towards Diverse and Natural Image Descriptions via a Conditional GAN,"Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the ""ground-truth"" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",http://arxiv.org/pdf/1703.06029v3
1902,Focal Loss for Dense Object Detection,Tsung-Yi Lin; Priya Goyal; Ross Girshick; Kaiming He; Piotr Doll√°r,Focal Loss for Dense Object Detection,"The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.",http://arxiv.org/pdf/1708.02002v1
1138,Inferring and Executing Programs for Visual Reasoning,Justin Johnson; Bharath Hariharan; Laurens van der Maaten; Judy Hoffman; Li Fei-Fei; C. Lawrence Zitnick; Ross Girshick,Inferring and Executing Programs for Visual Reasoning,"Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.",http://arxiv.org/pdf/1705.03633v1
177,Visual Forecasting by Imitating Dynamics in Natural Sequences,Kuo-Hao Zeng; William B. Shen; De-An Huang; Min Sun; Juan Carlos Niebles,Visual Forecasting by Imitating Dynamics in Natural Sequences,"We introduce a general framework for visual forecasting, which directly imitates visual sequences without additional supervision. As a result, our model can be applied at several semantic levels and does not require any domain knowledge or handcrafted features. We achieve this by formulating visual forecasting as an inverse reinforcement learning (IRL) problem, and directly imitate the dynamics in natural sequences from their raw pixel values. The key challenge is the high-dimensional and continuous state-action space that prohibits the application of previous IRL algorithms. We address this computational bottleneck by extending recent progress in model-free imitation with trainable deep feature representations, which (1) bypasses the exhaustive state-action pair visits in dynamic programming by using a dual formulation and (2) avoids explicit state sampling at gradient computation using a deep feature reparametrization. This allows us to apply IRL at scale and directly imitate the dynamics in high-dimensional continuous visual sequences from the raw pixel values. We evaluate our approach at three different level-of-abstraction, from low level pixels to higher level semantics: future frame generation, action anticipation, visual story forecasting. At all levels, our approach outperforms existing methods.",http://arxiv.org/pdf/1708.05827v1
1469,TorontoCity: Seeing the World With a Million Eyes,Shenlong Wang; Min Bai; Gell√©rt M√°ttyus; Hang Chu; Wenjie Luo; Bin Yang; Justin Liang; Joel Cheverie; Sanja Fidler; Raquel Urtasun,TorontoCity: Seeing the World with a Million Eyes,"In this paper we introduce the TorontoCity benchmark, which covers the full greater Toronto area (GTA) with 712.5 $km^2$ of land, 8439 $km$ of road and around 400,000 buildings. Our benchmark provides different perspectives of the world captured from airplanes, drones and cars driving around the city. Manually labeling such a large scale dataset is infeasible. Instead, we propose to utilize different sources of high-precision maps to create our ground truth. Towards this goal, we develop algorithms that allow us to align all data sources with the maps while requiring minimal human supervision. We have designed a wide variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction (reorganization), semantic labeling and scene type classification (recognition). Our pilot study shows that most of these tasks are still difficult for modern convolutional neural networks.",http://arxiv.org/pdf/1612.00423v1
1523,Low-Shot Visual Recognition by Shrinking and Hallucinating Features,Bharath Hariharan; Ross Girshick,Low-shot Visual Recognition by Shrinking and Hallucinating Features,"Low-shot visual learning---the ability to recognize novel object categories from very few examples---is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose a) representation regularization techniques, and b) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3x on the challenging ImageNet dataset.",http://arxiv.org/pdf/1606.02819v3
1609,A Coarse-Fine Network for Keypoint Localization,Shaoli Huang; Mingming Gong; Dacheng Tao,"Fine-grained pose prediction, normalization, and recognition","Pose variation and subtle differences in appearance are key challenges to fine-grained classification. While deep networks have markedly improved general recognition, many approaches to fine-grained recognition rely on anchoring networks to parts for better accuracy. Identifying parts to find correspondence discounts pose variation so that features can be tuned to appearance. To this end previous methods have examined how to find parts and extract pose-normalized features. These methods have generally separated fine-grained recognition into stages which first localize parts using hand-engineered and coarsely-localized proposal features, and then separately learn deep descriptors centered on inferred part positions. We unify these steps in an end-to-end trainable network supervised by keypoint locations and class labels that localizes parts by a fully convolutional network to focus the learning of feature representations for the fine-grained classification task. Experiments on the popular CUB200 dataset show that our method is state-of-the-art and suggest a continuing role for strong supervision.",http://arxiv.org/pdf/1511.07063v1
1806,Detect to Track and Track to Detect,Christoph Feichtenhofer; Axel Pinz; Andrew Zisserman,Implementation of an Onboard Visual Tracking System with Small Unmanned Aerial Vehicle (UAV),This paper presents a visual tracking system that is capable or running real time on-board a small UAV (Unmanned Aerial Vehicle). The tracking system is computationally efficient and invariant to lighting changes and rotation of the object or the camera. Detection and tracking is autonomously carried out on the payload computer and there are two different methods for creation of the image patches. The first method starts detecting and tracking using a stored image patch created prior to flight with previous flight data. The second method allows the operator on the ground to select the interest object for the UAV to track. The tracking system is capable of re-detecting the object of interest in the events of tracking failure. Performance of the tracking system was verified both in the lab and during actual flights of the UAV. Results show that the system can run on-board and track a diverse set of objects in real time.,http://arxiv.org/pdf/1205.5742v1
1946,Single Shot Text Detector With Regional Attention,Pan He; Weilin Huang; Tong He; Qile Zhu; Yu Qiao; Xiaolin Li,Single Shot Text Detector with Regional Attention,"We present a novel single-shot text detector that directly outputs word-level bounding boxes in a natural image. We propose an attention mechanism which roughly identifies text regions via an automatically learned attentional map. This substantially suppresses background interference in the convolutional features, which is the key to producing accurate inference of words, particularly at extremely small sizes. This results in a single model that essentially works in a coarse-to-fine manner. It departs from recent FCN- based text detectors which cascade multiple FCN models to achieve an accurate prediction. Furthermore, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. This enhances local details, and also encodes strong context information, allow- ing the detector to work reliably on multi-scale and multi- orientation text with single-scale images. Our text detector achieves an F-measure of 77% on the ICDAR 2015 bench- mark, advancing the state-of-the-art results in [18, 28]. Demo is available at: http://sstd.whuang.org/.",http://arxiv.org/pdf/1709.00138v1
2402,SubUNets: End-To-End Hand Shape and Continuous Sign Language Recognition,Necati Cihan Camgoz; Simon Hadfield; Oscar Koller; Richard Bowden,,,
2680,A Spatiotemporal Oriented Energy Network for Dynamic Texture Recognition,Isma Hadji; Richard P. Wildes,A Spatiotemporal Oriented Energy Network for Dynamic Texture Recognition,"This paper presents a novel hierarchical spatiotemporal orientation representation for spacetime image analysis. It is designed to combine the benefits of the multilayer architecture of ConvNets and a more controlled approach to spacetime analysis. A distinguishing aspect of the approach is that unlike most contemporary convolutional networks no learning is involved; rather, all design decisions are specified analytically with theoretical motivations. This approach makes it possible to understand what information is being extracted at each stage and layer of processing as well as to minimize heuristic choices in design. Another key aspect of the network is its recurrent nature, whereby the output of each layer of processing feeds back to the input. To keep the network size manageable across layers, a novel cross-channel feature pooling is proposed. The multilayer architecture that results systematically reveals hierarchical image structure in terms of multiscale, multiorientation properties of visual spacetime. To illustrate its utility, the network has been applied to the task of dynamic texture recognition. Empirical evaluation on multiple standard datasets shows that it sets a new state-of-the-art.",http://arxiv.org/pdf/1708.06690v1
1287,Probabilistic Structure From Motion With Objects (PSfMO),Paul Gay; Cosimo Rubino; Vaibhav Bansal; Alessio Del Bue,,,
1290,A 3D Morphable Model of Craniofacial Shape and Texture Variation,Hang Dai; Nick Pears; William A. P. Smith; Christian Duncan,,,
1361,Multi-View Dynamic Shape Refinement Using Local Temporal Integration,Vincent Leroy; Jean-Sebastien Franco; Edmond Boyer,,,
1394,Learning Hand Articulations by Hallucinating Heat Distribution,Chiho Choi; Sangpil Kim; Karthik Ramani,,,
1395,Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization With Spatially-Varying Lighting,Robert Maier; Kihwan Kim; Daniel Cremers; Jan Kautz; Matthias Nie√üner,Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting,"We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.",http://arxiv.org/pdf/1708.01670v1
1396,Robust Hand Pose Estimation During the Interaction With an Unknown Object,Chiho Choi; Sang Ho Yoon; Chin-Ning Chen; Karthik Ramani,,,
1473,Detailed Surface Geometry and Albedo Recovery From RGB-D Video Under Natural Illumination,Xinxin Zuo; Sen Wang; Jiangbin Zheng; Ruigang Yang,,,
1487,Monocular Free-Head 3D Gaze Tracking With Deep Learning and Geometry Constraints,Wangjiang Zhu; Haoping Deng,,,
1427,Filter Selection for Hyperspectral Estimation,Boaz Arad; Ohad Ben-Shahar,A Critical Survey of Deconvolution Methods for Separating cell-types in Complex Tissues,"Identifying concentrations of components from an observed mixture is a fundamental problem in signal processing. It has diverse applications in fields ranging from hyperspectral imaging to denoising biomedical sensors. This paper focuses on in-silico deconvolution of signals associated with complex tissues into their constitutive cell-type specific components, along with a quantitative characterization of the cell-types. Deconvolving mixed tissues/cell-types is useful in the removal of contaminants (e.g., surrounding cells) from tumor biopsies, as well as in monitoring changes in the cell population in response to treatment or infection. In these contexts, the observed signal from the mixture of cell-types is assumed to be a linear combination of the expression levels of genes in constitutive cell-types. The goal is to use known signals corresponding to individual cell-types along with a model of the mixing process to cast the deconvolution problem as a suitable optimization problem.   In this paper, we present a survey of models, methods, and assumptions underlying deconvolution techniques. We investigate the choice of the different loss functions for evaluating estimation error, constraints on solutions, preprocessing and data filtering, feature selection, and regularization to enhance the quality of solutions, along with the impact of these choices on the performance of regression-based methods for deconvolution. We assess different combinations of these factors and use detailed statistical measures to evaluate their effectiveness. We identify shortcomings of current methods and avenues for further investigation. For many of the identified shortcomings, such as normalization issues and data filtering, we provide new solutions. We summarize our findings in a prescriptive step-by-step process, which can be applied to a wide range of deconvolution problems.",http://arxiv.org/pdf/1510.04583v1
1525,A Microfacet-Based Reflectance Model for Photometric Stereo With Highly Specular Surfaces,Lixiong Chen; Yinqiang Zheng; Boxin Shi; Art Subpa-Asa; Imari Sato,,,
1331,Detecting Faces Using Inside Cascaded Contextual CNN,Kaipeng Zhang; Zhanpeng Zhang; Hao Wang; Zhifeng Li; Yu Qiao; Wei Liu,,,
1339,A Novel Space-Time Representation on the Positive Semidefinite Cone for Facial Expression Recognition,Anis Kacem; Mohamed Daoudi; Boulbaba Ben Amor; Juan Carlos Alvarez-Paiva,,,
1409,DeepCoder: Semi-Parametric Variational Autoencoders for Automatic Facial Action Coding,Dieu Linh Tran; Robert Walecki; Ognjen (Oggi) Rudovic; Stefanos Eleftheriadis; Bj√∂rn Schuller; Maja Pantic,DeepCoder: Semi-parametric Variational Autoencoders for Automatic Facial Action Coding,"Human face exhibits an inherent hierarchy in its representations (i.e., holistic facial expressions can be encoded via a set of facial action units (AUs) and their intensity). Variational (deep) auto-encoders (VAE) have shown great results in unsupervised extraction of hierarchical latent representations from large amounts of image data, while being robust to noise and other undesired artifacts. Potentially, this makes VAEs a suitable approach for learning facial features for AU intensity estimation. Yet, most existing VAE-based methods apply classifiers learned separately from the encoded features. By contrast, the non-parametric (probabilistic) approaches, such as Gaussian Processes (GPs), typically outperform their parametric counterparts, but cannot deal easily with large amounts of data. To this end, we propose a novel VAE semi-parametric modeling framework, named DeepCoder, which combines the modeling power of parametric (convolutional) and nonparametric (ordinal GPs) VAEs, for joint learning of (1) latent representations at multiple levels in a task hierarchy1, and (2) classification of multiple ordinal outputs. We show on benchmark datasets for AU intensity estimation that the proposed DeepCoder outperforms the state-of-the-art approaches, and related VAEs and deep learning models.",http://arxiv.org/pdf/1704.02206v2
1488,Pose-Invariant Face Alignment With a Single CNN,Amin Jourabloo; Mao Ye; Xiaoming Liu; Liu Ren,Pose-Invariant Face Alignment with a Single CNN,"Face alignment has witnessed substantial progress in the last decade. One of the recent focuses has been aligning a dense 3D face shape to face images with large head poses. The dominant technology used is based on the cascade of regressors, e.g., CNN, which has shown promising results. Nonetheless, the cascade of CNNs suffers from several drawbacks, e.g., lack of end-to-end training, hand-crafted features and slow training speed. To address these issues, we propose a new layer, named visualization layer, that can be integrated into the CNN architecture and enables joint optimization with different loss functions. Extensive evaluation of the proposed method on multiple datasets demonstrates state-of-the-art accuracy, while reducing the training time by more than half compared to the typical cascade of CNNs. In addition, we compare multiple CNN architectures with the visualization layer to further demonstrate the advantage of its utilization.",http://arxiv.org/pdf/1707.06286v1
1499,Unsupervised Domain Adaptation for Face Recognition in Unlabeled Videos,Kihyuk Sohn; Sifei Liu; Guangyu Zhong; Xiang Yu; Ming-Hsuan Yang; Manmohan Chandraker,Unsupervised Domain Adaptation for Face Recognition in Unlabeled Videos,"Despite rapid advances in face recognition, there remains a clear gap between the performance of still image-based face recognition and video-based face recognition, due to the vast difference in visual quality between the domains and the difficulty of curating diverse large-scale video datasets. This paper addresses both of those challenges, through an image to video feature-level domain adaptation approach, to learn discriminative video frame representations. The framework utilizes large-scale unlabeled video data to reduce the gap between different domains while transferring discriminative knowledge from large-scale labeled still images. Given a face recognition network that is pretrained in the image domain, the adaptation is achieved by (i) distilling knowledge from the network to a video adaptation network through feature matching, (ii) performing feature restoration through synthetic data augmentation and (iii) learning a domain-invariant feature through a domain adversarial discriminator. We further improve performance through a discriminator-guided feature fusion that boosts high-quality frames while eliminating those degraded by video domain-specific factors. Experiments on the YouTube Faces and IJB-A datasets demonstrate that each module contributes to our feature-level domain adaptation framework and substantially improves video face recognition performance to achieve state-of-the-art accuracy. We demonstrate qualitatively that the network learns to suppress diverse artifacts in videos such as pose, illumination or occlusion without being explicitly trained for them.",http://arxiv.org/pdf/1708.02191v1
1556,Deeply-Learned Part-Aligned Representations for Person Re-Identification,Liming Zhao; Xi Li; Yueting Zhuang; Jingdong Wang,Deeply-Learned Part-Aligned Representations for Person Re-Identification,"In this paper, we address the problem of person re-identification, which refers to associating the persons captured from different cameras. We propose a simple yet effective human part-aligned representation for handling the body part misalignment problem. Our approach decomposes the human body into regions (parts) which are discriminative for person matching, accordingly computes the representations over the regions, and aggregates the similarities computed between the corresponding regions of a pair of probe and gallery images as the overall matching score. Our formulation, inspired by attention models, is a deep neural network modeling the three steps together, which is learnt through minimizing the triplet loss function without requiring body part labeling information. Unlike most existing deep learning algorithms that learn a global or spatial partition-based local representation, our approach performs human body partition, and thus is more robust to pose changes and various human spatial distributions in the person bounding box. Our approach shows state-of-the-art results over standard datasets, Market-$1501$, CUHK$03$, CUHK$01$ and VIPeR.",http://arxiv.org/pdf/1707.07256v1
1298,Semantic Line Detection and Its Applications,Jun-Tae Lee; Han-Ul Kim; Chul Lee; Chang-Su Kim,,,
1309,A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing,Qingnan Fan; Jiaolong Yang; Gang Hua; Baoquan Chen; David Wipf,A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing,"This paper proposes a deep neural network structure that exploits edge information in addressing representative low-level vision tasks such as layer separation and image filtering. Unlike most other deep learning strategies applied in this context, our approach tackles these challenging problems by estimating edges and reconstructing images using only cascaded convolutional layers arranged such that no handcrafted or application-specific image-processing components are required. We apply the resulting transferrable pipeline to two different problem domains that are both sensitive to edges, namely, single image reflection removal and image smoothing. For the former, using a mild reflection smoothness assumption and a novel synthetic data generation method that acts as a type of weak supervision, our network is able to solve much more difficult reflection cases that cannot be handled by previous methods. For the latter, we also exceed the state-of-the-art quantitative and qualitative results by wide margins. In all cases, the proposed framework is simple, fast, and easy to transfer across disparate domains.",http://arxiv.org/pdf/1708.03474v1
1326,Revisiting Cross-Channel Information Transfer for Chromatic Aberration Correction,Tiancheng Sun; Yifan Peng; Wolfgang Heidrich,,,
1350,High-Quality Correspondence and Segmentation Estimation for Dual-Lens Smart-Phone Portraits,Xiaoyong Shen; Hongyun Gao; Xin Tao; Chao Zhou; Jiaya Jia,High-Quality Correspondence and Segmentation Estimation for Dual-Lens Smart-Phone Portraits,"Estimating correspondence between two images and extracting the foreground object are two challenges in computer vision. With dual-lens smart phones, such as iPhone 7Plus and Huawei P9, coming into the market, two images of slightly different views provide us new information to unify the two topics. We propose a joint method to tackle them simultaneously via a joint fully connected conditional random field (CRF) framework. The regional correspondence is used to handle textureless regions in matching and make our CRF system computationally efficient. Our method is evaluated over 2,000 new image pairs, and produces promising results on challenging portrait images.",http://arxiv.org/pdf/1704.02205v1
1356,Learning Visual Attention to Identify People With Autism Spectrum Disorder,Ming Jiang; Qi Zhao,,,
1365,DSLR-Quality Photos on Mobile Devices With Deep Convolutional Networks,Andrey Ignatov; Nikolay Kobyshev; Radu Timofte; Kenneth Vanhoey; Luc Van Gool,DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks,"Despite a rapid rise in the quality of built-in smartphone cameras, their physical limitations - small sensor size, compact lenses and the lack of specific hardware, - impede them to achieve the quality results of DSLR cameras. In this work we present an end-to-end deep learning approach that bridges this gap by translating ordinary photos into DSLR-quality images. We propose learning the translation function using a residual convolutional neural network that improves both color rendition and image sharpness. Since the standard mean squared loss is not well suited for measuring perceptual image quality, we introduce a composite perceptual error function that combines content, color and texture losses. The first two losses are defined analytically, while the texture loss is learned in an adversarial fashion. We also present DPED, a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera. Our quantitative and qualitative assessments reveal that the enhanced image quality is comparable to that of DSLR-taken photos, while the methodology is generalized to any type of digital camera.",http://arxiv.org/pdf/1704.02470v2
1419,Non-Uniform Blind Deblurring by Reblurring,Yuval Bahat; Netalee Efrat; Michal Irani,,,
1501,Misalignment-Robust Joint Filter for Cross-Modal Image Pairs,Takashi Shibata; Masayuki Tanaka; Masatoshi Okutomi,,,
1518,Low-Rank Tensor Completion: A Pseudo-Bayesian Learning Approach,Wei Chen; Nan Song,,,
1557,DeepCD: Learning Deep Complementary Descriptors for Patch Representations,Tsun-Yi Yang; Jo-Han Hsu; Yen-Yu Lin; Yung-Yu Chuang,,,
1352,Beyond Standard Benchmarks: Parameterizing Performance Evaluation in Visual Object Tracking,Luka ƒåehovin Zajc; Alan Luke≈æiƒ; Ale≈° Leonardis; Matej Kristan,Beyond standard benchmarks: Parameterizing performance evaluation in visual object tracking,"Object-to-camera motion produces a variety of apparent motion patterns that significantly affect performance of short-term visual trackers. Despite being crucial for designing robust trackers, their influence is poorly explored in standard benchmarks due to weakly defined, biased and overlapping attribute annotations. In this paper we propose to go beyond pre-recorded benchmarks with post-hoc annotations by presenting an approach that utilizes omnidirectional videos to generate realistic, consistently annotated, short-term tracking scenarios with exactly parameterized motion patterns. We have created an evaluation system, constructed a fully annotated dataset of omnidirectional videos and the generators for typical motion patterns. We provide an in-depth analysis of major tracking paradigms which is complementary to the standard benchmarks and confirms the expressiveness of our evaluation approach.",http://arxiv.org/pdf/1612.00089v2
1530,The Pose Knows: Video Forecasting by Generating Pose Futures,Jacob Walker; Kenneth Marino; Abhinav Gupta; Martial Hebert,The Pose Knows: Video Forecasting by Generating Pose Futures,"Current approaches in video forecasting attempt to generate videos directly in pixel space using Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our insight is to model the forecasting problem at a higher level of abstraction. Specifically, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene---humans---and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantitative and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction.",http://arxiv.org/pdf/1705.00053v1
1534,What Will Happen Next? Forecasting Player Moves in Sports Videos,Panna Felsen; Pulkit Agrawal; Jitendra Malik,,,
1329,Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling,Mehdi Bahri; Yannis Panagakis; Stefanos Zafeiriou,Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling,"Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. However, K-SVD is sensitive to the presence of noise and outliers in the training set. Additionally, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.",http://arxiv.org/pdf/1703.07886v2
198,Recurrent Topic-Transition GAN for Visual Paragraph Generation,Xiaodan Liang; Zhiting Hu; Hao Zhang; Chuang Gan; Eric P. Xing,Recurrent Topic-Transition GAN for Visual Paragraph Generation,"A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.",http://arxiv.org/pdf/1703.07022v2
1286,A Two-Streamed Network for Estimating Fine-Scaled Depth Maps From Single RGB Images,Jun Li; Reinhard Klein; Angela Yao,A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images,"Estimating depth from a single RGB image is an ill-posed and inherently ambiguous problem. State-of-the-art deep learning methods can now estimate accurate 2D depth maps, but when the maps are projected into 3D, they lack local detail and are often highly distorted. We propose a fast-to-train two-streamed CNN that predicts depth and depth gradients, which are then fused together into an accurate and detailed depth map. We also define a novel set loss over multiple images; by regularizing the estimation between a common set of images, the network is less prone to over-fitting and achieves better accuracy than competing methods. Experiments on the NYU Depth v2 dataset shows that our depth predictions are competitive with state-of-the-art and lead to faithful 3D projections.",http://arxiv.org/pdf/1607.00730v3
1318,Weakly Supervised Object Localization Using Things and Stuff Transfer,Miaojing Shi; Holger Caesar; Vittorio Ferrari,Weakly Supervised Object Localization Using Things and Stuff Transfer,"We propose to help weakly supervised object localization for classes where location annotations are not available, by transferring things and stuff knowledge from a source set with available annotations. The source and target classes might share similar appearance (e.g. bear fur is similar to cat fur) or appear against similar background (e.g. horse and sheep appear against grass). To exploit this, we acquire three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and co-occurrence relations between thing and stuff classes in the source. The segmentation model is used to generate thing and stuff segmentation maps on a target image, while the class similarity and co-occurrence knowledge help refining them. We then incorporate these maps as new cues into a multiple instance learning framework (MIL), propagating the transferred knowledge from the pixel level to the object proposal level. In extensive experiments, we conduct our transfer from the PASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007 datasets (targets). We evaluate our transfer across widely different thing classes, including some that are not similar in appearance, but appear against similar background. The results demonstrate significant improvement over standard MIL, and we outperform the state-of-the-art in the transfer setting.",http://arxiv.org/pdf/1703.08000v2
1367,Single Image Action Recognition Using Semantic Body Part Actions,Zhichen Zhao; Huimin Ma; Shaodi You,Single Image Action Recognition using Semantic Body Part Actions,"In this paper, we propose a novel single image action recognition algorithm which is based on the idea of semantic body part actions. Unlike existing bottom up methods, we argue that the human action is a combination of meaningful body part actions. In detail, we divide human body into five parts: head, torso, arms, hands and legs. And for each of the body parts, we define several semantic body part actions, e.g., hand holding, hand waving. These semantic body part actions are strongly related to the body actions, e.g., writing, and jogging. Based on the idea, we propose a deep neural network based system: first, body parts are localized by a Semi-FCN network. Second, for each body parts, a Part Action Res-Net is used to predict semantic body part actions. And finally, we use SVM to fuse the body part actions and predict the entire body action. Experiments on two dataset: PASCAL VOC 2012 and Stanford-40 report mAP improvement from the state-of-the-art by 3.8% and 2.6% respectively.",http://arxiv.org/pdf/1612.04520v1
1369,Incremental Learning of Object Detectors Without Catastrophic Forgetting,Konstantin Shmelkov; Cordelia Schmid; Karteek Alahari,Incremental Learning of Object Detectors without Catastrophic Forgetting,"Despite their success for object detection, convolutional neural networks are ill-equipped for incremental learning, i.e., adapting the original model trained on a set of classes to additionally detect objects of new classes, in the absence of the initial training data. They suffer from ""catastrophic forgetting"" - an abrupt degradation of performance on the original set of classes, when the training objective is adapted to the new classes. We present a method to address this issue, and learn object detectors incrementally, when neither the original training data nor annotations for the original classes in the new training set are available. The core of our proposed solution is a loss function to balance the interplay between predictions on the new classes and a new distillation loss which minimizes the discrepancy between responses for old classes from the original and the updated networks. This incremental learning can be performed multiple times, for a new set of classes in each step, with a moderate drop in performance compared to the baseline network trained on the ensemble of data. We present object detection results on the PASCAL VOC 2007 and COCO datasets, along with a detailed empirical analysis of the approach.",http://arxiv.org/pdf/1708.06977v1
1371,Generative Adversarial Networks Conditioned by Brain Signals,Simone Palazzo; Concetto Spampinato; Isaak Kavasidis; Daniela Giordano; Mubarak Shah,,,
1375,Learning to Disambiguate by Asking Discriminative Questions,Yining Li; Chen Huang; Xiaoou Tang; Chen Change Loy,Learning to Disambiguate by Asking Discriminative Questions,"The ability to ask questions is a powerful tool to gather information in order to learn about the world and resolve ambiguities. In this paper, we explore a novel problem of generating discriminative questions to help disambiguate visual instances. Our work can be seen as a complement and new extension to the rich research studies on image captioning and question answering. We introduce the first large-scale dataset with over 10,000 carefully annotated images-question tuples to facilitate benchmarking. In particular, each tuple consists of a pair of images and 4.6 discriminative questions (as positive samples) and 5.9 non-discriminative questions (as negative samples) on average. In addition, we present an effective method for visual discriminative question generation. The method can be trained in a weakly supervised manner without discriminative images-question tuples but just existing visual question answering datasets. Promising results are shown against representative baselines through quantitative evaluations and user studies.",http://arxiv.org/pdf/1708.02760v1
1393,Interpretable Explanations of Black Boxes by Meaningful Perturbation,Ruth C. Fong; Andrea Vedaldi,A causal framework for explaining the predictions of black-box sequence-to-sequence models,"We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an ""explanation"" consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.",http://arxiv.org/pdf/1707.01943v2
1418,DeepRoadMapper: Extracting Road Topology From Aerial Images,Gell√©rt M√°ttyus; Wenjie Luo; Raquel Urtasun,,,
1425,Monocular 3D Human Pose Estimation by Predicting Depth on Joints,Bruce Xiaohan Nie; Ping Wei; Song-Chun Zhu,,,
1433,Large-Scale Image Retrieval With Attentive Deep Local Features,Hyeonwoo Noh; Andre Araujo; Jack Sim; Tobias Weyand; Bohyung Han,Large-Scale Image Retrieval with Attentive Deep Local Features,"We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives---in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins.",http://arxiv.org/pdf/1612.06321v3
1466,Deep Globally Constrained MRFs for Human Pose Estimation,Ioannis Marras; Petar Palasek; Ioannis Patras,,,
1472,Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning,Soravit Changpinyo; Wei-Lun Chao; Fei Sha,Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning,"Leveraging class semantic descriptions and examples of known objects, zero-shot learning makes it possible to train a recognition model for an object class whose examples are not available. In this paper, we propose a novel zero-shot learning model that takes advantage of clustering structures in the semantic embedding space. The key idea is to impose the structural constraint that semantic representations must be predictive of the locations of their corresponding visual exemplars. To this end, this reduces to training multiple kernel-based regressors from semantic representation-exemplar pairs from labeled data of the seen object categories. Despite its simplicity, our approach significantly outperforms existing zero-shot learning methods on standard benchmark datasets, including the ImageNet dataset with more than 20,000 unseen categories.",http://arxiv.org/pdf/1605.08151v2
1515,Multi-Label Learning of Part Detectors for Heavily Occluded Pedestrian Detection,Chunluan Zhou; Junsong Yuan,,,
1542,SGN: Sequential Grouping Networks for Instance Segmentation,Shu Liu; Jiaya Jia; Sanja Fidler; Raquel Urtasun,,,
1544,Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors,Hong-Yu Zhou; Bin-Bin Gao; Jianxin Wu,Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors,"Object detection aims at high speed and accuracy simultaneously. However, fast models are usually less accurate, while accurate models cannot satisfy our need for speed. A fast model can be 10 times faster but 50\% less accurate than an accurate model. In this paper, we propose Adaptive Feeding (AF) to combine a fast (but less accurate) detector and an accurate (but slow) detector, by adaptively determining whether an image is easy or hard and choosing an appropriate detector for it. In practice, we build a cascade of detectors, including the AF classifier which make the easy vs. hard decision and the two detectors. The AF classifier can be tuned to obtain different tradeoff between speed and accuracy, which has negligible training time and requires no additional training data. Experimental results on the PASCAL VOC, MS COCO and Caltech Pedestrian datasets confirm that AF has the ability to achieve comparable speed as the fast detector and comparable accuracy as the accurate one at the same time. As an example, by combining the fast SSD300 with the accurate SSD500 detector, AF leads to 50\% speedup over SSD500 with the same precision on the VOC2007 test set.",http://arxiv.org/pdf/1707.06399v1
1559,Aesthetic Critiques Generation for Photos,Kuang-Yu Chang; Kung-Hung Lu; Chu-Song Chen,,,
1583,Hide-And-Seek: Forcing a Network to Be Meticulous for Weakly-Supervised Object and Action Localization,Krishna Kumar Singh; Yong Jae Lee,Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization,"We propose `Hide-and-Seek', a weakly-supervised framework that aims to improve object localization in images and action localization in videos. Most existing weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts, which leads to suboptimal performance. Our key idea is to hide patches in a training image randomly, forcing the network to seek other relevant parts when the most discriminative part is hidden. Our approach only needs to modify the input image and can work with any network designed for object localization. During testing, we do not need to hide any patches. Our Hide-and-Seek approach obtains superior performance compared to previous methods for weakly-supervised object localization on the ILSVRC dataset. We also demonstrate that our framework can be easily extended to weakly-supervised action localization.",http://arxiv.org/pdf/1704.04232v1
1306,Two-Phase Learning for Weakly Supervised Object Localization,Dahun Kim; Donghyeon Cho; Donggeun Yoo; In So Kweon,Two-Phase Learning for Weakly Supervised Object Localization,"Weakly supervised semantic segmentation and localiza- tion have a problem of focusing only on the most important parts of an image since they use only image-level annota- tions. In this paper, we solve this problem fundamentally via two-phase learning. Our networks are trained in two steps. In the first step, a conventional fully convolutional network (FCN) is trained to find the most discriminative parts of an image. In the second step, the activations on the most salient parts are suppressed by inference conditional feedback, and then the second learning is performed to find the area of the next most important parts. By combining the activations of both phases, the entire portion of the tar- get object can be captured. Our proposed training scheme is novel and can be utilized in well-designed techniques for weakly supervised semantic segmentation, salient region detection, and object location prediction. Detailed experi- ments demonstrate the effectiveness of our two-phase learn- ing in each task.",http://arxiv.org/pdf/1708.02108v3
1346,Curriculum Dropout,Pietro Morerio; Jacopo Cavazza; Riccardo Volpi; Ren√© Vidal; Vittorio Murino,Curriculum Dropout,"Dropout is a very effective way of regularizing neural networks. Stochastically ""dropping out"" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of ""starting easy"" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.",http://arxiv.org/pdf/1703.06229v2
1370,Predictor Combination at Test Time,Kwang In Kim; James Tompkin; Christian Richardt,Magnetic Nonpotentiality in Photospheric Active Regions as a Predictor of Solar Flares,"Based on several magnetic nonpotentiality parameters obtained from the vector photospheric active region magnetograms obtained with the Solar Magnetic Field Telescope at the Huairou Solar Observing Station over two solar cycles, a machine learning model has been constructed to predict the occurrence of flares in the corresponding active region within a certain time window. The Support Vector Classifier, a widely used general classifier, is applied to build and test the prediction models. Several classical verification measures are adopted to assess the quality of the predictions. We investigate different flare levels within various time windows, and thus it is possible to estimate the rough classes and erupting times of flares for particular active regions. Several combinations of predictors have been tested in the experiments. The True Skill Statistics are higher than 0.36 in 97% of cases and the Heidke Skill Scores range from 0.23 to 0.48. The predictors derived from longitudinal magnetic fields do perform well, however they are less sensitive in predicting large flares. Employing the nonpotentiality predictors from vector fields improves the performance of predicting large flares of magnitude $\geq$M5.0 and $\geq$X1.0.",http://arxiv.org/pdf/1308.1181v1
1444,Guided Perturbations: Self-Corrective Behavior in Convolutional Neural Networks,Swami Sankaranarayanan; Arpit Jain; Ser Nam Lim,Self corrective Perturbations for Semantic Segmentation and Classification,"Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.",http://arxiv.org/pdf/1703.07928v2
1483,Learning Robust Visual-Semantic Embeddings,Yao-Hung Hubert Tsai; Liang-Kang Huang; Ruslan Salakhutdinov,Learning Robust Visual-Semantic Embeddings,"Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.",http://arxiv.org/pdf/1703.05908v2
1510,PUnDA: Probabilistic Unsupervised Domain Adaptation for Knowledge Transfer Across Visual Categories,Behnam Gholami; Ognjen (Oggi) Rudovic; Vladimir Pavlovic,,,
1574,Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses,Christian Rupprecht; Iro Laina; Robert DiPietro; Maximilian Baust; Federico Tombari; Nassir Navab; Gregory D. Hager,Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses,"Many prediction tasks contain uncertainty. In some cases, uncertainty is inherent in the task itself. In future prediction, for example, many distinct outcomes are equally valid. In other cases, uncertainty arises from the way data is labeled. For example, in object detection, many objects of interest often go unlabeled, and in human pose estimation, occluded joints are often labeled with ambiguous values. In this work we focus on a principled approach for handling such scenarios. In particular, we propose a framework for reformulating existing single-prediction models as multiple hypothesis prediction (MHP) models and an associated meta loss and optimization procedure to train them. To demonstrate our approach, we consider four diverse applications: human pose estimation, future prediction, image classification and segmentation. We find that MHP models outperform their single-hypothesis counterparts in all cases, and that MHP models simultaneously expose valuable insights into the variability of predictions.",http://arxiv.org/pdf/1612.00197v3
1297,"CDTS: Collaborative Detection, Tracking, and Segmentation for Online Multiple Object Segmentation in Videos",Yeong Jun Koh; Chang-Su Kim,,,
1302,Temporal Superpixels Based on Proximity-Weighted Patch Matching,Se-Ho Lee; Won-Dong Jang; Chang-Su Kim,,,
1303,Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge,Ryota Hinami; Tao Mei; Shin'ichi Satoh,Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge,"This paper addresses the problem of joint detection and recounting of abnormal events in videos. Recounting of abnormal events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task in video surveillance, because it helps human observers quickly judge if they are false alarms or not. To describe the events in the human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object and action) is crucial. Although convolutional neural networks (CNNs) have achieved promising results in learning such concepts, it remains an open question as to how to effectively use CNNs for abnormal event detection, mainly due to the environment-dependent nature of the anomaly detection. In this paper, we tackle this problem by integrating a generic CNN model and environment-dependent anomaly detectors. Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. By appropriately plugging the model into anomaly detectors, we can detect and recount abnormal events while taking advantage of the discriminative power of CNNs. Our approach outperforms the state-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event detection and also produces promising results of abnormal event recounting.",http://arxiv.org/pdf/1709.09121v1
1407,TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals,Jiyang Gao; Zhenheng Yang; Kan Chen; Chen Sun; Ram Nevatia,TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals,"Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g. human actions) segments from untrimmed videos is an important step for large-scale video analysis. We propose a novel Temporal Unit Regression Network (TURN) model. There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals. TURN outperforms the state-of-the-art methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU. We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms state-of-the-art performance on THUMOS-14 and ActivityNet.",http://arxiv.org/pdf/1703.06189v2
1431,Online Real-Time Multiple Spatiotemporal Action Localisation and Prediction,Gurkirt Singh; Suman Saha; Michael Sapienza; Philip H. S. Torr; Fabio Cuzzolin,Online Real-time Multiple Spatiotemporal Action Localisation and Prediction,"We present a deep-learning framework for real-time multiple spatio-temporal (S/T) action localisation, classification and early prediction. Current state-of-the-art approaches work offline and are too slow to be useful in real- world settings. To overcome their limitations we introduce two major developments. Firstly, we adopt real-time SSD (Single Shot MultiBox Detector) convolutional neural networks to regress and classify detection boxes in each video frame potentially containing an action of interest. Secondly, we design an original and efficient online algorithm to incrementally construct and label `action tubes' from the SSD frame level detections. As a result, our system is not only capable of performing S/T detection in real time, but can also perform early action prediction in an online fashion. We achieve new state-of-the-art results in both S/T action localisation and early action prediction on the challenging UCF101-24 and J-HMDB-21 benchmarks, even when compared to the top offline competitors. To the best of our knowledge, ours is the first real-time (up to 40fps) system able to perform online S/T action localisation and early action prediction on the untrimmed videos of UCF101-24.",http://arxiv.org/pdf/1611.08563v6
1482,Leveraging Weak Semantic Relevance for Complex Video Event Classification,Chao Li; Jiewei Cao; Zi Huang; Lei Zhu; Heng Tao Shen,,,
1507,Weakly Supervised Summarization of Web Videos,Rameswar Panda; Abir Das; Ziyan Wu; Jan Ernst; Amit K. Roy-Chowdhury,,,
1569,FCN-rLSTM: Deep Spatio-Temporal Neural Networks for Vehicle Counting in City Cameras,Shanghang Zhang; Guanhang Wu; Jo√£o P. Costeira; Jos√© M. F. Moura,FCN-rLSTM: Deep Spatio-Temporal Neural Networks for Vehicle Counting in City Cameras,"In this paper, we develop deep spatio-temporal neural networks to sequentially count vehicles from low quality videos captured by city cameras (citycams). Citycam videos have low resolution, low frame rate, high occlusion and large perspective, making most existing methods lose their efficacy. To overcome limitations of existing methods and incorporate the temporal information of traffic video, we design a novel FCN-rLSTM network to jointly estimate vehicle density and vehicle count by connecting fully convolutional neural networks (FCN) with long short term memory networks (LSTM) in a residual learning fashion. Such design leverages the strengths of FCN for pixel-level prediction and the strengths of LSTM for learning complex temporal dynamics. The residual learning connection reformulates the vehicle count regression as learning residual functions with reference to the sum of densities in each frame, which significantly accelerates the training of networks. To preserve feature map resolution, we propose a Hyper-Atrous combination to integrate atrous convolution in FCN and combine feature maps of different convolution layers. FCN-rLSTM enables refined feature representation and a novel end-to-end trainable mapping from pixels to vehicle count. We extensively evaluated the proposed method on different counting tasks with three datasets, with experimental results demonstrating their effectiveness and robustness. In particular, FCN-rLSTM reduces the mean absolute error (MAE) from 5.31 to 4.21 on TRANCOS, and reduces the MAE from 2.74 to 1.53 on WebCamT. Training process is accelerated by 5 times on average.",http://arxiv.org/pdf/1707.09476v2
1366,Fast Face-Swap Using Convolutional Neural Networks,Iryna Korshunova; Wenzhe Shi; Joni Dambre; Lucas Theis,Fast Face-swap Using Convolutional Neural Networks,"We consider the problem of face swapping in images, where an input identity is transformed into a target identity while preserving pose, facial expression, and lighting. To perform this mapping, we use convolutional neural networks trained to capture the appearance of the target identity from an unstructured collection of his/her photographs.This approach is enabled by framing the face swapping problem in terms of style transfer, where the goal is to render an image in the style of another one. Building on recent advances in this area, we devise a new loss function that enables the network to produce highly photorealistic results. By combining neural networks with simple pre- and post-processing steps, we aim at making face swap work in real-time with no input from the user.",http://arxiv.org/pdf/1611.09577v2
1581,Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images,Tribhuvanesh Orekondy; Bernt Schiele; Mario Fritz,Towards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images,"With an increasing number of users sharing information online, privacy implications entailing such actions are a major concern. For explicit content, such as user profile or GPS data, devices (e.g. mobile phones) as well as web services (e.g. Facebook) offer to set privacy settings in order to enforce the users' privacy preferences. We propose the first approach that extends this concept to image content in the spirit of a Visual Privacy Advisor. First, we categorize personal information in images into 68 image attributes and collect a dataset, which allows us to train models that predict such information directly from images. Second, we run a user study to understand the privacy preferences of different users w.r.t. such attributes. Third, we propose models that predict user specific privacy score from images in order to enforce the users' privacy preferences. Our model is trained to predict the user specific privacy risk and even outperforms the judgment of the users, who often fail to follow their own privacy preferences on image data.",http://arxiv.org/pdf/1703.10660v2
249,First-Person Activity Forecasting With Online Inverse Reinforcement Learning,Nicholas Rhinehart; Kris M. Kitani,First-Person Activity Forecasting with Online Inverse Reinforcement Learning,"We address the problem of incrementally modeling and forecasting long-term goals of a first-person camera wearer: what the user will do, where they will go, and what goal they seek. In contrast to prior work in trajectory forecasting, our algorithm, DARKO, goes further to reason about semantic states (will I pick up an object?), and future goal states that are far in terms of both space and time. DARKO learns and forecasts from first-person visual observations of the user's daily behaviors via an Online Inverse Reinforcement Learning (IRL) approach. Classical IRL discovers only the rewards in a batch setting, whereas DARKO discovers the states, transitions, rewards, and goals of a user from streaming data. Among other results, we show DARKO forecasts goals better than competing methods in both noisy and ideal settings, and our approach is theoretically and empirically no-regret.",http://arxiv.org/pdf/1612.07796v3
531,Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment With Limited Resources,Adrian Bulat; Georgios Tzimiropoulos,Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources,"Our goal is to design architectures that retain the groundbreaking performance of CNNs for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual architecture that yields large performance improvement over the standard bottleneck block while having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (d) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. Code can be downloaded from https://www.adrianbulat.com/binary-cnn-landmarks",http://arxiv.org/pdf/1703.00862v2
777,MoFA: Model-Based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction,Ayush Tewari; Michael Zollh√∂fer; Hyeongwoo Kim; Pablo Garrido; Florian Bernard; Patrick P√©rez; Christian Theobalt,MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction,"In this work we propose a novel model-based deep convolutional autoencoder that addresses the highly challenging problem of reconstructing a 3D human face from a single in-the-wild color image. To this end, we combine a convolutional encoder network with an expert-designed generative model that serves as decoder. The core innovation is our new differentiable parametric decoder that encapsulates image formation analytically based on a generative model. Our decoder takes as input a code vector with exactly defined semantic meaning that encodes detailed face pose, shape, expression, skin reflectance and scene illumination. Due to this new way of combining CNN-based with model-based face reconstruction, the CNN-based encoder learns to extract semantically meaningful parameters from a single monocular input image. For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible. The obtained reconstructions compare favorably to current state-of-the-art approaches in terms of quality and richness of representation.",http://arxiv.org/pdf/1703.10580v1
1194,RPAN: An End-To-End Recurrent Pose-Attention Network for Action Recognition in Videos,Wenbin Du; Yali Wang; Yu Qiao,,,
1939,Temporal Non-Volume Preserving Approach to Facial Age-Progression and Age-Invariant Face Recognition,Chi Nhan Duong; Kha Gia Quach; Khoa Luu; Ngan Le; Marios Savvides,Temporal Non-Volume Preserving Approach to Facial Age-Progression and Age-Invariant Face Recognition,"Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages. In order to efficiently address the problem, this work first decomposes the aging process into multiple short-term stages. Then, a novel generative probabilistic model, named Temporal Non-Volume Preserving (TNVP) transformation, is presented to model the facial aging process at each stage. Unlike Generative Adversarial Networks (GANs), which requires an empirical balance threshold, and Restricted Boltzmann Machines (RBM), an intractable model, our proposed TNVP approach guarantees a tractable density function, exact inference and evaluation for embedding the feature transformations between faces in consecutive stages. Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces. Our approach can model any face in the wild provided with only four basic landmark points. Moreover, the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation. Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A large-scale face verification on Megaface challenge 1 is also performed to further show the advantages of our proposed approach.",http://arxiv.org/pdf/1703.08617v1
232,Attribute-Enhanced Face Recognition With Neural Tensor Fusion Networks,Guosheng Hu; Yang Hua; Yang Yuan; Zhihong Zhang; Zheng Lu; Sankha S. Mukherjee; Timothy M. Hospedales; Neil M. Robertson; Yongxin Yang,,,
467,Unlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro,Zhedong Zheng; Liang Zheng; Yi Yang,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,"The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/Person-reID_GAN.",http://arxiv.org/pdf/1701.07717v5
964,Egocentric Gesture Recognition Using Recurrent 3D Convolutional Neural Networks With Spatiotemporal Transformer Modules,Congqi Cao; Yifan Zhang; Yi Wu; Hanqing Lu; Jian Cheng,,,
1189,Recursive Spatial Transformer (ReST) for Alignment-Free Face Recognition,Wanglong Wu; Meina Kan; Xin Liu; Yi Yang; Shiguang Shan; Xilin Chen,,,
1600,Learning Discriminative Aggregation Network for Video-Based Face Recognition,Yongming Rao; Ji Lin; Jiwen Lu; Jie Zhou,,,
1859,Synergy Between Face Alignment and Tracking via Discriminative Global Consensus Optimization,Muhammad Haris Khan; John McDonagh; Georgios Tzimiropoulos,,,
2101,SVDNet for Pedestrian Retrieval,Yifan Sun; Liang Zheng; Weijian Deng; Shengjin Wang,SVDNet for Pedestrian Retrieval,"This paper proposes the SVDNet for retrieval problems, with focus on the application of person re-identification (re-ID). We view each weight vector within a fully connected (FC) layer in a convolutional neuron network (CNN) as a projection basis. It is observed that the weight vectors are usually highly correlated. This problem leads to correlations among entries of the FC descriptor, and compromises the retrieval performance based on the Euclidean distance. To address the problem, this paper proposes to optimize the deep representation learning process with Singular Vector Decomposition (SVD). Specifically, with the restraint and relaxation iteration (RRI) training scheme, we are able to iteratively integrate the orthogonality constraint in CNN training, yielding the so-called SVDNet. We conduct experiments on the Market-1501, CUHK03, and Duke datasets, and show that RRI effectively reduces the correlation among the projection vectors, produces more discriminative FC descriptors, and significantly improves the re-ID accuracy. On the Market-1501 dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for CaffeNet, and from 73.8% to 82.3% for ResNet-50.",http://arxiv.org/pdf/1703.05693v4
2114,Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features,Zijing Zhao; Ajay Kumar,,,
1678,Semantically Informed Multiview Surface Refinement,Maro≈° Bl√°ha; Mathias Rothermel; Martin R. Oswald; Torsten Sattler; Audrey Richard; Jan D. Wegner; Marc Pollefeys; Konrad Schindler,Semantically Informed Multiview Surface Refinement,"We present a method to jointly refine the geometry and semantic segmentation of 3D surface meshes. Our method alternates between updating the shape and the semantic labels. In the geometry refinement step, the mesh is deformed with variational energy minimization, such that it simultaneously maximizes photo-consistency and the compatibility of the semantic segmentations across a set of calibrated images. Label-specific shape priors account for interactions between the geometry and the semantic labels in 3D. In the semantic segmentation step, the labels on the mesh are updated with MRF inference, such that they are compatible with the semantic segmentations in the input images. Also, this step includes prior assumptions about the surface shape of different semantic classes. The priors induce a tight coupling, where semantic information influences the shape update and vice versa. Specifically, we introduce priors that favor (i) adaptive smoothing, depending on the class label; (ii) straightness of class boundaries; and (iii) semantic labels that are consistent with the surface orientation. The novel mesh-based reconstruction is evaluated in a series of experiments with real and synthetic data. We compare both to state-of-the-art, voxel-based semantic 3D reconstruction, and to purely geometric mesh refinement, and demonstrate that the proposed scheme yields improved 3D geometry as well as an improved semantic segmentation.",http://arxiv.org/pdf/1706.08336v1
1701,"BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects Without Using Depth",Mahdi Rad; Vincent Lepetit,"BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth","We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a ""holistic"" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes for the pose of objects' parts. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses for hand pose estimation. We improve the state-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.",http://arxiv.org/pdf/1703.10896v1
1734,Modeling Urban Scenes From Pointclouds,William Nguatem; Helmut Mayer,,,
1777,Parameter-Free Lens Distortion Calibration of Central Cameras,Filippo Bergamasco; Luca Cosmo; Andrea Gasparetto; Andrea Albarelli; Andrea Torsello,,,
1802,Pose Guided RGBD Feature Learning for 3D Object Pose Estimation,Vassileios Balntas; Andreas Doumanoglou; Caner Sahin; Juil Sock; Rigas Kouskouridas; Tae-Kyun Kim,,,
1823,Efficient Global Illumination for Morphable Models,Andreas Schneider; Sandro Sch√∂nborn; Lavrenti Frobeen; Bernhard Egger; Thomas Vetter,,,
1843,Low Compute and Fully Parallel Computer Vision With HashMatch,Sean Ryan Fanello; Julien Valentin; Adarsh Kowdle; Christoph Rhemann; Vladimir Tankovich; Carlo Ciliberto; Philip Davidson; Shahram Izadi,,,
1848,Dense Non-Rigid Structure-From-Motion and Shading With Unknown Albedos,Mathias Gallardo; Toby Collins; Adrien Bartoli,,,
1895,From Point Clouds to Mesh Using Regression,ƒΩubor Ladick√Ω; Olivier Saurer; SoHyeon Jeong; Fabio Maninchedda; Marc Pollefeys,,,
1921,Stereo DSO: Large-Scale Direct Sparse Visual Odometry With Stereo Cameras,Rui Wang; Martin Schw√∂rer; Daniel Cremers,Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras,"We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for highly accurate real-time visual odometry estimation of large-scale environments from stereo cameras. It jointly optimizes for all the model parameters within the active window, including the intrinsic/extrinsic camera parameters of all keyframes and the depth values of all selected pixels. In particular, we propose a novel approach to integrate constraints from static stereo into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is realized by sampling pixels uniformly from image regions with sufficient intensity gradient. Fixed-baseline stereo resolves scale drift. It also reduces the sensitivities to large optical flow and to rolling shutter effect which are known shortcomings of direct image alignment methods. Quantitative evaluation demonstrates that the proposed Stereo DSO outperforms existing state-of-the-art visual odometry methods both in terms of tracking accuracy and robustness. Moreover, our method delivers a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches while providing a higher reconstruction density than feature-based methods.",http://arxiv.org/pdf/1708.07878v1
1953,Space-Time Localization and Mapping,Minhaeng Lee; Charless C. Fowlkes,Local Operations and Completely Positive Maps in Algebraic Quantum Field Theory,"Einstein introduced the locality principle which states that all physical effect in some finite space-time region does not influence its space-like separated finite region. Recently, in algebraic quantum field theory, R\'{e}dei captured the idea of the locality principle by the notion of operational separability. The operation in operational separability is performed in some finite space-time region, and leaves unchanged the state in its space-like separated finite space-time region. This operation is defined with a completely positive map. In the present paper, we justify using a completely positive map as a local operation in algebraic quantum field theory, and show that this local operation can be approximately written with Kraus operators under the funnel property.",http://arxiv.org/pdf/1704.01229v1
1660,Benchmarking Single-Image Reflection Removal Algorithms,Renjie Wan; Boxin Shi; Ling-Yu Duan; Ah-Hwee Tan; Alex C. Kot,,,
1598,Attention-Aware Deep Reinforcement Learning for Video Face Recognition,Yongming Rao; Jiwen Lu; Jie Zhou,,,
1617,Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation,Bugra Tekin; Pablo M√°rquez-Neila; Mathieu Salzmann; Pascal Fua,Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation,Most recent approaches to monocular 3D human pose estimation rely on Deep Learning. They typically involve regressing from an image to either 3D joint coordinates directly or 2D joint locations from which 3D coordinates are inferred. Both approaches have their strengths and weaknesses and we therefore propose a novel architecture designed to deliver the best of both worlds by performing both simultaneously and fusing the information along the way. At the heart of our framework is a trainable fusion scheme that learns how to fuse the information optimally instead of being hand-designed. This yields significant improvements upon the state-of-the-art on standard 3D human pose estimation benchmarks.,http://arxiv.org/pdf/1611.05708v3
1633,Deep Facial Action Unit Recognition From Partially Labeled Data,Shan Wu; Shangfei Wang; Bowen Pan; Qiang Ji,,,
1735,Pose-Driven Deep Convolutional Model for Person Re-Identification,Chi Su; Jianing Li; Shiliang Zhang; Junliang Xing; Wen Gao; Qi Tian,Pose-driven Deep Convolutional Model for Person Re-identification,"Feature extraction and matching are two crucial components in person Re-Identification (ReID). The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images. To overcome these difficulties, in this work we propose a Pose-driven Deep Convolutional (PDC) model to learn improved feature extraction and matching models from end to end. Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts. To match the features from global human body and local body parts, a pose driven feature weighting sub-network is further designed to learn adaptive feature fusions. Extensive experimental analyses and results on three popular datasets demonstrate significant performance improvements of our model over all published state-of-the-art methods.",http://arxiv.org/pdf/1709.08325v1
1883,Recognition of Action Units in the Wild With Deep Nets and a New Global-Local Loss,C. Fabian Benitez-Quiroz; Yan Wang; Aleix M. Martinez,,,
1901,Faster Than Real-Time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses,Chandrasekhar Bhagavatula; Chenchen Zhu; Khoa Luu; Marios Savvides,Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses,"Facial alignment involves finding a set of landmark points on an image with a known semantic meaning. However, this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes. In order to extract consistent alignment points across large poses, the 3D structure of the face must be considered in the alignment step. However, extracting a 3D structure from a single 2D image usually requires alignment in the first place. We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network (3DSTN) to model both the camera projection matrix and the warping parameters of a 3D model. By utilizing a generic 3D model and a Thin Plate Spline (TPS) warping function, we are able to generate subject specific 3D shapes without the need for a large 3D shape basis. In addition, our proposed network can be trained in an end-to-end framework on entirely synthetic data from the 300W-LP dataset. Unlike other 3D methods, our approach only requires one pass through the network resulting in a faster than real-time alignment. Evaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW) and AFLW2000-3D datasets show our method achieves state-of-the-art performance over other 3D approaches to alignment.",http://arxiv.org/pdf/1707.05653v2
1987,Towards Large-Pose Face Frontalization in the Wild,Xi Yin; Xiang Yu; Kihyuk Sohn; Xiaoming Liu; Manmohan Chandraker,Towards Large-Pose Face Frontalization in the Wild,"Despite recent advances in face recognition using deep learning, severe accuracy drops are observed for large pose variations in unconstrained environments. Learning pose-invariant features is one solution, but needs expensively labeled large-scale data and carefully designed feature learning algorithms. In this work, we focus on frontalizing faces in the wild under various head poses, including extreme profile views. We propose a novel deep 3D Morphable Model (3DMM) conditioned Face Frontalization Generative Adversarial Network (GAN), termed as FF-GAN, to generate neutral head pose face images. Our framework differs from both traditional GANs and 3DMM based modeling. Incorporating 3DMM into the GAN structure provides shape and appearance priors for fast convergence with less training data, while also supporting end-to-end training. The 3DMM-conditioned GAN employs not only the discriminator and generator loss but also a new masked symmetry loss to retain visual quality under occlusions, besides an identity loss to recover high frequency information. Experiments on face recognition, landmark localization and 3D reconstruction consistently show the advantage of our frontalization method on faces in the wild datasets.",http://arxiv.org/pdf/1704.06244v3
1618,A Joint Intrinsic-Extrinsic Prior Model for Retinex,Bolun Cai; Xianming Xu; Kailing Guo; Kui Jia; Bin Hu; Dacheng Tao,,,
1689,Going Unconstrained With Rolling Shutter Deblurring,Mahesh Mohan M. R.; A. N. Rajagopalan; Gunasekaran Seetharaman,,,
1709,A Stagewise Refinement Model for Detecting Salient Objects in Images,Tiantian Wang; Ali Borji; Lihe Zhang; Pingping Zhang; Huchuan Lu,,,
1722,From Square Pieces to Brick Walls: The Next Challenge in Solving Jigsaw Puzzles,Shir Gur; Ohad Ben-Shahar,,,
1773,Online Video Deblurring via Dynamic Temporal Blending Network,Tae Hyun Kim; Kyoung Mu Lee; Bernhard Sch√∂lkopf; Michael Hirsch,Online Video Deblurring via Dynamic Temporal Blending Network,"State-of-the-art video deblurring methods are capable of removing non-uniform blur caused by unwanted camera shake and/or object motion in dynamic scenes. However, most existing methods are based on batch processing and thus need access to all recorded frames, rendering them computationally demanding and time consuming and thus limiting their practical use. In contrast, we propose an online (sequential) video deblurring method based on a spatio-temporal recurrent network that allows for real-time performance. In particular, we introduce a novel architecture which extends the receptive field while keeping the overall size of the network small to enable fast execution. In doing so, our network is able to remove even large blur caused by strong camera shake and/or fast moving objects. Furthermore, we propose a novel network layer that enforces temporal consistency between consecutive frames by dynamic temporal blending which compares and adaptively (at test time) shares features obtained at different time steps. We show the superiority of the proposed method in an extensive experimental evaluation.",http://arxiv.org/pdf/1704.03285v1
1787,Supervision by Fusion: Towards Unsupervised Learning of Deep Salient Object Detector,Dingwen Zhang; Junwei Han; Yu Zhang,,,
1852,Fast Multi-Image Matching via Density-Based Clustering,Roberto Tron; Xiaowei Zhou; Carlos Esteves; Kostas Daniilidis,,,
2027,Characterizing and Improving Stability in Neural Style Transfer,Agrim Gupta; Justin Johnson; Alexandre Alahi; Li Fei-Fei,Characterizing and Improving Stability in Neural Style Transfer,"Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not re- quire optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.",http://arxiv.org/pdf/1705.02092v1
1593,Cross-Modal Deep Variational Hashing,Venice Erin Liong; Jiwen Lu; Yap-Peng Tan; Jie Zhou,,,
1616,Spatial Memory for Context Reasoning in Object Detection,Xinlei Chen; Abhinav Gupta,Spatial Memory for Context Reasoning in Object Detection,"Modeling instance-level context and object-object relationships is extremely challenging. It requires reasoning about bounding boxes of different classes, locations \etc. Above all, instance-level spatial reasoning inherently requires modeling conditional distributions on previous detections. Unfortunately, our current object detection systems do not have any {\bf memory} to remember what to condition on! The state-of-the-art object detectors still detect all object in parallel followed by non-maximal suppression (NMS). While memory has been used for tasks such as captioning, they mostly use image-level memory cells without capturing the spatial layout. On the other hand, modeling object-object relationships requires {\bf spatial} reasoning -- not only do we need a memory to store the spatial layout, but also a effective reasoning module to extract spatial patterns. This paper presents a conceptually simple yet powerful solution -- Spatial Memory Network (SMN), to model the instance-level context efficiently and effectively. Our spatial memory essentially assembles object instances back into a pseudo ""image"" representation that is easy to be fed into another ConvNet for object-object context reasoning. This leads to a new sequential reasoning architecture where image and memory are processed in parallel to obtain detections which update the memory again. We show our SMN direction is promising as it provides 2.2\% improvement over baseline Faster RCNN on the COCO dataset so far.",http://arxiv.org/pdf/1704.04224v1
1648,Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual Cross Retrieval,Yuming Shen; Li Liu; Ling Shao; Jingkuan Song,Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual Cross Retrieval,"Cross-modal hashing is usually regarded as an effective technique for large-scale textual-visual cross retrieval, where data from different modalities are mapped into a shared Hamming space for matching. Most of the traditional textual-visual binary encoding methods only consider holistic image representations and fail to model descriptive sentences. This renders existing methods inappropriate to handle the rich semantics of informative cross-modal data for quality textual-visual search tasks. To address the problem of hashing cross-modal data with semantic-rich cues, in this paper, a novel integrated deep architecture is developed to effectively encode the detailed semantics of informative images and long descriptive sentences, named as Textual-Visual Deep Binaries (TVDB). In particular, region-based convolutional networks with long short-term memory units are introduced to fully explore image regional details while semantic cues of sentences are modeled by a text convolutional network. Additionally, we propose a stochastic batch-wise training routine, where high-quality binary codes and deep encoding functions are efficiently optimized in an alternating manner. Experiments are conducted on three multimedia datasets, i.e. Microsoft COCO, IAPR TC-12, and INRIA Web Queries, where the proposed TVDB model significantly outperforms state-of-the-art binary coding methods in the task of cross-modal retrieval.",http://arxiv.org/pdf/1708.02531v1
1703,Learning a Recurrent Residual Fusion Network for Multimodal Matching,Yu Liu; Yanming Guo; Erwin M. Bakker; Michael S. Lew,,,
1720,Rotational Subgroup Voting and Pose Clustering for Robust 3D Object Recognition,Anders Glent Buch; Lilita Kiforenko; Dirk Kraft,Rotational Subgroup Voting and Pose Clustering for Robust 3D Object Recognition,"It is possible to associate a highly constrained subset of relative 6 DoF poses between two 3D shapes, as long as the local surface orientation, the normal vector, is available at every surface point. Local shape features can be used to find putative point correspondences between the models due to their ability to handle noisy and incomplete data. However, this correspondence set is usually contaminated by outliers in practical scenarios, which has led to many past contributions based on robust detectors such as the Hough transform or RANSAC. The key insight of our work is that a single correspondence between oriented points on the two models is constrained to cast votes in a 1 DoF rotational subgroup of the full group of poses, SE(3). Kernel density estimation allows combining the set of votes efficiently to determine a full 6 DoF candidate pose between the models. This modal pose with the highest density is stable under challenging conditions, such as noise, clutter, and occlusions, and provides the output estimate of our method.   We first analyze the robustness of our method in relation to noise and show that it handles high outlier rates much better than RANSAC for the task of 6 DoF pose estimation. We then apply our method to four state of the art data sets for 3D object recognition that contain occluded and cluttered scenes. Our method achieves perfect recall on two LIDAR data sets and outperforms competing methods on two RGB-D data sets, thus setting a new standard for general 3D object recognition using point cloud data.",http://arxiv.org/pdf/1709.02142v1
1727,CoupleNet: Coupling Global Structure With Local Parts for Object Detection,Yousong Zhu; Chaoyang Zhao; Jinqiao Wang; Xu Zhao; Yi Wu; Hanqing Lu,CoupleNet: Coupling Global Structure with Local Parts for Object Detection,"The region-based Convolutional Neural Network (CNN) detectors such as Faster R-CNN or R-FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together. Although R-FCN has achieved higher detection speed while keeping the detection performance, the global structure information is ignored by the position-sensitive score maps. To fully explore the local and global properties, in this paper, we propose a novel fully convolutional network, named as CoupleNet, to couple the global structure with local parts for object detection. Specifically, the object proposals obtained by the Region Proposal Network (RPN) are fed into the the coupling module which consists of two branches. One branch adopts the position-sensitive RoI (PSRoI) pooling to capture the local part information of the object, while the other employs the RoI pooling to encode the global and context information. Next, we design different coupling strategies and normalization ways to make full use of the complementary advantages between the global and local branches. Extensive experiments demonstrate the effectiveness of our approach. We achieve state-of-the-art results on all three challenging datasets, i.e. a mAP of 82.7% on VOC07, 80.4% on VOC12, and 34.4% on COCO. Codes will be made publicly available.",http://arxiv.org/pdf/1708.02863v1
1731,Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training,Rakshith Shetty; Marcus Rohrbach; Lisa Anne Hendricks; Mario Fritz; Bernt Schiele,Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training,"While strong progress has been made in image captioning over the last years, machine and human captions are still quite distinct. A closer look reveals that this is due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans -- rightfully so -- generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not considered in today's systems.   To address these challenges, we change the training objective of the caption generator from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human generated captions. Instead of handcrafting such a learning target, we employ adversarial training in combination with an approximate Gumbel sampler to implicitly match the generated distribution to the human one. While our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions, we generate a set of diverse captions, that are significantly less biased and match the word statistics better in several aspects.",http://arxiv.org/pdf/1703.10476v1
1775,Drone-Based Object Counting by Spatially Regularized Regional Proposal Network,Meng-Ru Hsieh; Yen-Liang Lin; Winston H. Hsu,Drone-based Object Counting by Spatially Regularized Regional Proposal Network,"Existing counting methods often adopt regression-based approaches and cannot precisely localize the target objects, which hinders the further analysis (e.g., high-level understanding and fine-grained classification). In addition, most of prior work mainly focus on counting objects in static environments with fixed cameras. Motivated by the advent of unmanned flying vehicles (i.e., drones), we are interested in detecting and counting objects in such dynamic environments. We propose Layout Proposal Networks (LPNs) and spatial kernels to simultaneously count and localize target objects (e.g., cars) in videos recorded by the drone. Different from the conventional region proposal methods, we leverage the spatial layout information (e.g., cars often park regularly) and introduce these spatially regularized constraints into our network to improve the localization accuracy. To evaluate our counting method, we present a new large-scale car parking lot dataset (CARPK) that contains nearly 90,000 cars captured from different parking lots. To the best of our knowledge, it is the first and the largest drone view dataset that supports object counting, and provides the bounding box annotations.",http://arxiv.org/pdf/1707.05972v3
1824,BlitzNet: A Real-Time Deep Network for Scene Understanding,Nikita Dvornik; Konstantin Shmelkov; Julien Mairal; Cordelia Schmid,BlitzNet: A Real-Time Deep Network for Scene Understanding,"Real-time scene understanding has become crucial in many applications such as autonomous driving. In this paper, we propose a deep architecture, called BlitzNet, that jointly performs object detection and semantic segmentation in one forward pass, allowing real-time computations. Besides the computational gain of having a single network to perform several tasks, we show that object detection and semantic segmentation benefit from each other in terms of accuracy. Experimental results for VOC and COCO datasets show state-of-the-art performance for object detection and segmentation among real time systems.",http://arxiv.org/pdf/1708.02813v1
1857,Situation Recognition With Graph Neural Networks,Ruiyu Li; Makarand Tapaswi; Renjie Liao; Jiaya Jia; Raquel Urtasun; Sanja Fidler,Situation Recognition with Graph Neural Networks,"We address the problem of recognizing situations in images. Given an image, the task is to predict the most salient verb (action), and fill its semantic roles such as who is performing the action, what is the source and target of the action, etc. Different verbs have different roles (e.g. attacking has weapon), and each role can take on many possible values (nouns). We propose a model based on Graph Neural Networks that allows us to efficiently capture joint dependencies between roles using neural networks defined on a graph. Experiments with different graph connectivities show that our approach that propagates information between roles significantly outperforms existing work, as well as multiple baselines. We obtain roughly 3-5% improvement over previous work in predicting the full situation. We also provide a thorough qualitative analysis of our model and influence of different roles in the verbs.",http://arxiv.org/pdf/1708.04320v1
1900,Learning Visual N-Grams From Web Data,Ang Li; Allan Jabri; Armand Joulin; Laurens van der Maaten,,,
1945,Attention-Based Multimodal Fusion for Video Description,Chiori Hori; Takaaki Hori; Teng-Yok Lee; Ziming Zhang; Bret Harsham; John R. Hershey; Tim K. Marks; Kazuhiko Sumi,Attention-Based Multimodal Fusion for Video Description,"Currently successful methods for video description are based on encoder-decoder sentence generation using recur-rent neural networks (RNNs). Recent work has shown the advantage of integrating temporal and/or spatial attention mechanisms into these models, in which the decoder net-work predicts each word in the description by selectively giving more weight to encoded features from specific time frames (temporal attention) or to features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model to selectively attend not just to specific times or spatial regions, but to specific modalities of input such as image features, motion features, and audio features. Our new modality-dependent attention mechanism, which we call multimodal attention, provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state of the art. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention significantly outperforms the model that uses temporal attention alone.",http://arxiv.org/pdf/1701.03126v2
1981,"Learning the Latent ""Look"": Unsupervised Discovery of a Style-Coherent Embedding From Fashion Images",Wei-Lin Hsiao; Kristen Grauman,"Learning the Latent ""Look"": Unsupervised Discovery of a Style-Coherent Embedding from Fashion Images","What defines a visual style? Fashion styles emerge organically from how people assemble outfits of clothing, making them difficult to pin down with a computational model. Low-level visual similarity can be too specific to detect stylistically similar images, while manually crafted style categories can be too abstract to capture subtle style differences. We propose an unsupervised approach to learn a style-coherent representation. Our method leverages probabilistic polylingual topic models based on visual attributes to discover a set of latent style factors. Given a collection of unlabeled fashion images, our approach mines for the latent styles, then summarizes outfits by how they mix those styles. Our approach can organize galleries of outfits by style without requiring any style labels. Experiments on over 100K images demonstrate its promise for retrieving, mixing, and summarizing fashion images by their style.",http://arxiv.org/pdf/1707.03376v2
1997,Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks,Tanmay Gupta; Kevin Shih; Saurabh Singh; Derek Hoiem,Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks,"A grand goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.",http://arxiv.org/pdf/1704.00260v1
2001,Learning Discriminative Latent Attributes for Zero-Shot Classification,Huajie Jiang; Ruiping Wang; Shiguang Shan; Yi Yang; Xilin Chen,,,
2006,PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN,Hanwang Zhang; Zawlin Kyaw; Jinyang Yu; Shih-Fu Chang,,,
1621,Higher-Order Minimum Cost Lifted Multicuts for Motion Segmentation,Margret Keuper,Higher-Order Minimum Cost Lifted Multicuts for Motion Segmentation,"Most state-of-the-art motion segmentation algorithms draw their potential from modeling motion differences of local entities such as point trajectories in terms of pairwise potentials in graphical models. Inference in instances of minimum cost multicut problems defined on such graphs al- lows to optimize the number of the resulting segments along with the segment assignment. However, pairwise potentials limit the discriminative power of the employed motion models to translational differences. More complex models such as Euclidean or affine transformations call for higher-order potentials and a tractable inference in the resulting higher-order graphical models. In this paper, we (1) introduce a generalization of the minimum cost lifted multicut problem to hypergraphs, and (2) propose a simple primal feasible heuristic that allows for a reasonably efficient inference in instances of higher-order lifted multicut problem instances defined on point trajectory hypergraphs for motion segmentation. The resulting motion segmentations improve over the state-of-the-art on the FBMS-59 dataset.",http://arxiv.org/pdf/1704.01811v2
1631,Deep Free-Form Deformation Network for Object-Mask Registration,Haoyang Zhang; Xuming He,,,
1811,Region-Based Correspondence Between 3D Shapes via Spatially Smooth Biclustering,Matteo Denitto; Simone Melzi; Manuele Bicego; Umberto Castellani; Alessandro Farinelli; M√°rio A. T. Figueiredo; Yanir Kleiman; Maks Ovsjanikov,,,
1653,Learning Discriminative ab-Divergences for Positive Definite Matrices,Anoop Cherian; Panagiotis Stanitsas; Mehrtash Harandi; Vassilios Morellas; Nikolaos Papanikolopoulos,,,
1686,Consensus Convolutional Sparse Coding,Biswarup Choudhury; Robin Swanson; Felix Heide; Gordon Wetzstein; Wolfgang Heidrich,,,
1797,Domain-Adaptive Deep Network Compression,Marc Masana; Joost van de Weijer; Luis Herranz; Andrew D. Bagdanov; Jose M. √Ålvarez,Domain-adaptive deep network compression,"Deep Neural Networks trained on large datasets can be easily transferred to new domains with far fewer labeled examples by a process called fine-tuning. This has the advantage that representations learned in the large source domain can be exploited on smaller target domains. However, networks designed to be optimal for the source task are often prohibitively large for the target task. In this work we address the compression of networks after domain transfer.   We focus on compression algorithms based on low-rank matrix decomposition. Existing methods base compression solely on learned network weights and ignore the statistics of network activations. We show that domain transfer leads to large shifts in network activations and that it is desirable to take this into account when compressing. We demonstrate that considering activation statistics when compressing weights leads to a rank-constrained regression problem with a closed-form solution. Because our method takes into account the target domain, it can more optimally remove the redundancy in the weights. Experiments show that our Domain Adaptive Low Rank (DALR) method significantly outperforms existing low-rank compression techniques. With our approach, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone -- with only a minor or no loss in accuracy. When applied to domain-transferred networks it allows for compression down to only 5-20% of the original number of parameters with only a minor drop in performance.",http://arxiv.org/pdf/1709.01041v2
1817,Self-Supervised Learning of Pose Embeddings From Spatiotemporal Relations in Videos,√ñmer S√ºmer; Tobias Dencker; Bj√∂rn Ommer,Self-supervised Learning of Pose Embeddings from Spatiotemporal Relations in Videos,"Human pose analysis is presently dominated by deep convolutional networks trained with extensive manual annotations of joint locations and beyond. To avoid the need for expensive labeling, we exploit spatiotemporal relations in training videos for self-supervised learning of pose embeddings. The key idea is to combine temporal ordering and spatial placement estimation as auxiliary tasks for learning pose similarities in a Siamese convolutional network. Since the self-supervised sampling of both tasks from natural videos can result in ambiguous and incorrect training labels, our method employs a curriculum learning idea that starts training with the most reliable data samples and gradually increases the difficulty. To further refine the training process we mine repetitive poses in individual videos which provide reliable labels while removing inconsistencies. Our pose embeddings capture visual characteristics of human pose that can boost existing supervised representations in human pose estimation and retrieval. We report quantitative and qualitative results on these tasks in Olympic Sports, Leeds Pose Sports and MPII Human Pose datasets.",http://arxiv.org/pdf/1708.02179v1
1831,Approximate Grassmannian Intersections: Subspace-Valued Subspace Learning,Calvin Murdock; Fernando De la Torre,,,
1884,Side Information in Robust Principal Component Analysis: Algorithms and Applications,Niannan Xue; Yannis Panagakis; Stefanos Zafeiriou,Side Information in Robust Principal Component Analysis: Algorithms and Applications,"Robust Principal Component Analysis (RPCA) aims at recovering a low-rank subspace from grossly corrupted high-dimensional (often visual) data and is a cornerstone in many machine learning and computer vision applications. Even though RPCA has been shown to be very successful in solving many rank minimisation problems, there are still cases where degenerate or suboptimal solutions are obtained. This is likely to be remedied by taking into account of domain-dependent prior knowledge. In this paper, we propose two models for the RPCA problem with the aid of side information on the low-rank structure of the data. The versatility of the proposed methods is demonstrated by applying them to four applications, namely background subtraction, facial image denoising, face and facial expression recognition. Experimental results on synthetic and five real world datasets indicate the robustness and effectiveness of the proposed methods on these application domains, largely outperforming six previous approaches.",http://arxiv.org/pdf/1702.00648v2
1890,Summarization and Classification of Wearable Camera Streams by Learning the Distributions Over Deep Features of Out-Of-Sample Image Sequences,Alessandro Perina; Sadegh Mohammadi; Nebojsa Jojic; Vittorio Murino,,,
1897,Unsupervised Learning From Video to Detect Foreground Objects in Single Images,Ioana Croitoru; Simion-Vlad Bogolin; Marius Leordeanu,Unsupervised learning from video to detect foreground objects in single images,"Unsupervised learning from visual data is one of the most difficult challenges in computer vision, being a fundamental task for understanding how visual recognition works. From a practical point of view, learning from unsupervised visual input has an immense practical value, as very large quantities of unlabeled videos can be collected at low cost. In this paper, we address the task of unsupervised learning to detect and segment foreground objects in single images. We achieve our goal by training a student pathway, consisting of a deep neural network. It learns to predict from a single input image (a video frame) the output for that particular frame, of a teacher pathway that performs unsupervised object discovery in video. Our approach is different from the published literature that performs unsupervised discovery in videos or in collections of images at test time. We move the unsupervised discovery phase during the training stage, while at test time we apply the standard feed-forward processing along the student pathway. This has a dual benefit: firstly, it allows in principle unlimited possibilities of learning and generalization during training, while remaining very fast at testing. Secondly, the student not only becomes able to detect in single images significantly better than its unsupervised video discovery teacher, but it also achieves state of the art results on two important current benchmarks, YouTube Objects and Object Discovery datasets. Moreover, at test time, our system is at least two orders of magnitude faster than other previous methods.",http://arxiv.org/pdf/1703.10901v1
1927,Supplementary Meta-Learning: Towards a Dynamic Model for Deep Neural Networks,Feihu Zhang; Benjamin W. Wah,,,
1967,Adversarial Inverse Graphics Networks: Learning 2D-To-3D Lifting and Image-To-Image Translation From Unpaired Supervision,Hsiao-Yu Fish Tung; Adam W. Harley; William Seto; Katerina Fragkiadaki,Adversarial Inverse Graphics Networks: Learning 2D-to-3D Lifting and Image-to-Image Translation from Unpaired Supervision,"Researchers have developed excellent feed-forward models that learn to map images to desired outputs, such as to the images' latent factors, or to other images, using supervised learning. Learning such mappings from unlabelled data, or improving upon supervised models by exploiting unlabelled data, remains elusive. We argue that there are two important parts to learning without annotations: (i) matching the predictions to the input observations, and (ii) matching the predictions to known priors. We propose Adversarial Inverse Graphics networks (AIGNs): weakly supervised neural network models that combine feedback from rendering their predictions, with distribution matching between their predictions and a collection of ground-truth factors. We apply AIGNs to 3D human pose estimation and 3D structure and egomotion estimation, and outperform models supervised by only paired annotations. We further apply AIGNs to facial image transformation using super-resolution and inpainting renderers, while deliberately adding biases in the ground-truth datasets. Our model seamlessly incorporates such biases, rendering input faces towards young, old, feminine, masculine or Tom Cruise-like equivalents (depending on the chosen bias), or adding lip and nose augmentations while inpainting concealed lips and noses.",http://arxiv.org/pdf/1705.11166v3
1986,Active Learning for Human Pose Estimation,Buyu Liu; Vittorio Ferrari,Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video,"Understanding the camera wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the camera--the wearer's body pose. Prior work focuses on estimating the pose of hands and arms when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of daily life activities. We propose to infer the ""invisible pose"" of a person behind the egocentric camera. Given a single video, our efficient learning-based approach returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scene--which changes predictably as a function of body pose--as well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel energy minimization scheme to infer the pose sequence. It uses soft predictions of the poses per time instant together with a non-parametric model of human pose dynamics over longer windows. Our method outperforms an array of possible alternatives, including deep learning approaches for direct pose regression from images.",http://arxiv.org/pdf/1603.07763v1
1996,Interleaved Group Convolutions,Ting Zhang; Guo-Jun Qi; Bin Xiao; Jingdong Wang,Interleaved Group Convolutions for Deep Neural Networks,"In this paper, we present a simple and modularized neural network architecture, named interleaved group convolutional neural networks (IGCNets). The main point lies in a novel building block, a pair of two successive interleaved group convolutions: primary group convolution and secondary group convolution. The two group convolutions are complementary: (i) the convolution on each partition in primary group convolution is a spatial convolution, while on each partition in secondary group convolution, the convolution is a point-wise convolution; (ii) the channels in the same secondary partition come from different primary partitions. We discuss one representative advantage: Wider than a regular convolution with the number of parameters and the computation complexity preserved. We also show that regular convolutions, group convolution with summation fusion, and the Xception block are special cases of interleaved group convolutions. Empirical results over standard benchmarks, CIFAR-$10$, CIFAR-$100$, SVHN and ImageNet demonstrate that our networks are more efficient in using parameters and computation complexity with similar or higher accuracy.",http://arxiv.org/pdf/1707.02725v2
1788,Learning-Based Cloth Material Recovery From Video,Shan Yang; Junbang Liang; Ming C. Lin,,,
1863,Unsupervised Video Understanding by Reconciliation of Posture Similarities,Timo Milbich; Miguel Bautista; Ekaterina Sutter; Bj√∂rn Ommer,Unsupervised Video Understanding by Reconciliation of Posture Similarities,"Understanding human activity and being able to explain it in detail surpasses mere action classification by far in both complexity and value. The challenge is thus to describe an activity on the basis of its most fundamental constituents, the individual postures and their distinctive transitions. Supervised learning of such a fine-grained representation based on elementary poses is very tedious and does not scale. Therefore, we propose a completely unsupervised deep learning procedure based solely on video sequences, which starts from scratch without requiring pre-trained networks, predefined body models, or keypoints. A combinatorial sequence matching algorithm proposes relations between frames from subsets of the training data, while a CNN is reconciling the transitivity conflicts of the different subsets to learn a single concerted pose embedding despite changes in appearance across sequences. Without any manual annotation, the model learns a structured representation of postures and their temporal development. The model not only enables retrieval of similar postures but also temporal super-resolution. Additionally, based on a recurrent formulation, next frames can be synthesized.",http://arxiv.org/pdf/1708.01191v1
1873,Action Tubelet Detector for Spatio-Temporal Action Localization,Vicky Kalogeiton; Philippe Weinzaepfel; Vittorio Ferrari; Cordelia Schmid,Action Tubelet Detector for Spatio-Temporal Action Localization,"Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level that are then linked or tracked across time. In this paper, we leverage the temporal continuity of videos instead of operating at the frame level. We propose the ACtion Tubelet detector (ACT-detector) that takes as input a sequence of frames and outputs tubelets, i.e., sequences of bounding boxes with associated scores. The same way state-of-the-art object detectors rely on anchor boxes, our ACT-detector is based on anchor cuboids. We build upon the SSD framework. Convolutional features are extracted for each frame, while scores and regressions are based on the temporal stacking of these features, thus exploiting information from a sequence. Our experimental results show that leveraging sequences of frames significantly improves detection performance over using individual frames. The gain of our tubelet detector can be explained by both more accurate scores and more precise localization. Our ACT-detector outperforms the state-of-the-art methods for frame-mAP and video-mAP on the J-HMDB and UCF-101 datasets, in particular at high overlap thresholds.",http://arxiv.org/pdf/1705.01861v3
1935,AMTnet: Action-Micro-Tube Regression by End-To-End Trainable Deep Architecture,Suman Saha; Gurkirt Singh; Fabio Cuzzolin,AMTnet: Action-Micro-Tube Regression by End-to-end Trainable Deep Architecture,"Dominant approaches to action detection can only provide sub-optimal solutions to the problem, as they rely on seeking frame-level detections, to later compose them into ""action tubes"" in a post-processing step. With this paper we radically depart from current practice, and take a first step towards the design and implementation of a deep network architecture able to classify and regress whole video subsets, so providing a truly optimal solution of the action detection problem. In this work, in particular, we propose a novel deep net framework able to regress and classify 3D region proposals spanning two successive video frames, whose core is an evolution of classical region proposal networks (RPNs). As such, our 3D-RPN net is able to effectively encode the temporal aspect of actions by purely exploiting appearance, as opposed to methods which heavily rely on expensive flow maps. The proposed model is end-to-end trainable and can be jointly optimised for action localisation and classification in a single step. At test time the network predicts ""micro-tubes"" encompassing two successive frames, which are linked up into complete action tubes via a new algorithm which exploits the temporal encoding learned by the network and cuts computation time by 50%. Promising results on the J-HMDB-21 and UCF-101 action detection datasets show that our model does outperform the state-of-the-art when relying purely on appearance.",http://arxiv.org/pdf/1704.04952v2
1769,Constrained Convolutional Sparse Coding for Parametric Based Reconstruction of Line Drawings,Sara Shaheen; Lama Affara; Bernard Ghanem,,,
1779,Neural Ctrl-F: Segmentation-Free Query-By-String Word Spotting in Handwritten Manuscript Collections,Tomas Wilkinson; Jonas Lindstr√∂m; Anders Brun,,,
851,Spatial-Aware Object Embeddings for Zero-Shot Localization and Classification of Actions,Pascal Mettes; Cees G. M. Snoek,Spatial-Aware Object Embeddings for Zero-Shot Localization and Classification of Actions,"We aim for zero-shot localization and classification of human actions in video. Where traditional approaches rely on global attribute or object classification scores for their zero-shot knowledge transfer, our main contribution is a spatial-aware object embedding. To arrive at spatial awareness, we build our embedding on top of freely available actor and object detectors. Relevance of objects is determined in a word embedding space and further enforced with estimated spatial preferences. Besides local object awareness, we also embed global object awareness into our embedding to maximize actor and object interaction. Finally, we exploit the object positions and sizes in the spatial-aware embedding to demonstrate a new spatio-temporal action retrieval scenario with composite queries. Action localization and classification experiments on four contemporary action video datasets support our proposal. Apart from state-of-the-art results in the zero-shot localization and classification settings, our spatial-aware embedding is even competitive with recent supervised action localization alternatives.",http://arxiv.org/pdf/1707.09145v1
408,Semantic Video CNNs Through Representation Warping,Raghudeep Gadde; Varun Jampani; Peter V. Gehler,Semantic Video CNNs through Representation Warping,"In this work, we propose a technique to convert CNN models for semantic segmentation of static images into CNNs for video data. We describe a warping method that can be used to augment existing architectures with very little extra computational cost. This module is called NetWarp and we demonstrate its use for a range of network architectures. The main design principle is to use optical flow of adjacent frames for warping internal network representations across time. A key insight of this work is that fast optical flow methods can be combined with many different CNN architectures for improved performance and end-to-end training. Experiments validate that the proposed approach incurs only little extra computational cost, while improving performance, when video streams are available. We achieve new state-of-the-art results on the CamVid and Cityscapes benchmark datasets and show consistent improvements over different baseline networks. Our code and models will be available at http://segmentation.is.tue.mpg.de",http://arxiv.org/pdf/1708.03088v1
1324,Video Frame Synthesis Using Deep Voxel Flow,Ziwei Liu; Raymond A. Yeh; Xiaoou Tang; Yiming Liu; Aseem Agarwala,Video Frame Synthesis using Deep Voxel Flow,"We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.",http://arxiv.org/pdf/1702.02463v2
1345,Detail-Revealing Deep Video Super-Resolution,Xin Tao; Hongyun Gao; Renjie Liao; Jue Wang; Jiaya Jia,Detail-revealing Deep Video Super-resolution,"Previous CNN-based video super-resolution approaches need to align multiple frames to the reference. In this paper, we show that proper frame alignment and motion compensation is crucial for achieving high quality results. We accordingly propose a `sub-pixel motion compensation' (SPMC) layer in a CNN framework. Analysis and experiments show the suitability of this layer in video SR. The final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal image details. Our implementation can generate visually and quantitatively high-quality results, superior to current state-of-the-arts, without the need of parameter tuning.",http://arxiv.org/pdf/1704.02738v1
1148,Learning Video Object Segmentation With Visual Memory,Pavel Tokmakov; Karteek Alahari; Cordelia Schmid,Learning Video Object Segmentation with Visual Memory,"This paper addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a ""visual memory"" in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given a video frame as input, our approach assigns each pixel an object or background label based on the learned spatio-temporal features as well as the ""visual memory"" specific to the video, acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results. For example, our approach outperforms the top method on the DAVIS dataset by nearly 6%. We also provide an extensive ablative analysis to investigate the influence of each component in the proposed framework.",http://arxiv.org/pdf/1704.05737v2
500,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,Mehdi S. M. Sajjadi; Bernhard Sch√∂lkopf; Michael Hirsch,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,"Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values.   We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.",http://arxiv.org/pdf/1612.07919v2
1242,Makeup-Go: Blind Reversion of Portrait Edit,Ying-Cong Chen; Xiaoyong Shen; Jiaya Jia,,,
1866,Shadow Detection With Conditional Generative Adversarial Networks,Vu Nguyen; Tomas F. Yago Vicente; Maozheng Zhao; Minh Hoai; Dimitris Samaras,,,
1954,Learning High Dynamic Range From Outdoor Panoramas,Jinsong Zhang; Jean-Fran√ßois Lalonde,Learning High Dynamic Range from Outdoor Panoramas,"Outdoor lighting has extremely high dynamic range. This makes the process of capturing outdoor environment maps notoriously challenging since special equipment must be used. In this work, we propose an alternative approach. We first capture lighting with a regular, LDR omnidirectional camera, and aim to recover the HDR after the fact via a novel, learning-based inverse tonemapping method. We propose a deep autoencoder framework which regresses linear, high dynamic range data from non-linear, saturated, low dynamic range panoramas. We validate our method through a wide set of experiments on synthetic data, as well as on a novel dataset of real photographs with ground truth. Our approach finds applications in a variety of settings, ranging from outdoor light capture to image matching.",http://arxiv.org/pdf/1703.10200v3
2581,DCTM: Discrete-Continuous Transformation Matching for Semantic Flow,Seungryong Kim; Dongbo Min; Stephen Lin; Kwanghoon Sohn,DCTM: Discrete-Continuous Transformation Matching for Semantic Flow,"Techniques for dense semantic correspondence have provided limited ability to deal with the geometric variations that commonly exist between semantically similar images. While variations due to scale and rotation have been examined, there lack practical solutions for more complex deformations such as affine transformations because of the tremendous size of the associated solution space. To address this problem, we present a discrete-continuous transformation matching (DCTM) framework where dense affine transformation fields are inferred through a discrete label optimization in which the labels are iteratively updated via continuous regularization. In this way, our approach draws solutions from the continuous space of affine transformations in a manner that can be computed efficiently through constant-time edge-aware filtering and a proposed affine-varying CNN-based descriptor. Experimental results show that this model outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks.",http://arxiv.org/pdf/1707.05471v1
812,MemNet: A Persistent Memory Network for Image Restoration,Ying Tai; Jian Yang; Xiaoming Liu; Chunyan Xu,MemNet: A Persistent Memory Network for Image Restoration,"Recently, very deep convolutional neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the long-term dependency problem is rarely realized for these very deep models, which results in the prior states/layers having little influence on the subsequent ones. Motivated by the fact that human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a gate unit, to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the gate unit, which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i.e., image denosing, super-resolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at https://github.com/tyshiwo/MemNet.",http://arxiv.org/pdf/1708.02209v1
1164,Structure-Measure: A New Way to Evaluate Foreground Maps,Deng-Ping Fan; Ming-Ming Cheng; Yun Liu; Tao Li; Ali Borji,Structure-measure: A New Way to Evaluate Foreground Maps,"Foreground map evaluation is crucial for gauging the progress of object segmentation algorithms, in particular in the filed of salient object detection where the purpose is to accurately detect and segment the most salient object in a scene. Several widely-used measures such as Area Under the Curve (AUC), Average Precision (AP) and the recently proposed Fbw have been utilized to evaluate the similarity between a non-binary saliency map (SM) and a ground-truth (GT) map. These measures are based on pixel-wise errors and often ignore the structural similarities. Behavioral vision studies, however, have shown that the human visual system is highly sensitive to structures in scenes. Here, we propose a novel, efficient, and easy to calculate measure known an structural similarity measure (Structure-measure) to evaluate non-binary foreground maps. Our new measure simultaneously evaluates region-aware and object-aware structural similarity between a SM and a GT map. We demonstrate superiority of our measure over existing ones using 5 meta-measures on 5 benchmark datasets.",http://arxiv.org/pdf/1708.00786v1
1241,Weakly- and Self-Supervised Learning for Content-Aware Deep Image Retargeting,Donghyeon Cho; Jinsun Park; Tae-Hyun Oh; Yu-Wing Tai; In So Kweon,,,
1260,Practical and Efficient Multi-View Matching,Eleonora Maset; Federica Arrigoni; Andrea Fusiello,Joint optimization of fitting & matching in multi-view reconstruction,"Many standard approaches for geometric model fitting are based on pre-matched image features. Typically, such pre-matching uses only feature appearances (e.g. SIFT) and a large number of non-unique features must be discarded in order to control the false positive rate. In contrast, we solve feature matching and multi-model fitting problems in a joint optimization framework. This paper proposes several fit-&-match energy formulations based on a generalization of the assignment problem. We developed an efficient solver based on min-cost-max-flow algorithm that finds near optimal solutions. Our approach significantly increases the number of detected matches. In practice, energy-based joint fitting & matching allows to increase the distance between view-points previously restricted by robustness of local SIFT-matching and to improve the model fitting accuracy when compared to state-of-the-art multi-model fitting techniques.",http://arxiv.org/pdf/1303.2607v2
1658,Unrolled Memory Inner-Products: An Abstract GPU Operator for Efficient Vision-Related Computations,Yu-Sheng Lin; Wei-Chao Chen; Shao-Yi Chien,,,
1789,Learning to Push the Limits of Efficient FFT-Based Image Deconvolution,Jakob Kruse; Carsten Rother; Uwe Schmidt,,,
2100,Learning Spread-Out Local Feature Descriptors,Xu Zhang; Felix X. Yu; Sanjiv Kumar; Shih-Fu Chang,Learning Spread-out Local Feature Descriptors,"We propose a simple, yet powerful regularization technique that can be used to significantly improve both the pairwise and triplet losses in learning local feature descriptors. The idea is that in order to fully utilize the expressive power of the descriptor space, good local feature descriptors should be sufficiently ""spread-out"" over the space. In this work, we propose a regularization term to maximize the spread in feature descriptor inspired by the property of uniform distribution. We show that the proposed regularization with triplet loss outperforms existing Euclidean distance based descriptor learning techniques by a large margin. As an extension, the proposed regularization technique can also be used to improve image-level deep feature embedding.",http://arxiv.org/pdf/1708.06320v1
2507,Visual Odometry for Pixel Processor Arrays,Laurie Bose; Jianing Chen; Stephen J. Carey; Piotr Dudek; Walterio Mayol-Cuevas,,,
2144,"Joint Estimation of Camera Pose, Depth, Deblurring, and Super-Resolution From a Blurred Image Sequence",Haesol Park; Kyoung Mu Lee,"Joint Estimation of Camera Pose, Depth, Deblurring, and Super-Resolution from a Blurred Image Sequence","The conventional methods for estimating camera poses and scene structures from severely blurry or low resolution images often result in failure. The off-the-shelf deblurring or super-resolution methods may show visually pleasing results. However, applying each technique independently before matching is generally unprofitable because this naive series of procedures ignores the consistency between images. In this paper, we propose a pioneering unified framework that solves four problems simultaneously, namely, dense depth reconstruction, camera pose estimation, super-resolution, and deblurring. By reflecting a physical imaging process, we formulate a cost minimization problem and solve it using an alternating optimization technique. The experimental results on both synthetic and real videos show high-quality depth maps derived from severely degraded images that contrast the failures of naive multi-view stereo methods. Our proposed method also produces outstanding deblurred and super-resolved images unlike the independent application or combination of conventional video deblurring, super-resolution methods.",http://arxiv.org/pdf/1709.05745v1
2164,2D-Driven 3D Object Detection in RGB-D Images,Jean Lahoud; Bernard Ghanem,,,
2233,Ray Space Features for Plenoptic Structure-From-Motion,Yingliang Zhang; Peihong Yu; Wei Yang; Yuanxi Ma; Jingyi Yu,,,
2257,Depth Estimation Using Structured Light Flow ‚Ä Analysis of Projected Pattern Flow on an Object's Surface,Ryo Furukawa; Ryusuke Sagawa; Hiroshi Kawasaki,,,
2343,Monocular Dense 3D Reconstruction of a Complex Dynamic Scene From Two Perspective Frames,Suryansh Kumar; Yuchao Dai; Hongdong Li,Monocular Dense 3D Reconstruction of a Complex Dynamic Scene from Two Perspective Frames,"This paper proposes a new approach for monocular dense 3D reconstruction of a complex dynamic scene from two perspective frames. By applying superpixel over-segmentation to the image, we model a generically dynamic (hence non-rigid) scene with a piecewise planar and rigid approximation. In this way, we reduce the dynamic reconstruction problem to a ""3D jigsaw puzzle"" problem which takes pieces from an unorganized ""soup of superpixels"". We show that our method provides an effective solution to the inherent relative scale ambiguity in structure-from-motion. Since our method does not assume a template prior, or per-object segmentation, or knowledge about the rigidity of the dynamic scene, it is applicable to a wide range of scenarios. Extensive experiments on both synthetic and real monocular sequences demonstrate the superiority of our method compared with the state-of-the-art methods.",http://arxiv.org/pdf/1708.04398v1
2359,Optimal Transformation Estimation With Semantic Cues,Danda Pani Paudel; Adlane Habed; Luc Van Gool,,,
2374,Dynamics Enhanced Multi-Camera Motion Segmentation From Unsynchronized Videos,Xikang Zhang; Bengisu Ozbay; Mario Sznaier; Octavia Camps,,,
2453,Taking the Scenic Route to 3D: Optimising Reconstruction From Moving Cameras,Oscar Mendez; Simon Hadfield; Nicolas Pugeault; Richard Bowden,,,
2488,FLaME: Fast Lightweight Mesh Estimation Using Variational Smoothing on Delaunay Graphs,W. Nicholas Greene; Nicholas Roy,,,
2324,Efficient Algorithms for Moral Lineage Tracing,Markus Rempfler; Jan-Hendrik Lange; Florian Jug; Corinna Blasse; Eugene W. Myers; Bjoern H. Menze; Bjoern Andres,Efficient Algorithms for Moral Lineage Tracing,"Lineage tracing, the joint segmentation and tracking of living cells as they move and divide in a sequence of light microscopy images, is a challenging task. Jug et al. have proposed a mathematical abstraction of this task, the moral lineage tracing problem (MLTP), whose feasible solutions define both a segmentation of every image and a lineage forest of cells. Their branch-and-cut algorithm, however, is prone to many cuts and slow convergence for large instances. To address this problem, we make three contributions: (i) we devise the first efficient primal feasible local search algorithms for the MLTP, (ii) we improve the branch-and-cut algorithm by separating tighter cutting planes and by incorporating our primal algorithms, (iii) we show in experiments that our algorithms find accurate solutions on the problem instances of Jug et al. and scale to larger instances, leveraging moral lineage tracing to practical significance.",http://arxiv.org/pdf/1702.04111v2
2236,From RGB to Spectrum for Natural Scenes via Manifold-Based Mapping,Yan Jia; Yinqiang Zheng; Lin Gu; Art Subpa-Asa; Antony Lam; Yoichi Sato; Imari Sato,,,
2329,DeepFuse: A Deep Unsupervised Approach for Exposure Fusion With Extreme Exposure Image Pairs,K. Ram Prabhakar; V Sai Srikar; R. Venkatesh Babu,,,
2042,Learning Dense Facial Correspondences in Unconstrained Images,Ronald Yu; Shunsuke Saito; Haoxiang Li; Duygu Ceylan; Hao Li,Learning Dense Facial Correspondences in Unconstrained Images,"We present a minimalistic but effective neural network that computes dense facial correspondences in highly unconstrained RGB images. Our network learns a per-pixel flow and a matchability mask between 2D input photographs of a person and the projection of a textured 3D face model. To train such a network, we generate a massive dataset of synthetic faces with dense labels using renderings of a morphable face model with variations in pose, expressions, lighting, and occlusions. We found that a training refinement using real photographs is required to drastically improve the ability to handle real images. When combined with a facial detection and 3D face fitting step, we show that our approach outperforms the state-of-the-art face alignment methods in terms of accuracy and speed. By directly estimating dense correspondences, we do not rely on the full visibility of sparse facial landmarks and are not limited to the model space of regression-based approaches. We also assess our method on video frames and demonstrate successful per-frame processing under extreme pose variations, occlusions, and lighting conditions. Compared to existing 3D facial tracking techniques, our fitting does not rely on previous frames or frontal facial initialization and is robust to imperfect face detections.",http://arxiv.org/pdf/1709.00536v1
2485,Jointly Attentive Spatial-Temporal Pooling Networks for Video-Based Person Re-Identification,Shuangjie Xu; Yu Cheng; Kang Gu; Yang Yang; Shiyu Chang; Pan Zhou,Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification,"Person Re-Identification (person re-id) is a crucial task as its applications in visual surveillance and human-computer interaction. In this work, we present a novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for video-based person re-identification, which enables the feature extractor to be aware of the current input video sequences, in a way that interdependency from the matching items can directly influence the computation of each other's representation. Specifically, the spatial pooling layer is able to select regions from each frame, while the attention temporal pooling performed can select informative frames over the sequence, both pooling guided by the information from distance matching. Experiments are conduced on the iLIDS-VID, PRID-2011 and MARS datasets and the results demonstrate that this approach outperforms existing state-of-art methods. We also analyze how the joint pooling in both dimensions can boost the person re-id performance more effectively than using either of them separately.",http://arxiv.org/pdf/1708.02286v2
2106,Automatic Content-Aware Projection for 360¬∞ Videos,Yeong Won Kim; Chang-Ryeol Lee; Dae-Yong Cho; Yong Hoon Kwon; Hyeok-Jae Choi; Kuk-Jin Yoon,,,
2134,Blur-Invariant Deep Learning for Blind-Deblurring,T. M. Nimisha; Akash Kumar Singh; A. N. Rajagopalan,,,
2256,Non-Linear Convolution Filters for CNN-Based Learning,Georgios Zoumpourlis; Alexandros Doumanoglou; Nicholas Vretos; Petros Daras,Non-linear Convolution Filters for CNN-based Learning,"During the last years, Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in image classification. Their architectures have largely drawn inspiration by models of the primate visual system. However, while recent research results of neuroscience prove the existence of non-linear operations in the response of complex visual cells, little effort has been devoted to extend the convolution technique to non-linear forms. Typical convolutional layers are linear systems, hence their expressiveness is limited. To overcome this, various non-linearities have been used as activation functions inside CNNs, while also many pooling strategies have been applied. We address the issue of developing a convolution method in the context of a computational model of the visual cortex, exploring quadratic forms through the Volterra kernels. Such forms, constituting a more rich function space, are used as approximations of the response profile of visual cells. Our proposed second-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a network which combines linear and non-linear filters in its convolutional layers, can outperform networks that use standard linear filters with the same architecture, yielding results competitive with the state-of-the-art on these datasets.",http://arxiv.org/pdf/1708.07038v1
2275,AOD-Net: All-In-One Dehazing Network,Boyi Li; Xiulian Peng; Zhangyang Wang; Jizheng Xu; Dan Feng,An All-in-One Network for Dehazing and Beyond,"This paper proposes an image dehazing model built with a convolutional neural network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed based on a re-formulated atmospheric scattering model. Instead of estimating the transmission matrix and the atmospheric light separately as most previous models did, AOD-Net directly generates the clean image through a light-weight CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other deep models, e.g., Faster R-CNN, for improving high-level task performance on hazy images. Experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of PSNR, SSIM and the subjective visual quality. Furthermore, when concatenating AOD-Net with Faster R-CNN and training the joint pipeline from end to end, we witness a large improvement of the object detection performance on hazy images.",http://arxiv.org/pdf/1707.06543v1
2387,Simultaneous Detection and Removal of High Altitude Clouds From an Image,Tushar Sandhan; Jin Young Choi,,,
2404,Understanding Low- and High-Level Contributions to Fixation Prediction,Matthias K√ºmmerer; Thomas S. A. Wallis; Leon A. Gatys; Matthias Bethge,Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations,"Predicting where people look in natural scenes has attracted a lot of interest in computer vision and computational neuroscience over the past two decades. Two seemingly contrasting categories of cues have been proposed to influence where people look: \textit{low-level image saliency} and \textit{high-level semantic information}. Our first contribution is to take a detailed look at these cues to confirm the hypothesis proposed by Henderson~\cite{henderson1993eye} and Nuthmann \& Henderson~\cite{nuthmann2010object} that observers tend to look at the center of objects. We analyzed fixation data for scene free-viewing over 17 observers on 60 fully annotated images with various types of objects. Images contained different types of scenes, such as natural scenes, line drawings, and 3D rendered scenes. Our second contribution is to propose a simple combined model of low-level saliency and object center-bias that outperforms each individual component significantly over our data, as well as on the OSIE dataset by Xu et al.~\cite{xu2014predicting}. The results reconcile saliency with object center-bias hypotheses and highlight that both types of cues are important in guiding fixations. Our work opens new directions to understand strategies that humans use in observing scenes and objects, and demonstrates the construction of combined models of low-level saliency and high-level object-based information.",http://arxiv.org/pdf/1503.08853v1
2424,Image Super-Resolution Using Dense Skip Connections,Tong Tong; Gen Li; Xiejie Liu; Qinquan Gao,,,
2441,Convergence Analysis of MAP Based Blur Kernel Estimation,Sunghyun Cho; Seungyong Lee,Convergence Analysis of MAP based Blur Kernel Estimation,"One popular approach for blind deconvolution is to formulate a maximum a posteriori (MAP) problem with sparsity priors on the gradients of the latent image, and then alternatingly estimate the blur kernel and the latent image. While several successful MAP based methods have been proposed, there has been much controversy and confusion about their convergence, because sparsity priors have been shown to prefer blurry images to sharp natural images. In this paper, we revisit this problem and provide an analysis on the convergence of MAP based approaches. We first introduce a slight modification to a conventional joint energy function for blind deconvolution. The reformulated energy function yields the same alternating estimation process, but more clearly reveals how blind deconvolution works. We then show the energy function can actually favor the right solution instead of the no-blur solution under certain conditions, which explains the success of previous MAP based approaches. The reformulated energy function and our conditions for the convergence also provide a way to compare the qualities of different blur kernels, and we demonstrate its applicability to automatic blur kernel size selection, blur kernel estimation using light streaks, and defocus estimation.",http://arxiv.org/pdf/1611.07752v2
2461,Blob Reconstruction Using Unilateral Second Order Gaussian Kernels With Application to High-ISO Long-Exposure Image Denoising,Gang Wang; Carlos Lopez-Molina; Bernard De Baets,,,
2490,Deep Generative Adversarial Compression Artifact Removal,Leonardo Galteri; Lorenzo Seidenari; Marco Bertini; Alberto Del Bimbo,Deep Generative Adversarial Compression Artifact Removal,"Compression artifacts arise in images whenever a lossy compression algorithm is applied. These artifacts eliminate details present in the original image, or add noise and small structures; because of these effects they make images less pleasant for the human eye, and may also lead to decreased performance of computer vision algorithms such as object detectors. To eliminate such artifacts, when decompressing an image, it is required to recover the original image from a disturbed version. To this end, we present a feed-forward fully convolutional residual network model trained using a generative adversarial framework. To provide a baseline, we show that our model can be also trained optimizing the Structural Similarity (SSIM), which is a better loss with respect to the simpler Mean Squared Error (MSE). Our GAN is able to produce images with more photorealistic details than MSE or SSIM based networks. Moreover we show that our approach can be used as a pre-processing step for object detection in case images are degraded by compression to a point that state-of-the art detectors fail. In this task, our GAN method obtains better performance than MSE or SSIM trained networks.",http://arxiv.org/pdf/1704.02518v2
2434,Online Multi-Object Tracking Using CNN-Based Single Object Tracker With Spatial-Temporal Attention Mechanism,Qi Chu; Wanli Ouyang; Hongsheng Li; Xiaogang Wang; Bin Liu; Nenghai Yu,Online Multi-Object Tracking Using CNN-based Single Object Tracker with Spatial-Temporal Attention Mechanism,"In this paper, we propose a CNN-based framework for online MOT. This framework utilizes the merits of single object trackers in adapting appearance models and searching for target in the next frame. Simply applying single object tracker for MOT will encounter the problem in computational efficiency and drifted results caused by occlusion. Our framework achieves computational efficiency by sharing features and using ROI-Pooling to obtain individual features for each target. Some online learned target-specific CNN layers are used for adapting the appearance model for each target. In the framework, we introduce spatial-temporal attention mechanism (STAM) to handle the drift caused by occlusion and interaction among targets. The visibility map of the target is learned and used for inferring the spatial attention map. The spatial attention map is then applied to weight the features. Besides, the occlusion status can be estimated from the visibility map, which controls the online updating process via weighted loss on training samples with different occlusion statuses in different frames. It can be considered as temporal attention mechanism. The proposed algorithm achieves 34.3% and 46.0% in MOTA on challenging MOT15 and MOT16 benchmark dataset respectively.",http://arxiv.org/pdf/1708.02843v2
2039,Mutual Enhancement for Detection of Multiple Logos in Sports Videos,Yuan Liao; Xiaoqing Lu; Chengcui Zhang; Yongtao Wang; Zhi Tang,,,
2105,Referring Expression Generation and Comprehension via Attributes,Jingyu Liu; Liang Wang; Ming-Hsuan Yang,,,
2122,RoomNet: End-To-End Room Layout Estimation,Chen-Yu Lee; Vijay Badrinarayanan; Tomasz Malisiewicz; Andrew Rabinovich,RoomNet: End-to-End Room Layout Estimation,"This paper focuses on the task of room layout estimation from a monocular RGB image. Prior works break the problem into two sub-tasks: semantic segmentation of floor, walls, ceiling to produce layout hypotheses, followed by an iterative optimization step to rank these hypotheses. In contrast, we adopt a more direct formulation of this problem as one of estimating an ordered set of room layout keypoints. The room layout and the corresponding segmentation is completely specified given the locations of these ordered keypoints. We predict the locations of the room layout keypoints using RoomNet, an end-to-end trainable encoder-decoder network. On the challenging benchmark datasets Hedau and LSUN, we achieve state-of-the-art performance along with 200x to 600x speedup compared to the most recent work. Additionally, we present optional extensions to the RoomNet architecture such as including recurrent computations and memory units to refine the keypoint locations under the same parametric capacity.",http://arxiv.org/pdf/1703.06241v2
2174,SSH: Single Stage Headless Face Detector,Mahyar Najibi; Pouya Samangouei; Rama Chellappa; Larry S. Davis,SSH: Single Stage Headless Face Detector,"We introduce the Single Stage Headless (SSH) face detector. Unlike two stage proposal-classification detectors, SSH detects faces in a single stage directly from the early convolutional layers in a classification network. SSH is headless. That is, it is able to achieve state-of-the-art results while removing the ""head"" of its underlying classification network -- i.e. all fully connected layers in the VGG-16 which contains a large number of parameters. Additionally, instead of relying on an image pyramid to detect faces with various scales, SSH is scale-invariant by design. We simultaneously detect faces with different scales in a single forward pass of the network, but from different layers. These properties make SSH fast and light-weight. Surprisingly, with a headless VGG-16, SSH beats the ResNet-101-based state-of-the-art on the WIDER dataset. Even though, unlike the current state-of-the-art, SSH does not use an image pyramid and is 5X faster. Moreover, if an image pyramid is deployed, our light-weight network achieves state-of-the-art on all subsets of the WIDER dataset, improving the AP by 2.5%. SSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasets while using a small input size, leading to a runtime of 50 ms/image on a GPU.",http://arxiv.org/pdf/1708.03979v2
2207,AnnArbor: Approximate Nearest Neighbors Using Arborescence Coding,Artem Babenko; Victor Lempitsky,,,
2215,Boosting Image Captioning With Attributes,Ting Yao; Yingwei Pan; Yehao Li; Zhaofan Qiu; Tao Mei,Boosting Image Captioning with Attributes,"Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. To incorporate attributes, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework achieves superior results when compared to state-of-the-art deep models. Most remarkably, we obtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and publicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image representations by GoogleNet and achieve to date top-1 performance on COCO captioning Leaderboard.",http://arxiv.org/pdf/1611.01646v1
2245,Learning to Estimate 3D Hand Pose From Single RGB Images,Christian Zimmermann; Thomas Brox,Learning to Estimate 3D Hand Pose from Single RGB Images,"Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images. In this paper, we present an approach that estimates 3D hand pose from regular RGB images. This task has far more ambiguities due to the missing depth information. To this end, we propose a deep network that learns a network-implicit 3D articulation prior. Together with detected keypoints in the images, this network yields good estimates of the 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks. Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.",http://arxiv.org/pdf/1705.01389v2
2255,Locally-Transferred Fisher Vectors for Texture Classification,Yang Song; Fan Zhang; Qing Li; Heng Huang; Lauren J. O'Donnell; Weidong Cai,,,
2344,Object-Level Proposals,Jianxiang Ma; Anlong Ming; Zilong Huang; Xinggang Wang; Yu Zhou,Modeling Visual Compatibility through Hierarchical Mid-level Elements,"In this paper we present a hierarchical method to discover mid-level elements with the objective of modeling visual compatibility between objects. At the base-level, our method identifies patterns of CNN activations with the aim of modeling different variations/styles in which objects of the classes of interest may occur. At the top-level, the proposed method discovers patterns of co-occurring activations of base-level elements that define visual compatibility between pairs of object classes. Experiments on the massive Amazon dataset show the strength of our method at describing object classes and the characteristics that drive the compatibility between them.",http://arxiv.org/pdf/1604.00036v1
2429,Extreme Clicking for Efficient Object Annotation,Dim P. Papadopoulos; Jasper R. R. Uijlings; Frank Keller; Vittorio Ferrari,Extreme clicking for efficient object annotation,"Manually annotating object bounding boxes is central to building computer vision datasets, and it is very time consuming (annotating ILSVRC [53] took 35s for one high-quality box [62]). It involves clicking on imaginary corners of a tight box around the object. This is difficult as these corners are often outside the actual object and several adjustments are required to obtain a tight box. We propose extreme clicking instead: we ask the annotator to click on four physical points on the object: the top, bottom, left- and right-most points. This task is more natural and these points are easy to find. We crowd-source extreme point annotations for PASCAL VOC 2007 and 2012 and show that (1) annotation time is only 7s per box, 5x faster than the traditional way of drawing boxes [62]; (2) the quality of the boxes is as good as the original ground-truth drawn the traditional way; (3) detectors trained on our annotations are as accurate as those trained on the original ground-truth. Moreover, our extreme clicking strategy not only yields box coordinates, but also four accurate boundary points. We show (4) how to incorporate them into GrabCut to obtain more accurate segmentations than those delivered when initializing it from bounding boxes; (5) semantic segmentations models trained on these segmentations outperform those trained on segmentations derived from bounding boxes.",http://arxiv.org/pdf/1708.02750v1
2433,WordSup: Exploiting Word Annotations for Character Based Text Detection,Han Hu; Chengquan Zhang; Yuxuan Luo; Yuzhuo Wang; Junyu Han; Errui Ding,WordSup: Exploiting Word Annotations for Character based Text Detection,"Imagery texts are usually organized as a hierarchy of several visual elements, i.e. characters, words, text lines and text blocks. Among these elements, character is the most basic one for various languages such as Western, Chinese, Japanese, mathematical expression and etc. It is natural and convenient to construct a common text detection engine based on character detectors. However, training character detectors requires a vast of location annotated characters, which are expensive to obtain. Actually, the existing real text datasets are mostly annotated in word or line level. To remedy this dilemma, we propose a weakly supervised framework that can utilize word annotations, either in tight quadrangles or the more loose bounding boxes, for character detector training. When applied in scene text detection, we are thus able to train a robust character detector by exploiting word annotations in the rich large-scale real scene text datasets, e.g. ICDAR15 and COCO-text. The character detector acts as a key role in the pipeline of our text detection engine. It achieves the state-of-the-art performance on several challenging scene text detection benchmarks. We also demonstrate the flexibility of our pipeline by various scenarios, including deformed text detection and math expression recognition.",http://arxiv.org/pdf/1708.06720v1
2435,Illuminating Pedestrians via Simultaneous Detection & Segmentation,Garrick Brazil; Xi Yin; Xiaoming Liu,,,
2483,Generalized Orderless Pooling Performs Implicit Salient Matching,Marcel Simon; Yang Gao; Trevor Darrell; Joachim Denzler; Erik Rodner,Generalized orderless pooling performs implicit salient matching,"Most recent CNN architectures use average pooling as a final feature encoding step. In the field of fine-grained recognition, however, recent global representations like bilinear pooling offer improved performance. In this paper, we generalize average and bilinear pooling to ""alpha-pooling"", allowing for learning the pooling strategy during training. In addition, we present a novel way to visualize decisions made by these approaches. We identify parts of training images having the highest influence on the prediction of a given test image. It allows for justifying decisions to users and also for analyzing the influence of semantic parts. For example, we can show that the higher capacity VGG16 model focuses much more on the bird's head than, e.g., the lower-capacity VGG-M model when recognizing fine-grained bird categories. Both contributions allow us to analyze the difference when moving between average and bilinear pooling. In addition, experiments show that our generalized approach can outperform both across a variety of standard datasets.",http://arxiv.org/pdf/1705.00487v3
2085,Exploiting Spatial Structure for Localizing Manipulated Image Regions,Jawadul H. Bappy; Amit K. Roy-Chowdhury; Jason Bunk; Lakshmanan Nataraj; B. S. Manjunath,,,
2150,RDFNet: RGB-D Multi-Level Residual Feature Fusion for Indoor Semantic Segmentation,Seong-Jin Park; Ki-Sang Hong; Seungyong Lee,,,
2264,The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes,Gerhard Neuhold; Tobias Ollmann; Samuel Rota Bul√≤; Peter Kontschieder,,,
2306,Self-Organized Text Detection With Minimal Post-Processing via Border Learning,Yue Wu; Prem Natarajan,,,
2035,Sparse Exact PGA on Riemannian Manifolds,Monami Banerjee; Rudrasis Chakraborty; Baba C. Vemuri,,,
2055,Tensor RPCA by Bayesian CP Factorization With Complex Noise,Qiong Luo; Zhi Han; Xi'ai Chen; Yao Wang; Deyu Meng; Dong Liang; Yandong Tang,,,
2059,Multimodal Gaussian Process Latent Variable Models With Harmonization,Guoli Song; Shuhui Wang; Qingming Huang; Qi Tian,,,
2081,Segmentation-Aware Convolutional Networks Using Local Attention Masks,Adam W. Harley; Konstantinos G. Derpanis; Iasonas Kokkinos,Segmentation-Aware Convolutional Networks Using Local Attention Masks,"We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counter-acts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a ""segmentation-aware"" variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic segmentation we can match the performance of DenseCRFs while being faster and simpler, and in optical flow we obtain clearly sharper responses than networks that do not use local attention masks. In both cases, segmentation-aware convolution yields systematic improvements over strong baselines. Source code for this work is available online at http://cs.cmu.edu/~aharley/segaware.",http://arxiv.org/pdf/1708.04607v1
2249,Rotation Equivariant Vector Field Networks,Diego Marcos; Michele Volpi; Nikos Komodakis; Devis Tuia,Rotation equivariant vector field networks,"In many computer vision tasks, we expect a particular behavior of the output with respect to rotations of the input image. If this relationship is explicitly encoded, instead of treated as any other variation, the complexity of the problem is decreased, leading to a reduction in the size of the required model. In this paper, we propose the Rotation Equivariant Vector Field Networks (RotEqNet), a Convolutional Neural Network (CNN) architecture encoding rotation equivariance, invariance and covariance. Each convolutional filter is applied at multiple orientations and returns a vector field representing magnitude and angle of the highest scoring orientation at every spatial location. We develop a modified convolution operator relying on this representation to obtain deep architectures. We test RotEqNet on several problems requiring different responses with respect to the inputs' rotation: image classification, biomedical image segmentation, orientation estimation and patch matching. In all cases, we show that RotEqNet offers extremely compact models in terms of number of parameters and provides results in line to those of networks orders of magnitude larger.",http://arxiv.org/pdf/1612.09346v3
2276,ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression,Jian-Hao Luo; Jianxin Wu; Weiyao Lin,ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression,"We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31$\times$ FLOPs reduction and 16.63$\times$ compression on VGG-16, with only 0.52$\%$ top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1$\%$ top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.",http://arxiv.org/pdf/1707.06342v1
2300,AutoDIAL: Automatic DomaIn Alignment Layers,Fabio Maria Carlucci; Lorenzo Porzi; Barbara Caputo; Elisa Ricci; Samuel Rota Bul√≤,AutoDIAL: Automatic DomaIn Alignment Layers,"Classifiers trained on given databases perform poorly when tested on data acquired in different settings. This is explained in domain adaptation through a shift among distributions of the source and target domains. Attempts to align them have traditionally resulted in works reducing the domain shift by introducing appropriate loss terms, measuring the discrepancies between source and target distributions, in the objective function. Here we take a different route, proposing to align the learned representations by embedding in any given network specific Domain Alignment Layers, designed to match the source and target feature distributions to a reference one. Opposite to previous works which define a priori in which layers adaptation should be performed, our method is able to automatically learn the degree of feature alignment required at different levels of the deep network. Thorough experiments on different public benchmarks, in the unsupervised setting, confirm the power of our approach.",http://arxiv.org/pdf/1704.08082v2
2376,Focusing Attention: Towards Accurate Text Recognition in Natural Images,Zhanzhan Cheng; Fan Bai; Yunlu Xu; Gang Zheng; Shiliang Pu; Shuigeng Zhou,Focusing Attention: Towards Accurate Text Recognition in Natural Images,"Scene text recognition has been a hot research topic in computer vision due to its various applications. The state of the art is the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. However, we observe that existing attention-based methods perform poorly on complicated and/or low-quality images. One major reason is that existing methods cannot get accurate alignments between feature areas and targets for such images. We call this phenomenon ""attention drift"". To tackle this problem, in this paper we propose the FAN (the abbreviation of Focusing Attention Network) method that employs a focusing attention mechanism to automatically draw back the drifted attention. FAN consists of two major components: an attention network (AN) that is responsible for recognizing character targets as in the existing methods, and a focusing network (FN) that is responsible for adjusting attention by evaluating whether AN pays attention properly on the target areas in the images. Furthermore, different from the existing methods, we adopt a ResNet-based network to enrich deep representations of scene text images. Extensive experiments on various benchmarks, including the IIIT5k, SVT and ICDAR datasets, show that the FAN method substantially outperforms the existing methods.",http://arxiv.org/pdf/1709.02054v2
2400,Unsupervised Object Segmentation in Video by Efficient Selection of Highly Probable Positive Features,Emanuela Haller; Marius Leordeanu,Unsupervised object segmentation in video by efficient selection of highly probable positive features,"We address an essential problem in computer vision, that of unsupervised object segmentation in video, where a main object of interest in a video sequence should be automatically separated from its background. An efficient solution to this task would enable large-scale video interpretation at a high semantic level in the absence of the costly manually labeled ground truth. We propose an efficient unsupervised method for generating foreground object soft-segmentation masks based on automatic selection and learning from highly probable positive features. We show that such features can be selected efficiently by taking into consideration the spatio-temporal, appearance and motion consistency of the object during the whole observed sequence. We also emphasize the role of the contrasting properties between the foreground object and its background. Our model is created in two stages: we start from pixel level analysis, on top of which we add a regression model trained on a descriptor that considers information over groups of pixels and is both discriminative and invariant to many changes that the object undergoes throughout the video. We also present theoretical properties of our unsupervised learning method, that under some mild constraints is guaranteed to learn a correct discriminative classifier even in the unsupervised case. Our method achieves competitive and even state of the art results on the challenging Youtube-Objects and SegTrack datasets, while being at least one order of magnitude faster than the competition. We believe that the competitive performance of our method in practice, along with its theoretical properties, constitute an important step towards solving unsupervised discovery in video.",http://arxiv.org/pdf/1704.05674v1
2467,Nonparametric Variational Auto-Encoders for Hierarchical Representation Learning,Prasoon Goyal; Zhiting Hu; Xiaodan Liang; Chenyu Wang; Eric P. Xing,Nonparametric Variational Auto-encoders for Hierarchical Representation Learning,"The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.",http://arxiv.org/pdf/1703.07027v2
2495,Dense and Low-Rank Gaussian CRFs Using Deep Embeddings,Siddhartha Chandra; Nicolas Usunier; Iasonas Kokkinos,,,
2056,A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses,Quan Gan; Shangfei Wang; Longfei Hao; Qiang Ji,,,
2079,Moving Object Detection in Time-Lapse or Motion Trigger Image Sequences Using Low-Rank and Invariant Sparse Decomposition,Moein Shakeri; Hong Zhang,,,
2111,A Multilayer-Based Framework for Online Background Subtraction With Freely Moving Cameras,Yizhe Zhu; Ahmed Elgammal,A Multilayer-Based Framework for Online Background Subtraction with Freely Moving Cameras,"The exponentially increasing use of moving platforms for video capture introduces the urgent need to develop the general background subtraction algorithms with the capability to deal with the moving background. In this paper, we propose a multilayer-based framework for online background subtraction for videos captured by moving cameras. Unlike the previous treatments of the problem, the proposed method is not restricted to binary segmentation of background and foreground, but formulates it as a multi-label segmentation problem by modeling multiple foreground objects in different layers when they appear simultaneously in the scene. We assign an independent processing layer to each foreground object, as well as the background, where both motion and appearance models are estimated, and a probability map is inferred using a Bayesian filtering framework. Finally, Multi-label Graph-cut on Markov Random Field is employed to perform pixel-wise labeling. Extensive evaluation results show that the proposed method outperforms state-of-the-art methods on challenging video sequences.",http://arxiv.org/pdf/1709.01140v1
2120,Dynamic Label Graph Matching for Unsupervised Video Re-Identification,Mang Ye; Andy J. Ma; Liang Zheng; Jiawei Li; Pong C. Yuen,Dynamic Label Graph Matching for Unsupervised Video Re-Identification,"Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant cross-camera variations, this paper proposes a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods.",http://arxiv.org/pdf/1709.09297v1
2394,Spatiotemporal Modeling for Crowd Counting in Videos,Feng Xiong; Xingjian Shi; Dit-Yan Yeung,Spatiotemporal Modeling for Crowd Counting in Videos,"Region of Interest (ROI) crowd counting can be formulated as a regression problem of learning a mapping from an image or a video frame to a crowd density map. Recently, convolutional neural network (CNN) models have achieved promising results for crowd counting. However, even when dealing with video data, CNN-based methods still consider each video frame independently, ignoring the strong temporal correlation between neighboring frames. To exploit the otherwise very useful temporal information in video sequences, we propose a variant of a recent deep learning model called convolutional LSTM (ConvLSTM) for crowd counting. Unlike the previous CNN-based methods, our method fully captures both spatial and temporal dependencies. Furthermore, we extend the ConvLSTM model to a bidirectional ConvLSTM model which can access long-range information in both directions. Extensive experiments using four publicly available datasets demonstrate the reliability of our approach and the effectiveness of incorporating temporal information to boost the accuracy of crowd counting. In addition, we also conduct some transfer learning experiments to show that once our model is trained on one dataset, its learning experience can be transferred easily to a new dataset which consists of only very few video frames for model adaptation.",http://arxiv.org/pdf/1707.07890v1
2176,Personalized Cinemagraphs Using Semantic Understanding and Collaborative Learning,Tae-Hyun Oh; Kyungdon Joo; Neel Joshi; Baoyuan Wang; In So Kweon; Sing Bing Kang,Personalized Cinemagraphs using Semantic Understanding and Collaborative Learning,"Cinemagraphs are a compelling way to convey dynamic aspects of a scene. In these media, dynamic and still elements are juxtaposed to create an artistic and narrative experience. Creating a high-quality, aesthetically pleasing cinemagraph requires isolating objects in a semantically meaningful way and then selecting good start times and looping periods for those objects to minimize visual artifacts (such a tearing). To achieve this, we present a new technique that uses object recognition and semantic segmentation as part of an optimization method to automatically create cinemagraphs from videos that are both visually appealing and semantically meaningful. Given a scene with multiple objects, there are many cinemagraphs one could create. Our method evaluates these multiple candidates and presents the best one, as determined by a model trained to predict human preferences in a collaborative way. We demonstrate the effectiveness of our approach with multiple results and a user study.",http://arxiv.org/pdf/1708.02970v1
2246,What Is Around the Camera?,Stamatios Georgoulis; Konstantinos Rematas; Tobias Ritschel; Mario Fritz; Tinne Tuytelaars; Luc Van Gool,,,
835,Weakly-Supervised Learning of Visual Relations,Julia Peyre; Josef Sivic; Ivan Laptev; Cordelia Schmid,Weakly-supervised learning of visual relations,"This paper introduces a novel approach for modeling visual relations between pairs of objects. We call relation a triplet of the form (subject, predicate, object) where the predicate is typically a preposition (eg. 'under', 'in front of') or a verb ('hold', 'ride') that links a pair of objects (subject, object). Learning such relations is challenging as the objects have different spatial configurations and appearances depending on the relation in which they occur. Another major challenge comes from the difficulty to get annotations, especially at box-level, for all possible triplets, which makes both learning and evaluation difficult. The contributions of this paper are threefold. First, we design strong yet flexible visual features that encode the appearance and spatial configuration for pairs of objects. Second, we propose a weakly-supervised discriminative clustering model to learn relations from image-level labels only. Third we introduce a new challenging dataset of unusual relations (UnRel) together with an exhaustive annotation, that enables accurate evaluation of visual relation retrieval. We show experimentally that our model results in state-of-the-art results on the visual relationship dataset significantly improving performance on previously unseen relations (zero-shot learning), and confirm this observation on our newly introduced UnRel dataset.",http://arxiv.org/pdf/1707.09472v1
888,BIER - Boosting Independent Embeddings Robustly,Michael Opitz; Georg Waltner; Horst Possegger; Horst Bischof,,,
1548,3D Graph Neural Networks for RGBD Semantic Segmentation,Xiaojuan Qi; Renjie Liao; Jiaya Jia; Sanja Fidler; Raquel Urtasun,,,
1793,Learning Multi-Attention Convolutional Neural Network for Fine-Grained Image Recognition,Heliang Zheng; Jianlong Fu; Tao Mei; Jiebo Luo,HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis,"Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attention-based deep neural network, named as HydraPlus-Net (HP-net), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person re-identification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-the-art methods on various datasets.",http://arxiv.org/pdf/1709.09930v1
1114,Learning 3D Object Categories by Looking Around Them,David Novotny; Diane Larlus; Andrea Vedaldi,Learning 3D Object Categories by Looking Around Them,"Traditional approaches for learning 3D object categories use either synthetic data or manual supervision. In this paper, we propose a method which does not require manual annotations and is instead cued by observing objects from a moving vantage point. Our system builds on two innovations: a Siamese viewpoint factorization network that robustly aligns different videos together without explicitly comparing 3D shapes; and a 3D shape completion network that can extract the full shape of an object from partial observations. We also demonstrate the benefits of configuring networks to perform probabilistic predictions as well as of geometry-aware data augmentation schemes. We obtain state-of-the-art results on publicly-available benchmarks.",http://arxiv.org/pdf/1705.03951v2
15,Quantitative Evaluation of Confidence Measures in a Machine Learning World,Matteo Poggi; Fabio Tosi; Stefano Mattoccia,,,
372,Towards End-To-End Text Spotting With Convolutional Recurrent Neural Networks,Hui Li; Peng Wang; Chunhua Shen,Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks,"In this work, we jointly address the problem of text detection and recognition in natural scene images based on convolutional recurrent neural networks. We propose a unified network that simultaneously localizes and recognizes text with a single forward pass, avoiding intermediate processes like image cropping and feature re-calculation, word separation, or character grouping. In contrast to existing approaches that consider text detection and recognition as two distinct tasks and tackle them one by one, the proposed framework settles these two tasks concurrently. The whole framework can be trained end-to-end, requiring only images, the ground-truth bounding boxes and text labels. Through end-to-end training, the learned features can be more informative, which improves the overall performance. The convolutional features are calculated only once and shared by both detection and recognition, which saves processing time. Our proposed method has achieved competitive performance on several benchmark datasets.",http://arxiv.org/pdf/1707.03985v1
1160,DeepSetNet: Predicting Sets With Deep Neural Networks,S. Hamid Rezatofighi; Vijay Kumar B G; Anton Milan; Ehsan Abbasnejad; Anthony Dick; Ian Reid,DeepSetNet: Predicting Sets with Deep Neural Networks,"This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problem of multi-class image classification. Moreover, we show that the proposed cardinality loss can also trivially be applied to the tasks of object counting and pedestrian detection. Our approach outperforms existing methods in all three cases on standard datasets.",http://arxiv.org/pdf/1611.08998v5
1353,Learning From Video and Text via Large-Scale Discriminative Clustering,Antoine Miech; Jean-Baptiste Alayrac; Piotr Bojanowski; Ivan Laptev; Josef Sivic,Learning from Video and Text via Large-Scale Discriminative Clustering,"Discriminative clustering has been successfully applied to a number of weakly-supervised learning tasks. Such applications include person and action recognition, text-to-video alignment, object co-segmentation and colocalization in videos and images. One drawback of discriminative clustering, however, is its limited scalability. We address this issue and propose an online optimization algorithm based on the Block-Coordinate Frank-Wolfe algorithm. We apply the proposed method to the problem of weakly supervised learning of actions and actors from movies together with corresponding movie scripts. The scaling up of the learning problem to 66 feature length movies enables us to significantly improve weakly supervised action recognition.",http://arxiv.org/pdf/1707.09074v1
1406,TALL: Temporal Activity Localization via Language Query,Jiyang Gao; Chen Sun; Zhenheng Yang; Ram Nevatia,TALL: Temporal Activity Localization via Language Query,"This paper focuses on temporal localization of actions in untrimmed videos. Existing methods typically train classifiers for a pre-defined list of actions and apply them in a sliding window fashion. However, activities in the wild consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets users' needs. We propose to localize activities by natural language queries. Temporal Activity Localization via Language (TALL) is challenging as it requires: (1) suitable design of text and video representations to allow cross-modal matching of actions and language queries; (2) ability to locate actions accurately given features from sliding windows of limited granularity. We propose a novel Cross-modal Temporal Regression Localizer (CTRL) to jointly model text query and video clips, output alignment scores and action boundary regression results for candidate clips. For evaluation, we adopt TaCoS dataset, and build a new dataset for this task on top of Charades by adding sentence temporal annotations, called Charades-STA. We also build complex sentence queries in Charades-STA for test. Experimental results show that CTRL outperforms previous methods significantly on both datasets.",http://arxiv.org/pdf/1705.02101v2
1422,End-To-End Face Detection and Cast Grouping in Movies Using Erd≈ës-R√©nyi Clustering,SouYoung Jin; Hang Su; Chris Stauffer; Erik Learned-Miller,,,
1673,Active Decision Boundary Annotation With Deep Generative Models,Miriam Huijser; Jan C. van Gemert,Active Decision Boundary Annotation with Deep Generative Models,"This paper is on active learning where the goal is to reduce the data annotation burden by interacting with a (human) oracle during training. Standard active learning methods ask the oracle to annotate data samples. Instead, we take a profoundly different approach: we ask for annotations of the decision boundary. We achieve this using a deep generative model to create novel instances along a 1d line. A point on the decision boundary is revealed where the instances change class. Experimentally we show on three data sets that our method can be plugged-in to other active learning schemes, that human oracles can effectively annotate points on the decision boundary, that our method is robust to annotation noise, and that decision boundary annotations improve over annotating data samples.",http://arxiv.org/pdf/1703.06971v2
1684,Convolutional Dictionary Learning via Local Processing,Vardan Papyan; Yaniv Romano; Jeremias Sulam; Michael Elad,Convolutional Dictionary Learning via Local Processing,"Convolutional Sparse Coding (CSC) is an increasingly popular model in the signal and image processing communities, tackling some of the limitations of traditional patch-based sparse representations. Although several works have addressed the dictionary learning problem under this model, these relied on an ADMM formulation in the Fourier domain, losing the sense of locality and the relation to the traditional patch-based sparse pursuit. A recent work suggested a novel theoretical analysis of this global model, providing guarantees that rely on a localized sparsity measure. Herein, we extend this local-global relation by showing how one can efficiently solve the convolutional sparse pursuit problem and train the filters involved, while operating locally on image patches. Our approach provides an intuitive algorithm that can leverage standard techniques from the sparse representations field. The proposed method is fast to train, simple to implement, and flexible enough that it can be easily deployed in a variety of applications. We demonstrate the proposed training scheme for image inpainting and image separation, while achieving state-of-the-art results.",http://arxiv.org/pdf/1705.03239v1
69,Deep Adaptive Image Clustering,Jianlong Chang; Lingfeng Wang; Gaofeng Meng; Shiming Xiang; Chunhong Pan,Probing the Galactic Bulge with deep Adaptive Optics imaging: the age of NGC 6440,"We present first results of a pilot project aimed at exploiting the potentiality of ground based adaptive optics imaging in the near infrared to determine the age of stellar clusters in the Galactic Bulge. We have used a combination of high resolution adaptive optics (ESO-VLT NAOS-CONICA) and wide-field (ESO-NTT-SOFI) photometry of the metal rich globular cluster NGC 6440 located towards the inner Bulge, to compute a deep color magnitude diagram from the tip of the Red Giant Branch down to J~22$, two magnitudes below the Main Sequence Turn Off (TO). The magnitude difference between the TO level and the red Horizontal Branch has been used as an age indicator. It is the first time that such a measurement for a bulge globular cluster has been obtained with a ground based telescope. From a direct comparison with 47 Tuc and with a set of theoretical isochrones, we concluded that NGC 6440 is old and likely coeval to 47 Tuc. This result adds a new evidence that the Galactic Bulge is ~2 Gyr younger at most than the pristine, metal poor population of the Galactic Halo.",http://arxiv.org/pdf/0809.3939v1
771,One Network to Solve Them All ‚Ä Solving Linear Inverse Problems Using Deep Projection Models,J. H. Rick Chang; Chun-Liang Li; Barnab√°s P√≥czos; B. V. K. Vijaya Kumar; Aswin C. Sankaranarayanan,,,
1044,Representation Learning by Learning to Count,Mehdi Noroozi; Hamed Pirsiavash; Paolo Favaro,Learning to count with deep object features,"Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene.",http://arxiv.org/pdf/1505.08082v1
1208,StackGAN: Text to Photo-Realistic Image Synthesis With Stacked Generative Adversarial Networks,Han Zhang; Tao Xu; Hongsheng Li; Shaoting Zhang; Xiaogang Wang; Xiaolei Huang; Dimitris N. Metaxas,StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks,"Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",http://arxiv.org/pdf/1612.03242v2
1340,Unsupervised Learning of Object Landmarks by Factorized Spatial Embeddings,James Thewlis; Hakan Bilen; Andrea Vedaldi,Unsupervised learning of object landmarks by factorized spatial embeddings,"Learning automatically the structure of object categories remains an important open problem in computer vision. In this paper, we propose a novel unsupervised approach that can discover and learn landmarks in object categories, thus characterizing their structure. Our approach is based on factorizing image deformations, as induced by a viewpoint change or an object deformation, by learning a deep neural network that detects landmarks consistently with such visual effects. Furthermore, we show that the learned landmarks establish meaningful correspondences between different object instances in a category without having to impose this requirement explicitly. We assess the method qualitatively on a variety of object types, natural and man-made. We also show that our unsupervised landmarks are highly predictive of manually-annotated landmarks in face benchmark datasets, and can be used to regress these with a high degree of accuracy.",http://arxiv.org/pdf/1705.02193v2
2513,Editable Parametric Dense Foliage From 3D Capture,Gaurav Chaurasia; Paul Beardsley,,,
2592,Refractive Structure-From-Motion Through a Flat Refractive Interface,Fran√ßois Chadebecq; Francisco Vasconcelos; George Dwyer; Ren√© Lacher; S√©bastien Ourselin; Tom Vercauteren; Danail Stoyanov,,,
2616,Submodular Trajectory Optimization for Aerial 3D Scanning,Mike Roberts; Debadeepta Dey; Anh Truong; Sudipta Sinha; Shital Shah; Ashish Kapoor; Pat Hanrahan; Neel Joshi,Submodular Trajectory Optimization for Aerial 3D Scanning,"Drones equipped with cameras are emerging as a powerful tool for large-scale aerial 3D scanning, but existing automatic flight planners do not exploit all available information about the scene, and can therefore produce inaccurate and incomplete 3D models. We present an automatic method to generate drone trajectories, such that the imagery acquired during the flight will later produce a high-fidelity 3D model. Our method uses a coarse estimate of the scene geometry to plan camera trajectories that: (1) cover the scene as thoroughly as possible; (2) encourage observations of scene geometry from a diverse set of viewing angles; (3) avoid obstacles; and (4) respect a user-specified flight time budget. Our method relies on a mathematical model of scene coverage that exhibits an intuitive diminishing returns property known as submodularity. We leverage this property extensively to design a trajectory planning algorithm that reasons globally about the non-additive coverage reward obtained across a trajectory, jointly with the cost of traveling between views. We evaluate our method by using it to scan three large outdoor scenes, and we perform a quantitative evaluation using a photorealistic video game simulator.",http://arxiv.org/pdf/1705.00703v3
2908,Camera Calibration by Global Constraints on the Motion of Silhouettes,Gil Ben-Artzi,Camera Calibration by Global Constraints on the Motion of Silhouettes,"We address the problem of epipolar geometry using the motion of silhouettes. Such methods match epipolar lines or frontier points across views, which are then used as the set of putative correspondences. We introduce an approach that improves by two orders of magnitude the performance over state-of-the-art methods, by significantly reducing the number of outliers in the putative matching. We model the frontier points' correspondence problem as constrained flow optimization, requiring small differences between their coordinates over consecutive frames. Our approach is formulated as a Linear Integer Program and we show that due to the nature of our problem, it can be solved efficiently in an iterative manner. Our method was validated on four standard datasets providing accurate calibrations across very different viewpoints.",http://arxiv.org/pdf/1704.04360v1
2913,Deltille Grids for Geometric Camera Calibration,Hyowon Ha; Michal Perdoch; Hatem Alismail; In So Kweon; Yaser Sheikh,,,
2523,A Lightweight Single-Camera Polarization Compass With Covariance Estimation,Wolfgang St√ºrzl,,,
3015,Reflectance Capture Using Univariate Sampling of BRDFs,Zhuo Hui; Kalyan Sunkavalli; Joon-Young Lee; Sunil Hadap; Jian Wang; Aswin C. Sankaranarayanan,,,
3078,Estimating Defocus Blur via Rank of Local Patches,Guodong Xu; Yuhui Quan; Hui Ji,,,
2530,RGB-Infrared Cross-Modality Person Re-Identification,Ancong Wu; Wei-Shi Zheng; Hong-Xing Yu; Shaogang Gong; Jianhuang Lai,,,
2559,Intrinsic 3D Dynamic Surface Tracking Based on Dynamic Ricci Flow and Teichm√ºller Map,Xiaokang Yu; Na Lei; Yalin Wang; Xianfeng Gu,,,
2788,Multi-Scale Deep Learning Architectures for Person Re-Identification,Xuelin Qian; Yanwei Fu; Yu-Gang Jiang; Tao Xiang; Xiangyang Xue,Multi-scale Deep Learning Architectures for Person Re-identification,"Person Re-identification (re-id) aims to match people across non-overlapping camera views in a public space. It is a challenging problem because many people captured in surveillance videos wear similar clothes. Consequently, the differences in their appearance are often subtle and only detectable at the right location and scales. Existing re-id models, particularly the recently proposed deep learning based ones match people at a single scale. In contrast, in this paper, a novel multi-scale deep learning model is proposed. Our model is able to learn deep discriminative feature representations at different scales and automatically determine the most suitable scales for matching. The importance of different spatial locations for extracting discriminative features is also learned explicitly. Experiments are carried out to demonstrate that the proposed model outperforms the state-of-the art on a number of benchmarks",http://arxiv.org/pdf/1709.05165v1
2968,Range Loss for Deep Face Recognition With Long-Tailed Training Data,Xiao Zhang; Zhiyuan Fang; Yandong Wen; Zhifeng Li; Yu Qiao,Range Loss for Deep Face Recognition with Long-tail,"Convolutional neural networks have achieved great improvement on face recognition in recent years because of its extraordinary ability in learning discriminative features of people with different identities. To train such a well-designed deep network, tremendous amounts of data is indispensable. Long tail distribution specifically refers to the fact that a small number of generic entities appear frequently while other objects far less existing. Considering the existence of long tail distribution of the real world data, large but uniform distributed data are usually hard to retrieve. Empirical experiences and analysis show that classes with more samples will pose greater impact on the feature learning process and inversely cripple the whole models feature extracting ability on tail part data. Contrary to most of the existing works that alleviate this problem by simply cutting the tailed data for uniform distributions across the classes, this paper proposes a new loss function called range loss to effectively utilize the whole long tailed data in training process. More specifically, range loss is designed to reduce overall intra-personal variations while enlarging inter-personal differences within one mini-batch simultaneously when facing even extremely unbalanced data. The optimization objective of range loss is the $k$ greatest range's harmonic mean values in one class and the shortest inter-class distance within one batch. Extensive experiments on two famous and challenging face recognition benchmarks (Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) not only demonstrate the effectiveness of the proposed approach in overcoming the long tail effect but also show the good generalization ability of the proposed approach.",http://arxiv.org/pdf/1611.08976v1
3220,Face Sketch Matching via Coupled Deep Transform Learning,Shruti Nagpal; Maneet Singh; Richa Singh; Mayank Vatsa; Afzel Noore; Angshul Majumdar,,,
2685,Realistic Dynamic Facial Textures From a Single Image Using GANs,Kyle Olszewski; Zimo Li; Chao Yang; Yi Zhou; Ronald Yu; Zeng Huang; Sitao Xiang; Shunsuke Saito; Pushmeet Kohli; Hao Li,,,
2837,Pixel Recursive Super Resolution,Ryan Dahl; Mohammad Norouzi; Jonathon Shlens,Pixel Recursive Super Resolution,"We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details--hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.",http://arxiv.org/pdf/1702.00783v2
2980,Recurrent Color Constancy,Yanlin Qian; Ke Chen; Jarno Nikkanen; Joni-Kristian K√§m√§r√§inen; Ji≈ô√≠ Matas,Functions that Emerge through End-to-End Reinforcement Learning - The Direction for Artificial General Intelligence -,"Recently, triggered by the impressive results in TV-games or game of Go by Google DeepMind, end-to-end reinforcement learning (RL) is collecting attentions. Although little is known, the author's group has propounded this framework for around 20 years and already has shown various functions that emerge in a neural network (NN) through RL. In this paper, they are introduced again at this timing.   ""Function Modularization"" approach is deeply penetrated subconsciously. The inputs and outputs for a learning system can be raw sensor signals and motor commands. ""State space"" or ""action space"" generally used in RL show the existence of functional modules. That has limited reinforcement learning to learning only for the action-planning module. In order to extend reinforcement learning to learning of the entire function on a huge degree of freedom of a massively parallel learning system and to explain or develop human-like intelligence, the author has believed that end-to-end RL from sensors to motors using a recurrent NN (RNN) becomes an essential key. Especially in the higher functions, this approach is very effective by being free from the need to decide their inputs and outputs.   The functions that emerge, we have confirmed, through RL using a NN cover a broad range from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in a RNN. Those are (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective attention, (10)prediction, (11)exploration. The end-to-end RL enables the emergence of very flexible comprehensive functions that consider many things in parallel although it is difficult to give the boundary of each function clearly.",http://arxiv.org/pdf/1703.02239v2
2987,Saliency Pattern Detection by Ranking Structured Trees,Lei Zhu; Haibin Ling; Jin Wu; Huiping Deng; Jin Liu,,,
2921,Monocular Video-Based Trailer Coupler Detection Using Multiplexer Convolutional Neural Network,Yousef Atoum; Joseph Roth; Michael Bliss; Wende Zhang; Xiaoming Liu,,,
3120,Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking,Heng Fan; Haibin Ling,Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking,"Being intensively studied, visual tracking has seen great recent advances in either speed (e.g., with correlation filters) or accuracy (e.g., with deep features). Real-time and high accuracy tracking algorithms, however, remain scarce. In this paper we study the problem from a new perspective and present a novel parallel tracking and verifying (PTAV) framework, by taking advantage of the ubiquity of multi-thread techniques and borrowing from the success of parallel tracking and mapping in visual SLAM. Our PTAV framework typically consists of two components, a tracker T and a verifier V, working in parallel on two separate threads. The tracker T aims to provide a super real-time tracking inference and is expected to perform well most of the time; by contrast, the verifier V checks the tracking results and corrects T when needed. The key innovation is that, V does not work on every frame but only upon the requests from T; on the other end, T may adjust the tracking according to the feedback from V. With such collaboration, PTAV enjoys both the high efficiency provided by T and the strong discriminative power by V. In our extensive experiments on popular benchmarks including OTB2013, OTB2015, TC128 and UAV20L, PTAV achieves the best tracking accuracy among all real-time trackers, and in fact performs even better than many deep learning based solutions. Moreover, as a general framework, PTAV is very flexible and has great rooms for improvement and generalization.",http://arxiv.org/pdf/1708.00153v1
3128,Non-Rigid Object Tracking via Deformable Patches Using Shape-Preserved KCF and Level Sets,Xin Sun; Ngai-Man Cheung; Hongxun Yao; Yiluan Guo,,,
2712,A Discriminative View of MRF Pre-Processing Algorithms,Chen Wang; Charles Herrmann; Ramin Zabih,A discriminative view of MRF pre-processing algorithms,"While Markov Random Fields (MRFs) are widely used in computer vision, they present a quite challenging inference problem. MRF inference can be accelerated by pre-processing techniques like Dead End Elimination (DEE) or QPBO-based approaches which compute the optimal labeling of a subset of variables. These techniques are guaranteed to never wrongly label a variable but they often leave a large number of variables unlabeled. We address this shortcoming by interpreting pre-processing as a classification problem, which allows us to trade off false positives (i.e., giving a variable an incorrect label) versus false negatives (i.e., failing to label a variable). We describe an efficient discriminative rule that finds optimal solutions for a subset of variables. Our technique provides both per-instance and worst-case guarantees concerning the quality of the solution. Empirical studies were conducted over several benchmark datasets. We obtain a speedup factor of 2 to 12 over expansion moves without preprocessing, and on difficult non-submodular energy functions produce slightly lower energy.",http://arxiv.org/pdf/1708.02668v1
2529,Offline Handwritten Signature Modeling and Verification Based on Archetypal Analysis,Elias N. Zois; Ilias Theodorakopoulos; George Economou,,,
2544,Long Short-Term Memory Kalman Filters: Recurrent Neural Estimators for Pose Regularization,Huseyin Coskun; Felix Achilles; Robert DiPietro; Nassir Navab; Federico Tombari,Long Short-Term Memory Kalman Filters:Recurrent Neural Estimators for Pose Regularization,"One-shot pose estimation for tasks such as body joint localization, camera pose estimation, and object tracking are generally noisy, and temporal filters have been extensively used for regularization. One of the most widely-used methods is the Kalman filter, which is both extremely simple and general. However, Kalman filters require a motion model and measurement model to be specified a priori, which burdens the modeler and simultaneously demands that we use explicit models that are often only crude approximations of reality. For example, in the pose-estimation tasks mentioned above, it is common to use motion models that assume constant velocity or constant acceleration, and we believe that these simplified representations are severely inhibitive. In this work, we propose to instead learn rich, dynamic representations of the motion and noise models. In particular, we propose learning these models from data using long short term memory, which allows representations that depend on all previous observations and all previous states. We evaluate our method using three of the most popular pose estimation tasks in computer vision, and in all cases we obtain state-of-the-art performance.",http://arxiv.org/pdf/1708.01885v1
2620,Learning Spatio-Temporal Representation With Pseudo-3D Residual Networks,Zhaofan Qiu; Ting Yao; Tao Mei,,,
2654,"Deeper, Broader and Artier Domain Generalization",Da Li; Yongxin Yang; Yi-Zhe Song; Timothy M. Hospedales,,,
2667,Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval,Jifei Song; Qian Yu; Yi-Zhe Song; Tao Xiang; Timothy M. Hospedales,,,
2709,Soft-NMS ‚Ä Improving Object Detection With One Line of Code,Navaneeth Bodla; Bharat Singh; Rama Chellappa; Larry S. Davis,,,
2720,Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images,Aron Yu; Kristen Grauman,Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images,"Distinguishing subtle differences in attributes is valuable, yet learning to make visual comparisons remains non-trivial. Not only is the number of possible comparisons quadratic in the number of training images, but also access to images adequately spanning the space of fine-grained visual differences is limited. We propose to overcome the sparsity of supervision problem via synthetically generated images. Building on a state-of-the-art image generation engine, we sample pairs of training images exhibiting slight modifications of individual attributes. Augmenting real training image pairs with these examples, we then train attribute ranking models to predict the relative strength of an attribute in novel pairs of real images. Our results on datasets of faces and fashion images show the great promise of bootstrapping imperfect image generators to counteract sample sparsity for learning to rank.",http://arxiv.org/pdf/1612.06341v2
2905,Video Scene Parsing With Predictive Feature Learning,Xiaojie Jin; Xin Li; Huaxin Xiao; Xiaohui Shen; Zhe Lin; Jimei Yang; Yunpeng Chen; Jian Dong; Luoqi Liu; Zequn Jie; Jiashi Feng; Shuicheng Yan,Video Scene Parsing with Predictive Feature Learning,"In this work, we address the challenging video scene parsing problem by developing effective representation learning methods given limited parsing annotations. In particular, we contribute two novel methods that constitute a unified parsing framework. (1) \textbf{Predictive feature learning}} from nearly unlimited unlabeled video data. Different from existing methods learning features from single frame parsing, we learn spatiotemporal discriminative features by enforcing a parsing network to predict future frames and their parsing maps (if available) given only historical frames. In this way, the network can effectively learn to capture video dynamics and temporal context, which are critical clues for video scene parsing, without requiring extra manual annotations. (2) \textbf{Prediction steering parsing}} architecture that effectively adapts the learned spatiotemporal features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance. Extensive experiments over two challenging datasets, Cityscapes and Camvid, have demonstrated the effectiveness of our methods by showing significant improvement over well-established baselines.",http://arxiv.org/pdf/1612.00119v2
2954,Understanding and Mapping Natural Beauty,Scott Workman; Richard Souvenir; Nathan Jacobs,Understanding and Mapping Natural Beauty,"While natural beauty is often considered a subjective property of images, in this paper, we take an objective approach and provide methods for quantifying and predicting the scenicness of an image. Using a dataset containing hundreds of thousands of outdoor images captured throughout Great Britain with crowdsourced ratings of natural beauty, we propose an approach to predict scenicness which explicitly accounts for the variance of human ratings. We demonstrate that quantitative measures of scenicness can benefit semantic image understanding, content-aware image processing, and a novel application of cross-view mapping, where the sparsity of ground-level images can be addressed by incorporating unlabeled overhead images in the training and prediction steps. For each application, our methods for scenicness prediction result in quantitative and qualitative improvements over baseline approaches.",http://arxiv.org/pdf/1612.03142v2
3023,Human Pose Estimation Using Global and Local Normalization,Ke Sun; Cuiling Lan; Junliang Xing; Wenjun Zeng; Dong Liu; Jingdong Wang,Human Pose Estimation using Global and Local Normalization,"In this paper, we address the problem of estimating the positions of human joints, i.e., articulated pose estimation. Recent state-of-the-art solutions model two key issues, joint detection and spatial configuration refinement, together using convolutional neural networks. Our work mainly focuses on spatial configuration refinement by reducing variations of human poses statistically, which is motivated by the observation that the scattered distribution of the relative locations of joints e.g., the left wrist is distributed nearly uniformly in a circular area around the left shoulder) makes the learning of convolutional spatial models hard. We present a two-stage normalization scheme, human body normalization and limb normalization, to make the distribution of the relative joint locations compact, resulting in easier learning of convolutional spatial models and more accurate pose estimation. In addition, our empirical results show that incorporating multi-scale supervision and multi-scale fusion into the joint detection network is beneficial. Experiment results demonstrate that our method consistently outperforms state-of-the-art methods on the benchmarks.",http://arxiv.org/pdf/1709.07220v1
3025,HashNet: Deep Learning to Hash by Continuation,Zhangjie Cao; Mingsheng Long; Jianmin Wang; Philip S. Yu,HashNet: Deep Learning to Hash by Continuation,"Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill-posed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.",http://arxiv.org/pdf/1702.00758v4
3037,Scaling the Scattering Transform: Deep Hybrid Networks,Edouard Oyallon; Eugene Belilovsky; Sergey Zagoruyko,Scaling the Scattering Transform: Deep Hybrid Networks,"We use the scattering network as a generic and fixed ini-tialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1 x 1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns invariance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.",http://arxiv.org/pdf/1703.08961v2
3079,Flip-Invariant Motion Representation,Takumi Kobayashi,,,
3088,Scene Categorization With Spectral Features,Salman H. Khan; Munawar Hayat; Fatih Porikli,,,
3101,Image2song: Song Retrieval via Bridging Image Content and Lyric Words,Xuelong Li; Di Hu; Xiaoqiang Lu,Image2song: Song Retrieval via Bridging Image Content and Lyric Words,"Image is usually taken for expressing some kinds of emotions or purposes, such as love, celebrating Christmas. There is another better way that combines the image and relevant song to amplify the expression, which has drawn much attention in the social network recently. Hence, the automatic selection of songs should be expected. In this paper, we propose to retrieve semantic relevant songs just by an image query, which is named as the image2song problem. Motivated by the requirements of establishing correlation in semantic/content, we build a semantic-based song retrieval framework, which learns the correlation between image content and lyric words. This model uses a convolutional neural network to generate rich tags from image regions, a recurrent neural network to model lyric, and then establishes correlation via a multi-layer perceptron. To reduce the content gap between image and lyric, we propose to make the lyric modeling focus on the main image content via a tag attention. We collect a dataset from the social-sharing multimodal data to study the proposed problem, which consists of (image, music clip, lyric) triplets. We demonstrate that our proposed model shows noticeable results in the image2song retrieval task and provides suitable songs. Besides, the song2image task is also performed.",http://arxiv.org/pdf/1708.05851v1
2649,Deep Functional Maps: Structured Prediction for Dense Shape Correspondence,Or Litany; Tal Remez; Emanuele Rodol√_; Alex Bronstein; Michael Bronstein,Deep Functional Maps: Structured Prediction for Dense Shape Correspondence,"We introduce a new framework for learning dense correspondence between deformable 3D shapes. Existing learning based approaches model shape correspondence as a labelling problem, where each point of a query shape receives a label identifying a point on some reference domain; the correspondence is then constructed a posteriori by composing the label predictions of two input shapes. We propose a paradigm shift and design a structured prediction model in the space of functional maps, linear operators that provide a compact representation of the correspondence. We model the learning process via a deep residual network which takes dense descriptor fields defined on two shapes as input, and outputs a soft map between the two given objects. The resulting correspondence is shown to be accurate on several challenging benchmarks comprising multiple categories, synthetic models, real scans with acquisition artifacts, topological noise, and partiality.",http://arxiv.org/pdf/1704.08686v2
2707,Training Deep Networks to Be Spatially Sensitive,Nicholas Kolkin; Eli Shechtman; Gregory Shakhnarovich,Dynamic texture and scene classification by transferring deep image features,"Dynamic texture and scene classification are two fundamental problems in understanding natural video content. Extracting robust and effective features is a crucial step towards solving these problems. However the existing approaches suffer from the sensitivity to either varying illumination, or viewpoint changing, or even camera motion, and/or the lack of spatial information. Inspired by the success of deep structures in image classification, we attempt to leverage a deep structure to extract feature for dynamic texture and scene classification. To tackle with the challenges in training a deep structure, we propose to transfer some prior knowledge from image domain to video domain. To be specific, we propose to apply a well-trained Convolutional Neural Network (ConvNet) as a mid-level feature extractor to extract features from each frame, and then form a representation of a video by concatenating the first and the second order statistics over the mid-level features. We term this two-level feature extraction scheme as a Transferred ConvNet Feature (TCoF). Moreover we explore two different implementations of the TCoF scheme, i.e., the \textit{spatial} TCoF and the \textit{temporal} TCoF, in which the mean-removed frames and the difference between two adjacent frames are used as the inputs of the ConvNet, respectively. We evaluate systematically the proposed spatial TCoF and the temporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN, and Maryland, and demonstrate that the proposed approach yields superior performance.",http://arxiv.org/pdf/1502.00303v1
2931,3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds,Fangyu Liu; Shuaipeng Li; Liqiang Zhang; Chenghu Zhou; Rongtian Ye; Yuebin Wang; Jiwen Lu,3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds,"Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.",http://arxiv.org/pdf/1707.06783v1
2962,Semi Supervised Semantic Segmentation Using Generative Adversarial Network,Nasim Souly; Concetto Spampinato; Mubarak Shah,Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network,"Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs significant number of pixellevel annotated data, which is often unavailable. To address this lack, in this paper, we leverage, on one hand, massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework ,based on Generative Adversarial Networks (GANs), which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, enabling a bottom-up clustering process, which, in turn, improves multiclass pixel classification. To ensure higher quality of generated images for GANs with consequent improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We tested our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance also compared to state-of-the-art semantic segmentation method",http://arxiv.org/pdf/1703.09695v1
2575,Efficient Low Rank Tensor Ring Completion,Wenqi Wang; Vaneet Aggarwal; Shuchin Aeron,Efficient Low Rank Tensor Ring Completion,"Using the matrix product state (MPS) representation of the recently proposed tensor ring decompositions, in this paper we propose a tensor completion algorithm, which is an alternating minimization algorithm that alternates over the factors in the MPS representation. This development is motivated in part by the success of matrix completion algorithms that alternate over the (low-rank) factors. In this paper, we propose a spectral initialization for the tensor ring completion algorithm and analyze the computational complexity of the proposed algorithm. We numerically compare it with existing methods that employ a low rank tensor train approximation for data completion and show that our method outperforms the existing ones for a variety of real computer vision settings, and thus demonstrate the improved expressive power of tensor ring as compared to tensor train.",http://arxiv.org/pdf/1707.08184v1
2593,Semantic Image Synthesis via Adversarial Learning,Hao Dong; Simiao Yu; Chao Wu; Yike Guo,Semantic Image Synthesis via Adversarial Learning,"In this paper, we propose a way of synthesizing realistic images directly with natural language description, which has many useful applications, e.g. intelligent image manipulation. We attempt to accomplish such synthesis: given a source image and a target text description, our model synthesizes images to meet two requirements: 1) being realistic while matching the target text description; 2) maintaining other image features that are irrelevant to the text description. The model should be able to disentangle the semantic information from the two modalities (image and text), and generate new images from the combined semantics. To achieve this, we proposed an end-to-end neural architecture that leverages adversarial learning to automatically learn implicit loss functions, which are optimized to fulfill the aforementioned two requirements. We have evaluated our model by conducting experiments on Caltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated that our model is capable of synthesizing realistic images that match the given descriptions, while still maintain other features of original images.",http://arxiv.org/pdf/1707.06873v1
2594,Unified Deep Supervised Domain Adaptation and Generalization,Saeid Motiian; Marco Piccirilli; Donald A. Adjeroh; Gianfranco Doretto,Unified Deep Supervised Domain Adaptation and Generalization,"This work provides a unified framework for addressing the problem of visual supervised domain adaptation and generalization with deep models. The main idea is to exploit the Siamese architecture to learn an embedding subspace that is discriminative, and where mapped visual domains are semantically aligned and yet maximally separated. The supervised setting becomes attractive especially when only few target data samples need to be labeled. In this scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution. In addition, the approach has a high speed of adaptation, which requires an extremely low number of labeled target training samples, even one per category can be effective. The approach is extended to domain generalization. For both applications the experiments show very promising results.",http://arxiv.org/pdf/1709.10190v1
2632,Interpretable Transformations With Encoder-Decoder Networks,Daniel E. Worrall; Stephan J. Garbin; Daniyar Turmukhambetov; Gabriel J. Brostow,,,
2643,Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization,Kamran Ghasedi Dizaji; Amirhossein Herandi; Cheng Deng; Weidong Cai; Heng Huang,Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization,"Image clustering is one of the most important computer vision applications, which has been extensively studied in literature. However, current clustering methods mostly suffer from lack of efficiency and scalability when dealing with large-scale and high-dimensional data. In this paper, we propose a new clustering model, called DEeP Embedded RegularIzed ClusTering (DEPICT), which efficiently maps data into a discriminative embedding subspace and precisely predicts cluster assignments. DEPICT generally consists of a multinomial logistic regression function stacked on top of a multi-layer convolutional autoencoder. We define a clustering objective function using relative entropy (KL divergence) minimization, regularized by a prior for the frequency of cluster assignments. An alternating strategy is then derived to optimize the objective by updating parameters and estimating cluster assignments. Furthermore, we employ the reconstruction loss functions in our autoencoder, as a data-dependent regularization term, to prevent the deep embedding function from overfitting. In order to benefit from end-to-end optimization and eliminate the necessity for layer-wise pretraining, we introduce a joint learning framework to minimize the unified clustering and reconstruction loss functions together and train all network layers simultaneously. Experimental results indicate the superiority and faster running time of DEPICT in real-world clustering tasks, where no labeled data is available for hyper-parameter tuning.",http://arxiv.org/pdf/1704.06327v3
2683,Deep Scene Image Classification With the MFAFVNet,Yunsheng Li; Mandar Dixit; Nuno Vasconcelos,,,
2919,Learning Bag-Of-Features Pooling for Deep Convolutional Neural Networks,Nikolaos Passalis; Anastasios Tefas,Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) are well established models capable of achieving state-of-the-art classification accuracy for various computer vision tasks. However, they are becoming increasingly larger, using millions of parameters, while they are restricted to handling images of fixed size. In this paper, a quantization-based approach, inspired from the well-known Bag-of-Features model, is proposed to overcome these limitations. The proposed approach, called Convolutional BoF (CBoF), uses RBF neurons to quantize the information extracted from the convolutional layers and it is able to natively classify images of various sizes as well as to significantly reduce the number of parameters in the network. In contrast to other global pooling operators and CNN compression techniques the proposed method utilizes a trainable pooling layer that it is end-to-end differentiable, allowing the network to be trained using regular back-propagation and to achieve greater distribution shift invariance than competitive methods. The ability of the proposed method to reduce the parameters of the network and increase the classification accuracy over other state-of-the-art techniques is demonstrated using three image datasets.",http://arxiv.org/pdf/1707.08105v2
3060,Adversarial Examples Detection in Deep Networks With Convolutional Filter Statistics,Xin Li; Fuxin Li,Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics,"Deep learning has greatly improved visual recognition in recent years. However, recent research has shown that there exist many adversarial examples that can negatively impact the performance of such an architecture. This paper focuses on detecting those adversarial examples by analyzing whether they come from the same distribution as the normal examples. Instead of directly training a deep neural network to detect adversarials, a much simpler approach is proposed based on statistics on outputs from convolutional layers. A cascade classifier is designed to efficiently detect adversarials. Furthermore, trained from one particular adversarial generating mechanism, the resulting classifier can successfully detect adversarials from a completely different mechanism as well. After detecting adversarial examples, we show that many of them can be recovered by simply performing a small average filter on the image. Those findings should provoke us to think more about the classification mechanisms in deep convolutional neural networks.",http://arxiv.org/pdf/1612.07767v1
2550,Joint Prediction of Activity Labels and Starting Times in Untrimmed Videos,Tahmida Mahmud; Mahmudul Hasan; Amit K. Roy-Chowdhury,,,
2582,R-C3D: Region Convolutional 3D Network for Temporal Activity Detection,Huijuan Xu; Abir Das; Kate Saenko,,,
2601,Temporal Context Network for Activity Localization in Videos,Xiyang Dai; Bharat Singh; Guyue Zhang; Larry S. Davis; Yan Qiu Chen,Temporal Context Network for Activity Localization in Videos,"We present a Temporal Context Network (TCN) for precise temporal localization of human activities. Similar to the Faster-RCNN architecture, proposals are placed at equal intervals in a video which span multiple temporal scales. We propose a novel representation for ranking these proposals. Since pooling features only inside a segment is not sufficient to predict activity boundaries, we construct a representation which explicitly captures context around a proposal for ranking it. For each temporal segment inside a proposal, features are uniformly sampled at a pair of scales and are input to a temporal convolutional neural network for classification. After ranking proposals, non-maximum suppression is applied and classification is performed to obtain final detections. TCN outperforms state-of-the-art methods on the ActivityNet dataset and the THUMOS14 dataset.",http://arxiv.org/pdf/1708.02349v1
2780,Localizing Moments in Video With Natural Language,Lisa Anne Hendricks; Oliver Wang; Eli Shechtman; Josef Sivic; Trevor Darrell; Bryan Russell,Localizing Moments in Video with Natural Language,"We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.",http://arxiv.org/pdf/1708.01641v1
2892,TORNADO: A Spatio-Temporal Convolutional Regression Network for Video Action Proposal,Hongyuan Zhu; Romain Vial; Shijian Lu,,,
2939,Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos,Rui Hou; Chen Chen; Mubarak Shah,,,
3016,Learning Action Recognition Model From Depth and Skeleton Videos,Hossein Rahmani; Mohammed Bennamoun,Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition,"We propose Human Pose Models that represent RGB and depth images of human poses independent of clothing textures, backgrounds, lighting conditions, body shapes and camera viewpoints. Learning such universal models requires training images where all factors are varied for every human pose. Capturing such data is prohibitively expensive. Therefore, we develop a framework for synthesizing the training data. First, we learn representative human poses from a large corpus of real motion captured human skeleton data. Next, we fit synthetic 3D humans with different body shapes to each pose and render each from 180 camera viewpoints while randomly varying the clothing textures, background and lighting. Generative Adversarial Networks are employed to minimize the gap between synthetic and real image distributions. CNN models are then learned that transfer human poses to a shared high-level invariant space. The learned CNN models are then used as invariant feature extractors from real RGB and depth frames of human action videos and the temporal variations are modelled by Fourier Temporal Pyramid. Finally, linear SVM is used for classification. Experiments on three benchmark cross-view human action datasets show that our algorithm outperforms existing methods by significant margins for RGB only and RGB-D action recognition.",http://arxiv.org/pdf/1707.00823v1
3105,"The ""Something Something"" Video Database for Learning and Evaluating Visual Common Sense",Raghav Goyal; Samira Ebrahimi Kahou; Vincent Michalski; Joanna Materzy≈Ñska; Susanne Westphal; Heuna Kim; Valentin Haenel; Ingo Fruend; Peter Yianilos; Moritz Mueller-Freitag; Florian Hoppe; Christian Thurau; Ingo Bax; Roland Memisevic,,,
2638,GPLAC: Generalizing Vision-Based Robotic Skills Using Weakly Labeled Images,Avi Singh; Larry Yang; Sergey Levine,GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled Images,"We tackle the problem of learning robotic sensorimotor control policies that can generalize to visually diverse and unseen environments. Achieving broad generalization typically requires large datasets, which are difficult to obtain for task-specific interactive processes such as reinforcement learning or learning from demonstration. However, much of the visual diversity in the world can be captured through passively collected datasets of images or videos. In our method, which we refer to as GPLAC (Generalized Policy Learning with Attentional Classifier), we use both interaction data and weakly labeled image data to augment the generalization capacity of sensorimotor policies. Our method combines multitask learning on action selection and an auxiliary binary classification objective, together with a convolutional neural network architecture that uses an attentional mechanism to avoid distractors. We show that pairing interaction data from just a single environment with a diverse dataset of weakly labeled data results in greatly improved generalization to unseen environments, and show that this generalization depends on both the auxiliary objective and the attentional architecture that we propose. We demonstrate our results in both simulation and on a real robotic manipulator, and demonstrate substantial improvement over standard convolutional architectures and domain adaptation methods.",http://arxiv.org/pdf/1708.02313v1
2975,Semi-Global Weighted Least Squares in Image Filtering,Wei Liu; Xiaogang Chen; Chuanhua Shen; Zhi Liu; Jie Yang,Semi-Global Weighted Least Squares in Image Filtering,"Solving the global method of Weighted Least Squares (WLS) model in image filtering is both time- and memory-consuming. In this paper, we present an alternative approximation in a time- and memory- efficient manner which is denoted as Semi-Global Weighed Least Squares (SG-WLS). Instead of solving a large linear system, we propose to iteratively solve a sequence of subsystems which are one-dimensional WLS models. Although each subsystem is one-dimensional, it can take two-dimensional neighborhood information into account due to the proposed special neighborhood construction. We show such a desirable property makes our SG-WLS achieve close performance to the original two-dimensional WLS model but with much less time and memory cost. While previous related methods mainly focus on the 4-connected/8-connected neighborhood system, our SG-WLS can handle a more general and larger neighborhood system thanks to the proposed fast solution. We show such a generalization can achieve better performance than the 4-connected/8-connected neighborhood system in some applications. Our SG-WLS is $\sim20$ times faster than the WLS model. For an image of $M\times N$, the memory cost of SG-WLS is at most at the magnitude of $max\{\frac{1}{M}, \frac{1}{N}\}$ of that of the WLS model. We show the effectiveness and efficiency of our SG-WLS in a range of applications.",http://arxiv.org/pdf/1705.01674v3
3098,Scale Recovery for Monocular Visual Odometry Using Depth Estimated With Deep Convolutional Neural Fields,Xiaochuan Yin; Xiangwei Wang; Xiaoguo Du; Qijun Chen,,,